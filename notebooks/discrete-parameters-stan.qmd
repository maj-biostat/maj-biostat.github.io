---
title: "Discrete parameters in stan"
author: "maj"
date: "2024-11-20"
date-modified: last-modified
bibliography: ../etc/refs.bib
csl: ../etc/biomed-central.csl
categories:
  - stan
  - discrete parameters
---

```{r}
library(cmdstanr)
library(data.table)
library(ggplot2)
```


Hamiltonian Monte Carlo is usually used to estimate continuous parameters.
However, sometimes we are interested in discrete parameters.

HMC runs on log-probabilities.
Therefore, as long as we can increment the log-probability associated with our model (whatever the model is), we can code it up, even if it uses discrete parameters.
For models with discrete parameter, we have to 'marginalise out' the discrete parameter(s) and increment the log-density by the marginal density.

Ben Lambert has an example:

Consider a series of $K$ experiments where we get someone flips a coin a fixed number of times,  $n$, for each experiment, but we are never told $n$.
The individual uses the same coin across all the experiments and the probability of heads remains constant $\text{Pr}(X = \text{heads}) = \theta$.
We define $X_k$ as the number of heads obtained in each experiment $k$ yielding $(X_1, X_2, \dots X_K)$.
Again, both $\theta$ and $n$ are unknown to us; $\theta$ is continuous, but $n$ is discrete.

We want to be able to make inference on both $\theta$ and $n$, i.e. talk about the probability intervals for theta and the probability that $n$ takes on certain values.

Assume that we know $K = 10$ experiments were run and the following data was observed $X_k = (2, 4, 3, 3, 3, 3, 3, 3, 4, 4)$.

We adopt independent priors on $\theta$ and $n$:

$$
\begin{aligned}
n &\sim \text{Discrete-Unif}(5, 8) \\
\theta &\sim \text{Unif}(0, 1)
\end{aligned}
$$

We can write down the joint posterior of $\theta$ and $n$, i.e. $\text{Pr}(\theta, n | X)$ and then marginalise out the discrete parameter, $n$.
Once we have an expression that excludes the $n$, then we can get stan to use that expression to conduct the sampling we want it to do.
We have:

$$
\begin{aligned}
\text{Pr}(\theta | X) &= \sum_{n=5}^8 \text{Pr}(\theta, n | X)
\end{aligned}
$$

Stan runs on the log probability so we need to think in those terms:

$$
\begin{aligned}
\log(\text{Pr}(\theta | X)) &= \log \left(\sum_{n=5}^8 \text{Pr}(\theta, n | X) \right) \\
 &= \log \left(\sum_{n=5}^8 \exp( \log( \text{Pr}(\theta, n | X))) \right) \\
\end{aligned}
$$

where the second line is to ensure we are dealing with log probabilities for both terms.

In stan, the above can be achieved in a mathematically stable way via the [`log-sum-exp`](https://mc-stan.org/docs/stan-users-guide/floating-point.html#log-sum-of-exponentials){.external target="_blank"} function.

$$
\begin{aligned}
\log(\text{Pr}(\theta | X)) &= \text{log\_sum\_exp}_{n=5}^8 \left( \log( \text{Pr}(\theta, n | X) ) \right)
\end{aligned}
$$

Unfortunately, we do not have $\text{Pr}(\theta, n | X)$ but we can use Bayes rule to determine what it is:

$$
\begin{aligned}
\text{Pr}(\theta, n | X) &\propto \text{Pr}(X | \theta, n) \text{Pr}(\theta, n) \\
&= \text{Pr}(X | \theta, n) \text{Pr}(n)\text{Pr}(\theta)
\end{aligned}
$$

where the second line comes from the fact that we use independent priors.
Taking logs, we get:

$$
\begin{aligned}
\log(\text{Pr}(\theta, n | X)) &\propto \log(\text{Pr}(X | \theta, n)) +  \log(\text{Pr}(n)) + \log(\text{Pr}(\theta)) \\
\end{aligned}
$$

+ The first term on the RHS is the likelihood for which we use the binomial distribution. 
+ The second term is the log of the discrete uniform distribution that we defined earlier. For a given $n \in \{ 5,6,7,8 \}$ this is $\log(1/4)$ as we assume each option has equal probability.
+ Finally, the third term is standard uniform. However, given that the third term does not contain $n$, we do not actually need to include it in the expression for the joint distribution (although we do still include it in the model block as a standard uniform).

The above allows us to estimate models with discrete parameters by marginalising them out of the joint density.
But, what if we want to do inference on the discrete parameters?

Answer; write down the unnormalised density of $n$ conditional on $X$ and estimate via MCMC:

$$
\begin{aligned}
q(n | x) \approx \frac{1}{B} \sum_{i = 1}^B q(n, \theta_i | X)
\end{aligned}
$$

where $B$ is the number of MCMC samples and $\theta_i$ are the posterior samples.
Essentially, this is averaging over $\theta_i$.

To get the normalised version, we need to form a simplex (sum of elements is 1 and elements are non-negative and less than 1) across the four possible values for $n$ giving probabilities for $n = 5$, $n = 6$, $n = 7$ and $n = 8$.
We can obtain this from:

$$
\begin{aligned}
p(n | x) &\approx \frac{q(n|X)}{\sum_{n = 5}^8 q(n, | X)} \\
 &= \frac{\exp( \log( q(n | x) ) )}{ \exp( \text{log\_sum\_exp} (\log( q(n | X))) )}
\end{aligned}
$$

and in stan, we would write this as:

$$
\begin{aligned}
p(n | x) = \exp\left[  \log(q(n | X)) - \text{log\_sum\_exp}(\log( q(n | X)) ) \right]
\end{aligned}
$$

An implementation for the above discussion is shown below:

```{r}
#| class-output: stan
#| echo: false

cat(readLines("stan/discrete-param-1.stan"), sep = "\n")
```

Running the model with the assumed data gives us our parameter estimates for both $\theta$ and $n$.

```{r, eval = T}
m1 <- cmdstanr::cmdstan_model("stan/discrete-param-1.stan")

ld <- list(
  K = 10, X = c(2, 4, 3, 3, 3, 3, 3, 3, 4, 4)
)

f1 <- m1$sample(
    ld, iter_warmup = 1000, iter_sampling = 1000,
    parallel_chains = 4, chains = 4, refresh = 0, show_exceptions = F,
    max_treedepth = 10)

f1$summary()

```


Obviously, the above is somewhat contrived. 
We gnerally do not know the bounds on the discrete parameters.
For example, how did we know that the bounds of $n$ were 5 and 8?
How would we have modified the model to account for observing a 6 in the data?



<!-- ::: {#refs} -->
<!-- ::: -->
