[
  {
    "objectID": "notebooks/splines-01.html",
    "href": "notebooks/splines-01.html",
    "title": "Splines 1",
    "section": "",
    "text": "Setup and dependencies\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(gt)\nsuppressPackageStartupMessages(library(cmdstanr))\nsuppressPackageStartupMessages(library(brms))\nsuppressPackageStartupMessages(library(mgcv))\n\n# devtools::install_github(\"thomasp85/patchwork\")\nlibrary(patchwork)\n\nlibrary(splines2)\nlibrary(pander)\n\n\ntoks &lt;- unlist(tstrsplit(getwd(), \"/\")) \nif(toks[length(toks)] == \"maj-biostat.github.io\"){\n  prefix_stan &lt;- \"./stan\"\n} else {\n  prefix_stan &lt;- \"../stan\"\n}"
  },
  {
    "objectID": "notebooks/splines-01.html#introduction",
    "href": "notebooks/splines-01.html#introduction",
    "title": "Splines 1",
    "section": "Introduction",
    "text": "Introduction\nThe concept of linearity in the linear model refers to linearity in the parameters and not the predictors themselves. A model becomes nonlinear if the parameters are involved in nonlinear functions, e.g. \\(\\beta_1 ^{x_1}\\), \\(\\text{sin}(\\beta_1 x_1)\\) etc. In referring to linearity violation, one is usually considering whether the relationship between a predictor and outcome is being adequately represented by a linear term.\nFor an example of a non-linearity relationship, one might look towards the age versus height in humans. In our early, or early to mid-years, the height/age relationship is somewhat linear. However, from age 18 onwards, increases in age would not be expected to result in changes in height. This may change again, much later in life, when height might start to decrease with increasing age. More generally, one could consider any such relationship between an outcome and an explanatory variable in which the relationship between the predictor and outcome is not a straight line on the scale of the linear predictor. When this occurs, a one unit change in the explanatory variable will result in a varying change in the outcome that depends on where in the scale of the predictor the change was made.\nPiecwise regression is a way to model such structure where breakpoints are defined (usually arbitrarily) along the range of the explanatory variable, allowing the relationship with the outcome to vary. Formally, a piecewise regression model with \\(K\\) breakpoints can be defined as:\n\\[\n\\begin{align*}\ny_i = \\beta_0 + \\beta_1 x_i + \\sum_{k=1}^K \\delta_k (x_i - \\tau_k)_{+} + \\epsilon_i\n\\end{align*}\n\\]\nwhere\n\n\\(x\\) is a continuous predictor\n\\(\\tau_1, \\tau_2, \\dots, \\tau_K\\) are a set of ordered breakpoints\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope of the first segment\n\\(\\delta_k\\) is the change in the slope at the breakpoint \\(\\tau_{k-1} \\text{for segment } k = 2, \\dots, K + 1\\)\n\\(\\epsilon\\) is the error\n\nIf we assume \\(0 &lt; x &lt; 10\\) with \\(K = 3\\) breakpoints, at \\(x = 2\\), \\(x = 4.5\\) and \\(x = 8.5\\) then we get \\(K + 1 = 4\\) segments, i.e. \n\nSegment 1: \\(0 \\le x \\le 2\\) for which the slope is \\(\\beta_1\\)\nSegment 2: \\(2 &lt; x \\le 4.5\\) for which the slope is \\(\\beta_1 + \\delta_1\\)\nSegment 3: \\(4.5 &lt; x \\le 8.5\\) for which the slope is \\(\\beta_1 + \\delta_1 + \\delta_2\\)\nSegment 4: \\(x &gt; 8.5\\) for which the slope is \\(\\beta_1 + \\delta_1 + \\delta_2 + \\delta_3\\)\n\nThis model uses the basis function:\n\\[\n\\begin{align*}\n(x - \\tau)_{+} = max(0, x - \\tau)\n\\end{align*}\n\\]\nand these give zeros when \\(x \\le \\tau\\) and are linearly increasing when \\(x &gt; \\tau\\). The parameters associated with these basis functions influence the mean when \\(x_i &gt; \\tau_k\\) and \\(\\delta_k\\) represent the incremental change to the slope at the breakpoint \\(\\tau_k\\). This form of model gives a kink in the functional form of the association at each \\(x = \\tau_k\\) which allows the slope to change while ensuring continuity.\nFigure 1 shows the basis functions for \\(x\\) over the \\((0, 10)\\) interval. The basis functions are an essential concept in the construction of splines.\n\n\nPiecewise basis\nx &lt;- seq(0, 10, len = 100)\n\n# Breakpoints\ntau1 &lt;- 2\ntau2 &lt;- 4.5\ntau3 &lt;- 8.5\n\n# Construct basis functions\nx1 &lt;- pmax(0, x - tau1)\nx2 &lt;- pmax(0, x - tau2)\nx3 &lt;- pmax(0, x - tau3)\n\nd_fig &lt;- data.table(cbind(x = x, basis_1 = x1, basis_2 = x2, basis_3 = x3))\nd_fig &lt;- melt(d_fig, id.vars = \"x\")\n\nggplot(d_fig, aes(\n  x = x, y = value, group = variable, col = variable)) +\n  geom_line() +\n  scale_x_continuous(\"x\", breaks = 0:10) +\n  scale_y_continuous(\"f(x)\") +\n  scale_color_discrete(\"\") +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nFigure 1: Basis functions - piecewise regression"
  },
  {
    "objectID": "notebooks/splines-01.html#b-spline-to-p-splines",
    "href": "notebooks/splines-01.html#b-spline-to-p-splines",
    "title": "Splines 1",
    "section": "B-spline to P-splines",
    "text": "B-spline to P-splines\nSelecting some arbitrary values for the parameters, data can be simulated as shown in Figure 2 and these will be used throughout.\n\n\\(x\\) is a continuous predictor\n\\(\\tau_1 = 3\\)\n\\(\\tau_2 = 6\\)\n\\(\\beta_0 = 1\\)\n\\(\\beta_1 = 0.2\\)\n\\(\\delta_1 = 1.0\\)\n\\(\\delta_2 = -1.2\\) flatlines\n\\(\\delta_3 = -0.8\\)\n\\(\\sigma = 0.15\\) standard deviation for epsilon noise\n\n\n\nData simulation\nN &lt;- 250\n\nd_1 &lt;- data.table()\nd_1[, x := sort(runif(N, 0, 10))]\n\n# tau defined above\n# Construct basis for piecewise reg\nd_1[, u_1 := pmax(0, x - tau1)]\nd_1[, u_2 := pmax(0, x - tau2)]\nd_1[, u_3 := pmax(0, x - tau3)]\n\n# Parameters\nbeta0 &lt;- 1\nbeta1 &lt;- 0.2        # initial slope\ndelta1 &lt;- 1.0       # change in slope at tau1\ndelta2 &lt;- -1.2     # change in slope at tau2\ndelta3 &lt;- -0.8     # change in slope at tau2\nsigma &lt;- 0.15\n\n# Generate response\nd_1[, mu := beta0 + beta1 * x + delta1 * u_1 + delta2 * u_2 + delta3 * u_3]\nd_1[, y := rnorm(.N, mu, sigma) ]\n\nd_fig &lt;- copy(d_1)\n\nggplot(d_fig, aes(\n  x = x, y = y)) +\n  geom_point(size = 0.5) +\n  scale_x_continuous(\"x\", breaks = 0:10) +\n  scale_y_continuous(\"f(x)\") +\n  # scale_color_discrete(\"\") +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nFigure 2: Simulated data\n\n\n\n\n\nEstimating a regression function based on a piecewise specification is trivial but the lack of smoothness might be a concern in many settings. One might therefore want to opt for an approach that has smooth transitions across the range of \\(x\\) rather than the kinks. In doing so, we have made an assumption on the qualitative properties of the relationship, i.e. that the \\(f\\) from \\(y = f(x) + \\epsilon\\) is smooth.\nVariations of the parameterisation are possible, for example see https://stats.stackexchange.com/questions/570182/how-to-create-a-b-spline-basis-without-intercept-and-linear-trend-included.\nRoughly speaking, splines are piecewise polynomials that are continuously differentiable (i.e smooth) up to a certain degree and connected at a sequence of breakpoints called knots. In contrast to global polynomial models, which combine higher-order terms for \\(x\\) (i.e. \\(x^2\\), \\(x^3\\) etc) splines partition \\(x\\) into smaller intervals (as was does with the piecewise regression) and influence the fit locally, rather than globally.\nB-splines basis can be computed using an recursive algorithm, which is skipped here as the implementation is provided in several R packages. The basis matrix is formed as:\n\\[\n\\begin{align*}\nB = \\begin{bmatrix}\n  B_1(x_1) & B_2(x_1) & \\dots & B_q(x_1) \\\\\n  B_1(x_2) & B_2(x_2) & \\dots & B_q(x_2) \\\\\n  \\vdots   & \\vdots   & \\dots & \\vdots \\\\\n  B_1(x_n) & B_2(x_n) & \\dots & B_q(x_n) \\\\\na & b & c\n\\end{bmatrix}\n\\end{align*}\n\\]\nand then the mean of the fitted line is computed \\(\\mu = B \\gamma\\) (although other terms might be entered independently of the bais).\nAs an example, we might adopt a B-spline as shown in Figure 3 with four internal knots.\n\n\nB-spline basis functions\nx &lt;- seq(0, 10, len = 200)\n\n# Breakpoints\nknots &lt;- seq(1, 9, by = 2)\n\n# Construct basis functions\nd_B_ref &lt;- data.table(x = x, bSpline(x, knots = knots, intercept = F))\nnames(d_B_ref) &lt;- c(\"x\", paste0(\"g_\", 1:(ncol(d_B_ref)-1)))\n\nd_fig &lt;- melt(d_B_ref, id.vars = \"x\")\n\nggplot(d_fig, aes(\n  x = x, y = value, group = variable, col = variable)) +\n  geom_line() +\n  geom_vline(\n    data = data.table(k = knots),\n    aes(xintercept = k), lty = 2, lwd = 0.3\n  ) +\n  scale_x_continuous(\"x\", breaks = 0:10) +\n  scale_y_continuous(\"f(x)\") +\n  scale_color_discrete(\"\") +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nFigure 3: Basis functions - B-spline\n\n\n\n\n\nThe simulated data generated earlier can be used to form a B-spline basis matrix from the \\(x\\) values. The result can then be passed into a linear regression as the design matrix to obtain estimates for \\(\\gamma\\).\nIf we choose \\(K=\\) 5 knots and with functions of degree 3, we end up with 8 columns in the basis matrix (assuming the intercept was dropped from the basis and included independently in the model, as is done by default when using lm).\nTo predict the values for the fitted curve, we use the reference basis matrix that was created using sequential values of \\(x\\) over a range of interest and then run predict, see Figure 4 (A).\nThe number or placement of knots clearly has an impact on the nature of the fit and the amount of curvature that can be represented. In the toy example, we know the true data generation process and thus can see that the spline has overfit the data. If the number of knots is increased to 50, overfitting is further exacerbated as shown in Figure 4 (B). However, the extent the wiggliness depends not only on the number of columns in the basis matrix, but also the parameter values. Specifically, when the \\(\\gamma\\) parameters are highly erratic, there will be more movement in the fitted curves.\nThe main idea of P-splines (penalised B-splines) is to take a large basis matrix and then apply a penalty to some measure of roughness. For the penalty, differences (first or second) in neighbouring values of the parameters can be considered from which roughness can be computed as the sum of squares. In traditional least squares, the roughness is entered into the objective function, i.e.\n\\[\n\\begin{align*}\nO = (y - B \\gamma)^\\prime W (y - B \\gamma) + \\lambda || D \\alpha ||^2\n\\end{align*}\n\\]\nwhere \\(\\lambda\\) is a derived measure of roughness, \\(D\\) is a matrix difference operator and \\(W\\) corresponds to a set of regression weights.\n\n\nModel based on spline with 5 knots\n# Construct basis functions except exclude the intercept\nd_B &lt;- data.table(bSpline(d_1$x, knots = knots, intercept = F))\nnames(d_B) &lt;- paste0(\"g_\", 1:ncol(d_B))\n\nd_1 &lt;- cbind(d_1, d_B)\n\n# x has now been replaced entirely by the linear combination of basis\n# functions\n\nfmla_1 &lt;- as.formula(paste(\"y ~ \", paste(names(d_B), collapse = \"+\")))\nf1 &lt;- lm(fmla_1, data = d_1)\n\n# predict y for the new x (as represented by the reference basis)\nd_B_ref[, y_hat := predict(f1, newdata = d_B_ref)]\n\n# save the plot\np1 &lt;- ggplot(d_1, aes(x = x, y = y)) +\n  geom_point(size = 0.5) +\n  geom_line(\n    data = d_B_ref, aes(x = x, y = y_hat),\n    col = \"red\"\n  ) +\n  scale_x_continuous(\"x\", breaks = 0:10) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\nModel based on spline with 50 knots\nknots &lt;- seq(1, 9, len = 50)\n\nd_2 &lt;- copy(d_1[, .(x, mu, y)])\n\n# Construct basis functions except exclude the intercept\nd_B &lt;- data.table(bSpline(d_2$x, knots = knots, intercept = F))\nnames(d_B) &lt;- paste0(\"g_\", 1:ncol(d_B))\n\nd_2 &lt;- cbind(d_2, d_B)\n\n# x has now been replaced entirely by the linear combination of basis\n# functions\n\nfmla_1 &lt;- as.formula(paste(\"y ~ \", paste(names(d_B), collapse = \"+\")))\nf1 &lt;- lm(fmla_1, data = d_2)\n\nx &lt;- seq(0, 10, len = 200)\n\n# Recreate reference basis with 50 knots\nd_B_ref &lt;- data.table(x = x, bSpline(x, knots = knots, intercept = F))\nnames(d_B_ref) &lt;- c(\"x\", paste0(\"g_\", 1:(ncol(d_B_ref)-1)))\n\n# Predict outcome\nd_B_ref[, y_hat := predict(f1, newdata = d_B_ref)]\n\n# This was just out of interest as to where the values of the coefficients were\n# relative to the where the modes of the basis functions sit on x\nB_modes &lt;- apply(d_B_ref[, .SD, .SDcols = names(d_B_ref) %like% \"g_\"], 2, which.max)\n# d_B_ref$x[B_modes]\n\np2 &lt;- ggplot(d_2, aes(x = x, y = y)) +\n  geom_point(size = 0.5) +\n  geom_line(\n    data = d_B_ref, aes(x = x, y = y_hat),\n    col = \"red\"\n  ) +\n  # geom_point(\n  #   data = data.table(\n  #     x = d_B_ref$x[B_modes], \n  #     y = coef(f1)[-1]),\n  #   aes(x = x, y = y), pch = 2, size = 2\n  # ) +\n  scale_x_continuous(\"x\", breaks = 0:10) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\nRender plots\np1 + p2 +\n  patchwork::plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\nFigure 4: Simulated data and fitted splines with 5 knots (A) and 50 knots (B)"
  },
  {
    "objectID": "notebooks/splines-01.html#bayesian-variations",
    "href": "notebooks/splines-01.html#bayesian-variations",
    "title": "Splines 1",
    "section": "Bayesian variations",
    "text": "Bayesian variations\nA Bayesian analogue of the B-spline fit can is shown below and in absence of any penalty, the posterior mean of the fitted curve align closely to those that were obtained from least squares, see Figure 5 and parameter comparison. However, the results are sensitive to specification of the priors for the basis matrix parameters.\n\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  // basis (excludes intercept)\n  int Q;\n  matrix[N, Q] B;\n  \n  // for prediction over domain of x\n  int N_ref;\n  matrix[N_ref, Q] B_ref;\n  \n  \n  vector[2] prior_g;\n  int prior_only;\n}\nparameters {\n  real b0;\n  real&lt;lower=0&gt; s_e;\n  vector[Q] g;\n}\ntransformed parameters{\n  \n}\nmodel {\n  \n  target += normal_lpdf(b0 | 0, 2);\n  target += normal_lpdf(g | prior_g[1], prior_g[2]);\n  target += exponential_lpdf(s_e | 1);\n  if(!prior_only){\n    target += normal_lpdf(y | b0 + B * g, s_e);  \n  }\n  \n}\ngenerated quantities{\n  \n  vector[N_ref] mu = b0 + B_ref * g;\n  \n  \n}\n\n\n\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 1.2 seconds.\n\n\n\n\nComparing selection of frequentist and Bayesian model parameters\nr_1 &lt;- unname(round(c(\n  b0 = summary(f1)$coef[c(\"(Intercept)\"), 1],\n  s_e = summary(f1)$sigma,\n  g_1 = summary(f1)$coef[c(\"g_1\"), 1],\n  g_2 = summary(f1)$coef[c(\"g_2\"), 1],\n  g_3 = summary(f1)$coef[c(\"g_3\"), 1],\n  g_4 = summary(f1)$coef[c(\"g_4\"), 1]\n), 3))\n  \nr_2 &lt;- b1$summary(variables  = c(\"b0\", \"s_e\", \"g[1]\", \"g[2]\", \"g[3]\", \"g[4]\"), \"mean\")\n\npandoc.table(cbind(bayes = r_2, frequentist = r_1))\n\n\n\n-------------------------------------------\n bayes.variable   bayes.mean   frequentist \n---------------- ------------ -------------\n       b0           1.101         1.075    \n\n      s_e           0.1501        0.15     \n\n      g[1]         -0.2138       -0.173    \n\n      g[2]          0.4031        0.421    \n\n      g[3]          0.1481        0.177    \n\n      g[4]          0.2131        0.24     \n-------------------------------------------\n\n\n\n\nRender plots\np2 + p3 +\n  patchwork::plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\nFigure 5: Classical (A) vs Bayesian fit (B)"
  },
  {
    "objectID": "notebooks/splines-01.html#implementing-penalities",
    "href": "notebooks/splines-01.html#implementing-penalities",
    "title": "Splines 1",
    "section": "Implementing penalities",
    "text": "Implementing penalities\nIn a Bayesian context, a P-spline analogue would aim to replicate essential aspects of the approach to penalisation. One way to do achieve this is to induce a dependency over the set of spline parameters through a random walk prior. As the P-splines commonly use second differences on the parameters as the input to calculating a roughness measure, from which a derived value is used as a penalty, a second order random walk (RW2) prior is apt. Assuming equal spacing in the knots1 we can specify a RW2 using a difference representation, which closely maps to a relatively efficient implementation.\nRW2 assumes that the second differences on the latent process \\(\\theta_j\\) are IID normal:\n\\[\n\\begin{align*}\n\\delta_2 \\sim \\text{Normal}(0, \\tau_{\\delta_2})\n\\end{align*}\n\\]\nWhile it is possible to work directly with \\(\\theta\\) it is usually better to reconstruct the latent process via the differences, so define:\n\n\\(\\delta_1[k] = \\theta_k - \\theta_{k-1}\\) as the first order differences\n\\(\\delta_2[k] = \\delta_1[k] - \\delta_1[k-1]\\) as the first order differences\n\nthe second line expands to\n\\[\n\\begin{align*}\n\\delta_2[k] &= (\\theta_k - \\theta_{k-1}) - (\\theta_{k-1} - \\theta_{k-2}) \\\\\n  &= \\theta_k - 2 \\theta_{k-1} + \\theta_{k-2}\n\\end{align*}\n\\]\nIn the implementation, we draw starting values for \\(\\theta_1\\), \\(\\delta_1[1]\\) (indexing the first element of \\(\\delta_1\\) of length \\(Q-1\\)) and \\(\\delta_2\\) (which has length \\(Q-2\\)) from our priors and then we reconstruct the first differences as:\n\\[\n\\begin{align*}\n\\delta_1[1] &\\sim \\text{Normal}(0, \\tau_{\\delta_1}) \\\\\n\\delta_1[2] &= \\delta_1[1] + \\delta_2[1] \\\\\n\\delta_1[3] &= \\delta_1[2] + \\delta_2[2] \\\\\n&\\vdots  \\\\\n\\delta_1[Q-1] &= \\delta_1[Q-2] + \\delta_2[Q-2] \\\\\n\\end{align*}\n\\]\nand then reconstruct \\(\\theta\\) as:\n\\[\n\\begin{align*}\n\\theta[1] &\\sim \\text{Normal}(0, \\sigma_\\theta) \\\\\n\\theta[2] &= \\theta[1] + \\delta_1[1] \\\\\n\\theta[3] &= \\theta[2] + \\delta_1[2] \\\\\n& \\quad \\vdots  \\\\\n\\theta[Q] &= \\theta[Q-1] + \\delta_1[Q-1] \\\\\n\\end{align*}\n\\]\nThe stan implementation and fitted results are shown below, Figure 6. The model takes a quite a while to fit under MCMC, but addresses some of the overfitting issues, even though we continue to fit the model based using a basis matrix setup over 50 knots. Accepting the fact that for this data, 50 knots is excessive, the point is that we can effectively reduce the effective dimension of the model via the RW2 prior.\n\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  // basis (excludes intercept)\n  int Q;\n  matrix[N, Q] B;\n  \n  // for prediction over domain of x\n  int N_ref;\n  matrix[N_ref, Q] B_ref;\n  \n  int prior_only;\n}\nparameters {\n  real b0;\n  real&lt;lower=0&gt; s_e;\n  // offsets\n  real z0;\n  real z1;\n  vector[Q-2] delta2;   \n  real&lt;lower=0&gt; s_d;\n}\ntransformed parameters{\n  vector[Q] g;\n  vector[Q-1] delta1;\n  \n  g[1] = z0;\n  delta1[1] = z1;\n  \n  for(i in 2:(Q-1)){\n    delta1[i] = delta1[i-1] + delta2[i-1] * s_d;\n  }\n  \n  for(i in 2:Q){\n    g[i] = g[i-1] + delta1[i-1];\n  }\n  \n}\nmodel {\n  \n  target += normal_lpdf(b0 | 0, 10);\n  \n  // standard normal prior on the offsets\n  target += normal_lpdf(z0 | 0, 1); \n  target += normal_lpdf(z1 | 0, 1); \n  target += normal_lpdf(delta2 | 0, 1);  \n  // prior on sd for rw\n  target += exponential_lpdf(s_d | 1);  \n  \n  \n  target += exponential_lpdf(s_e | 1);\n  if(!prior_only){\n    target += normal_lpdf(y | b0 + B * g, s_e);  \n  }\n  \n}\ngenerated quantities{\n  \n  vector[N_ref] mu = b0 + B_ref * g;\n  \n  \n}\n\n\n\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 78.5 seconds.\n\n\n\n\nRender plots\np3 + p4 +\n  patchwork::plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\nFigure 6: Unpenalised (A) vs Penalised (RW2) fit (B)"
  },
  {
    "objectID": "notebooks/splines-01.html#imposing-sparsity-on-deviations",
    "href": "notebooks/splines-01.html#imposing-sparsity-on-deviations",
    "title": "Splines 1",
    "section": "Imposing sparsity on deviations",
    "text": "Imposing sparsity on deviations\nA further restriction might be imposed on the deviations to induce sparsity where parameters as implemented via a mixture of normals.\nThe first order difference components from the RW2 deviations, used to give the skeleton for the fitted curve, are shown in Figure 7 for the RW2 model and the RW2 with sparsity models. While the sparsity is applied to the second order differences, the approach has induced sparsity in the first order differences, whereby low magnitude shifts have been brought closer to zero and their uncertainty reduced.\nThe approach is shown in the implementation below and can help avoid wiggliness in parts of the domain where the relationship is flat, Figure 8.\n\n\nfunctions {\n  real spike_slab_lpdf(real delta, real pi, real sigma1, real sigma2) {\n    real log_p1 = log1m(pi) + normal_lpdf(delta | 0, sigma1);\n    real log_p2 = log(pi) + normal_lpdf(delta | 0, sigma2);\n    return log_sum_exp(log_p1, log_p2);\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  // basis (excludes intercept)\n  int Q;\n  matrix[N, Q] B;\n  \n  // for prediction over domain of x\n  int N_ref;\n  matrix[N_ref, Q] B_ref;\n  \n  int prior_only;\n}\nparameters {\n  real b0;\n  real&lt;lower=0&gt; s_e;\n  // offsets\n  real z0;\n  real z1;\n  vector[Q-2] delta2;   \n  real&lt;lower=0, upper=1&gt; pi; // mixture weight\n  real&lt;lower=0&gt; s_spike;\n  real&lt;lower=0&gt; s_slab;\n  \n}\ntransformed parameters{\n  \n  vector[Q] g;\n  vector[Q-1] delta1;\n  \n  g[1] = z0;\n  delta1[1] = z1;\n  \n  for(i in 2:(Q-1)){\n    // delta2 has already been scaled via the horseshoe above\n    delta1[i] = delta1[i-1] + delta2[i-1];\n  }\n  \n  for(i in 2:Q){\n    g[i] = g[i-1] + delta1[i-1];\n  }\n  \n}\nmodel {\n  \n  target += normal_lpdf(b0 | 0, 10);\n  \n  // standard normal prior on the offsets\n  target += normal_lpdf(z0 | 0, 1); \n  target += normal_lpdf(z1 | 0, 1); \n  \n  // mixing prob\n  target += beta_lpdf(pi | 1, 1);\n  target += exponential_lpdf(s_spike | 1); \n  target += exponential_lpdf(s_slab | 1); \n\n  for (i in 1:(Q - 2)) {\n    target += spike_slab_lpdf(delta2[i] | pi, s_spike, s_slab);\n  }\n  \n  target += exponential_lpdf(s_e | 1);\n  if(!prior_only){\n    target += normal_lpdf(y | b0 + B * g, s_e);  \n  }\n  \n}\ngenerated quantities{\n  \n  vector[N_ref] mu = b0 + B_ref * g;\n  \n  \n}\n\n\n\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 79.5 seconds.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.3.\nSee https://mc-stan.org/misc/warnings for details.\n\n\n# A tibble: 3 × 10\n  variable   mean  median    sd     mad       q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 s_spike  0.0434 0.00405 0.174 0.00457 0.000598 0.171  1.51     1.92     13.7\n2 s_slab   0.163  0.140   0.127 0.0732  0.0184   0.345  1.10    10.4      15.4\n3 pi       0.236  0.150   0.238 0.0903  0.0445   0.872  1.11    13.4      16.6\n\n\n\n\nRender plots\nd_g &lt;- data.table(b3$draws(variable = \"delta1\", format = \"matrix\"))\nd_g &lt;- melt(d_g, measure.vars = names(d_g))\nd_fig_1 &lt;- d_g[, .(mu = mean(value),\n                 q_025 = quantile(value, prob = 0.025),\n                 q_975 = quantile(value, prob = 0.975)), \n             keyby = .(variable)]\nd_fig_1[, x := 1:.N]\n\nd_g &lt;- data.table(b5$draws(variable = \"delta1\", format = \"matrix\"))\nd_g &lt;- melt(d_g, measure.vars = names(d_g))\nd_fig_2 &lt;- d_g[, .(mu = mean(value),\n                 q_025 = quantile(value, prob = 0.025),\n                 q_975 = quantile(value, prob = 0.975)), \n             keyby = .(variable)]\nd_fig_2[, x := 1:.N]\n\nd_fig &lt;- rbind(\n  cbind(model = \"RW2\", d_fig_1),\n  cbind(model = \"RW2 + sparsity\", d_fig_2)\n)\nd_fig[, variable := gsub(\"delta1\", \"d1\", variable)]\nd_fig[, variable := factor(\n  variable, \n  levels = paste0(\"d1[\", 1:length(unique(d_fig$variable)), \"]\"))]\n\nggplot(d_fig, aes(x = model, y = mu, col = model)) +\n  geom_linerange(aes(ymin = q_025, ymax = q_975), \n                 position = position_dodge2(width = 1), lwd = 0.5) +\n  geom_point(position = position_dodge(width = 1), size = 0.6) +\n  # scale_x_continuous(\"\") +\n  scale_y_continuous(\"\") +\n  scale_colour_discrete(\"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\",\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()\n        ) +\n  ggh4x::facet_wrap2(~ variable, scales = \"free_y\", nrow = 8)\n\n\n\n\n\n\n\n\nFigure 7: First differences from penalised (RW2) fit vs RW2 with sparsity\n\n\n\n\n\n\n\nRender plots\np4 + p5 +\n  patchwork::plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\nFigure 8: Penalised (RW2) (A) vs Penalised (RW2) with sparsity (B)"
  },
  {
    "objectID": "notebooks/splines-01.html#other-perspectives",
    "href": "notebooks/splines-01.html#other-perspectives",
    "title": "Splines 1",
    "section": "Other perspectives",
    "text": "Other perspectives\nThere are many other approaches to spline-based modelling. For example, it is common to convert a spline model into an equivalent mixed model form. This approach imposes a shared distribution on the basis matrix parameters. An alternative to the B-spline is a low-rank thin plate spline which uses a radial basis. I believe this is the default approach in brms and in mgcv."
  },
  {
    "objectID": "notebooks/splines-01.html#footnotes",
    "href": "notebooks/splines-01.html#footnotes",
    "title": "Splines 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is important caveat and the approach needs to be changed somewhat if equal spacing is not possible.↩︎"
  },
  {
    "objectID": "notebooks/random-walk-prior.html",
    "href": "notebooks/random-walk-prior.html",
    "title": "Random walk priors",
    "section": "",
    "text": "First order random walk\nFor regular spacings, a first-order random walk prior can be specified as:\n\\[\n\\begin{aligned}\n\\eta_0 &\\sim \\text{Logistic}(0,1) \\\\\n\\delta &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma_\\delta &\\sim \\text{Exponential}(1) \\\\\n\\eta_{[1]} &= \\eta_0 \\\\\n\\eta_{[k]} &= \\sum_{i = 2}^{N}(\\eta_{[k-1]}  + \\delta  \\sigma_\\delta) \\\\\n\\end{aligned}\n\\]\nSimulate data from an oscillator:\n\nlibrary(data.table)\nlibrary(ggplot2)\n\nset.seed(2)\nd_obs &lt;- data.table(\n  x = sort(runif(100, 0, 2*pi))\n)\nd_obs[, eta := sin(x)]\nd_obs[, n := rpois(.N, 200)]\nd_obs[, y := rbinom(.N, n, plogis(eta))]\n\n# we only observe 30% of the data generated\nd_obs[, y_mis := rbinom(.N, 1, 0.7)]\n\nNaive implementation of a first order random walk in stan.\n\n\ndata {    \n  int N; \n  // the way the model is set up it does not matter if some of the n's are\n  // zero because the likelihood uses y_sub, which is obtained by reference\n  // to the missing indicator y_mis, which explicitly says that there were\n  // no observations at the given value of x.\n  array[N] int y;    \n  array[N] int n;    \n  vector[N] x;    \n  array[N] int y_mis; \n  \n  int prior_only;    \n  \n  // priors\n  real r_nu;\n  \n}    \ntransformed data {\n  // x_diff gives us the variable spacing in x and allows us to scale\n  // the variance appropriately\n  vector[N-1] x_diff;\n  // the number of observations we truly had once missingness is accounted for\n  int N_sub = N - sum(y_mis);\n  // our truly observed responses (successes) and trials\n  array[N_sub] int y_sub;\n  array[N_sub] int n_sub;\n  // \n  for(i in 1:(N-1)){x_diff[i] = x[i+1] - x[i];}\n  // go through the data that was passed in and build the data on which \n  // we will fit the model\n  int j = 1;\n  for(i in 1:N){\n    if(y_mis[i] == 0){\n      y_sub[j] = y[i];\n      n_sub[j] = n[i];\n      j += 1;\n    }\n  }  \n}\nparameters{  \n  // the first response\n  real b0;    \n  // offsets\n  vector[N-1] delta;    \n  // how variable the response is\n  real&lt;lower=0&gt; nu;   \n}    \ntransformed parameters{    \n  // the complete modelled mean response\n  vector[N] e; \n  // this is the variance scaled for the distance between each x\n  // note this is truly a variance and not an sd\n  vector[N-1] tau;    \n  // \n  vector[N_sub] eta_sub;    \n  // adjust the variance for the distance b/w doses    \n  // note that nu is squared to turn it into variance\n  for(i in 2:N){tau[i-1] = x_diff[i-1]*pow(nu, 2);}    \n  // resp is random walk with missingness filled in due to the \n  // dependency in the prior\n  e[1] = b0;    \n  // each subsequent observation has a mean equal to the previous one\n  // plus some normal deviation with mean zero and variance calibrated for\n  // the distance between subsequent observations.\n  for(i in 2:N){e[i] = e[i-1] + delta[i-1] * sqrt(tau[i-1]);}    \n  // eta_sub is what gets passed to the likelihood\n  { \n    int k = 1;\n    for(i in 1:N){\n      if(y_mis[i] == 0){\n        eta_sub[k] = e[i];\n        k += 1;\n      }\n    }\n  }\n}    \nmodel{    \n  // prior on initial response\n  target += logistic_lpdf(b0 | 0, 1);\n  // prior on sd\n  target += exponential_lpdf(nu | r_nu);\n  // standard normal prior on the offsets\n  target += normal_lpdf(delta | 0, 1);    \n  if(!prior_only){target += binomial_logit_lpmf(y_sub | n_sub, eta_sub);}    \n}    \ngenerated quantities{    \n  // predicted values at each value of x\n  vector[N] p;    \n  vector[N-1] e_diff;    \n  vector[N-1] e_grad;    \n  // compute diffs\n  for(i in 1:(N-1)){e_diff[i] = e[i+1] - e[i];}\n  e_grad = e_diff ./ x_diff;\n  p = inv_logit(e);\n}    \n\n\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/random-walk-01.stan\")\n\n\nld = list(\n  N = nrow(d_obs), \n  y = d_obs[, y], \n  n = d_obs[, n],\n  x = d_obs[, x], \n  y_mis = d_obs[, y_mis], \n  prior_only = F, \n  r_nu =  3\n  )\n\n\nf1 &lt;- m1$sample(\n  ld, iter_warmup = 1000, iter_sampling = 2000,\n  parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = F,\n  max_treedepth = 10)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 3.0 seconds.\n\nf1$summary(variables = c(\"nu\"))\n\n# A tibble: 1 × 10\n  variable  mean median    sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 nu       0.533  0.523 0.100 0.0962 0.390 0.710  1.00    1298.    1384.\n\n\nRepresentation of output.\n\nd_out &lt;- data.table(f1$draws(variables = \"p\", format = \"matrix\"))\n\nd_fig &lt;- melt(d_out, measure.vars = names(d_out))\nd_fig &lt;- d_fig[, .(\n  mu = mean(value), \n  q_025 = quantile(value, prob = 0.025),\n  q_975 = quantile(value, prob = 0.975)\n), keyby = variable]\nd_fig[, ix := gsub(\"p[\", \"\", variable, fixed = T)]\nd_fig[, ix := as.numeric(gsub(\"]\", \"\", ix, fixed = T))]\nd_fig[, x := d_obs[ix, x]]\n\n\nggplot(d_obs, aes(x = x, y = plogis(eta))) +\n  geom_line(lty = 1) +\n  geom_point(data = d_obs[y_mis == 0],\n             aes(x = x, y = y/n), size = 0.7) +\n  geom_point(data = d_obs[y_mis == 1],\n             aes(x = x, y = y/n), size = 0.7, pch = 2) +\n  geom_ribbon(data = d_fig, \n              aes(x = x, ymin = q_025, ymax = q_975),\n              inherit.aes = F, fill = 2, alpha = 0.3) +\n  geom_line(data = d_fig, \n              aes(x = x, y = mu), col = 2) +\n  geom_point(data = d_fig, \n              aes(x = x, y = mu), col = 2, size = 0.6) +\n  scale_x_continuous(\"x\") +\n  scale_y_continuous(\"Probability\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 1: True function (black line), data on which the model was fit (black points), data we did not see (black triangles), random walk (red line) with interpolated points (red) and uncertainty (red ribbon).\n\n\n\n\n\n\n\nSecond order random walk\nThe second order random walk for regular locations has density\n\\[\n\\begin{aligned}\n\\pi(x) \\propto \\exp\\left( -\\frac{1}{2} \\sum_{i=2}^{n-1} (x_{i-1} - 2x_i + x_{i+1})^2  \\right)\n\\end{aligned}\n\\]\nThe main term can be interpreted as an estimate of the second order derivative of a continuous time function. But this is not generally suitable for irregular spacings of x [1].\n\n\nReferences\n\n\n1. Lindgren F, Rue H. On the second-order random walk model for irregular locations. Scandinavian Journal of Statistics. 2008;35:691–700."
  },
  {
    "objectID": "notebooks/principal-stratification.html",
    "href": "notebooks/principal-stratification.html",
    "title": "Principal Stratification",
    "section": "",
    "text": "We routinely consider pre-treatment adjustment, however, post-treatment confounding is not so often considered. Post-treatment confounding arises as a result of intermediate variables \\(D\\) that lie on the pathway between the treatment \\(Z\\) and the outcome \\(Y\\).\nAdjusting analyses for these post-treatment variables in the same manner as one would do for baseline covariates can lead to biased effect estimates. Non-compliance is one example of an intermediate variable where this can occur.\nTypically, \\(Z\\) (assignment) strongly influence \\(D\\) but \\(D \\ne Z\\) for some (or many) units. Given that units with \\(Z=1, D = d\\) are sometimes not the same as units with \\(Z=0, D = d\\) direct comparisons can be problematic. For example, the units that do not comply when assigned to treatment may not be the same as units that do not comply when assigned to control. This could arise if, say, the units that do not comply when assigned to treatment are sicker than average and cannot accommodate the side effects on the treatment arm, which do not occur so readily on the control arm.\nThe ideas for principal stratification were popularised in the paper by Frangakis and Rubin [1]. They generalised the instrument variable approach. Other references include [2], [3], [4], [5].\nThe key concept is that principal strata can be conceptualised and these are not affected by treatment assignment. That is, the units can be group in terms of joint potential outcomes for \\(D\\) and membership in any stratum only reflects a subjects characteristics, which are defined pre-treatment. Within stratum comparisons are thus well defined causal effects.\nThese strata are defined differently dependent on the setting - take noncompliance as an example. Define \\(D_i(z)\\) as the unit level potential outcome for the intermediate variable given assignment to treatment arm \\(z\\) for \\(z=0,1\\). \\(D_i(z) = 0\\) if subject \\(i\\) received control given assignment to \\(z\\) \\(D_i(z) = 1\\) if subject \\(i\\) received treatment given assignment to \\(z\\)\n\nnever takers = \\(\\{i: D_i(0) = 0, D_i(1) = 0 \\}\\)\ndefiers = \\(\\{i: D_i(0) = 1, D_i(1) = 0 \\}\\)\ncompliers = \\(\\{i: D_i(0) = 0, D_i(1) = 1 \\}\\)\nalways takers = \\(\\{i: D_i(0) = 1, D_i(1) = 1 \\}\\)\n\nthe complier average causal effect is then\n\\[\n\\tau^{\\text{CACE}} = \\mathbb{E}[ Y_i(1) - Y_i(0) | D_i(0) = 0, D_i(1) = 1  ]\n\\]\nWhile this is one set of PS that might be associated with the case of non-compliance, the definitions differ dependent on the situation. Centrally, a PS has the goal of characterising treatment effect heterogeneity across different subpopulations.\n\n\n\n\nTable 1: Composition of principal strata for non-compliance setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssigned (Z)\n\nReceived (D)\n\n\n\nControl (D = 0)\nTreatment (D = 1)\n\n\n\n\nControl (Z = 0)\nnever-takers and compliers\nalways-takers and defiers\n\n\nTreatment (Z = 1)\nnever-takers and defiers\nalways-takers and compliers\n\n\n\n\n\n\n\n\n\n\nThe issue with PS is that what we observe constitute latent mixtures of units rather than specific strata as shown in Table 1. From the above example, of those people who we assign and observe to take the control assignment, we do not know which units are those that would take control regardless of what their assignment was versus which units are the compliers. That is, the specific strata are not observed and therefore additional assumptions are required to estimate principal causal effects.\nThe assumption we absolutely need are unconfounded assignment. That is, the potential outcomes for \\(Y\\) and \\(D\\) are independent of treatment assignment conditional on covariates. This implies that membership in a strata has the same distribution between treatment arms. Without this, there is no way to move forward.\nAdditional assumptions that sharpen identification are monotonicity and exclusion restriction.\nMonotonicity rules out certain components of a mixture that contributes to a given strata. For example, we rule out the possibility of defiers.\nExclusion restriction rules out any direct effect from treatment not mediated through the actual treatment among never-takers and always-takers.\nWhile we can estimate the CACE in the simplest case from the identiability assumptions, we can attempt to resolve the strata in a more comprehensive way by adopting a latent mixture model approach, as explicated by Liu and Li [6].\nFor each unit there exists \\((Y_i(1), Y_i(0), D_i(1), D_i(0), \\mathbf{X_i}, Z_i)\\) but we only observe \\(Y\\) based on the assigned treatment and \\(D\\) under that treatment. Therefore, strata membership, \\(S_i = (D_i(0), D_i(1))\\), is unobserved.\nTo proceed, we need a model for the strata membership \\(S\\) and the outcome \\(Y\\). For example, the \\(S\\) model can take the form of a multinomial model of some form and the outcome might be a GLM. The full likelihood decomposes into an S-model (a principal strata model given the covariates) and a Y-model (an outcome model given the stratum, covariates and treatment). This yields:\n\\[\n\\begin{aligned}\nl(\\theta) \\propto \\prod_{i=1}^n \\left(  \\sum_{s \\in \\mathcal{S}:D_i=D(s,Z_i)}  \\text{Pr}(S_i = s | X_i, \\theta)  \\text{Pr}(Y_i | S_i = s, Z_i, X_i, \\theta) \\right)\n\\end{aligned}\n\\]\nfor the \\(i = 1, \\dots n\\) units in the sample where \\(\\mathcal{S}\\) is the set of all PS and \\(D(s, z)\\) denotes the actual treatment \\(D_i\\) induced by PS \\(S_i = s\\) and assigned treatment \\(Z_i = z\\), i.e. a product of multiple components:\n\\[\n\\begin{aligned}\nl(\\theta) &\\propto \\prod_{i:Z_i=0,D_i=0} (\\pi_{i,c}f_{i,c0} + \\pi_{i,n}f_{i,n0}) \\times \\prod_{i:Z_i=0,D_i=1} (\\pi_{i,a}f_{i,a0} + \\pi_{i,d}f_{i,d0}) \\\\\n\\quad &\\times \\prod_{i:Z_i=1,D_i=0} (\\pi_{i,n}f_{i,n1} + \\pi_{i,d}f_{i,d1}) \\times \\prod_{i:Z_i=1,D_i=1} (\\pi_{i,a}f_{i,a1} + \\pi_{i,c}f_{i,c1})\n\\end{aligned}\n\\]\nwhere \\(f_{i,sz} = \\text{Pr}(Y_i | S_i = s, Z_i, \\mathbf{X}_i, \\theta)\\) and \\(\\pi_{i,s} = \\text{Pr}(S_i = s | \\mathbf{X}_i, \\theta)\\) and where the \\(c,n,a,d\\) denote the compliers, never-takeres, always-takers and defiers (for the noncompliance case considered here).\nFor any given strata, we can write the PCE based on the iterated expectations:\n\\[\n\\begin{aligned}\n\\tau_s = \\mathbb{E}[  \\mathbb{E}[Y_i | Z_i = 1, S_i = s, X_i] | S_i = s] - \\mathbb{E}[  \\mathbb{E}[Y_i | Z_i = 0, S_i = s, X_i] | S_i = s]\n\\end{aligned}\n\\]\nIf we let \\(g_{z,s}(x;\\theta) = \\mathbb{E}[ Y_i | Z_i = z, S_i = s, X_i = x, \\theta]\\) and \\(p_s(x;\\theta) = \\text{Pr}(S_i = s | X_i = x, \\theta)\\) from the Y-model and S-model respectively, then the PCE can be computed from the posterior as:\n\\[\n\\begin{aligned}\n\\hat{\\tau}_s (\\theta) = \\frac{ \\sum_{i=1}^n g_{1,s}(X_i;\\theta) p_s(X_i; \\theta)   }{\\sum_i=1^n  p_s(X_i; \\theta)}  - \\frac{ \\sum_{i=1}^n g_{0,s}(X_i;\\theta) p_s(X_i; \\theta)   }{\\sum_i=1^n  p_s(X_i; \\theta)}\n\\end{aligned}\n\\]\nAssuming that \\(\\theta_k\\) are samples from the posterior distribution for \\(\\theta\\) then these can be plugged into the above to approximate the distribution of \\(\\tau_s\\).\n\nImplementation\nAn implementation of the combined S-model and Y-model is shown below for the case of a binary outcome and a binary treatment with a binary intermediate variable. The model assumes neither monotonicity hence and exclusion restriction.\n\n\ndata {\n    int&lt;lower=0&gt; N; // number of observations\n    int&lt;lower=0&gt; PS; // number of predictors for principal stratum model\n    int&lt;lower=0&gt; PG; // number of predictors for outcome model\n    int&lt;lower=0, upper=1&gt; Z[N]; // treatment arm\n    int&lt;lower=0, upper=1&gt; D[N]; // post randomization confounding variable\n    int&lt;lower=0, upper=1&gt; Y[N]; // binary outcome\n    matrix[N, PS] XS; // model matrix for principal stratum model\n    matrix[N, PG] XG; // model matrix for outcome model\n}\ntransformed data {\n    int S[8];\n   S[1] = 1;\n   S[2] = 1;\n   S[3] = 2;\n   S[4] = 2;\n   S[5] = 3;\n   S[6] = 3;\n   S[7] = 4;\n   S[8] = 4;\n}\n \nparameters {\n    matrix[3, PS] beta_S; // coefficients for principal stratum model\n    matrix[8, PG] beta_G; // coefficients for outcome model\n}\ntransformed parameters {\n}\nmodel {\n    // random effect\n    // prior\n    if (PS &gt;= 2)\n        to_vector(beta_S[:, 2:PS]) ~ normal(0, 1);\n    if (PG &gt;= 2)\n        to_vector(beta_G[:, 2:PG]) ~ normal(0, 1);\n    // model\n    for (n in 1:N) {\n        int length;\n        real log_prob[4];\n        log_prob[1] = 0;\n        for (s in 2:4) {\n            log_prob[s] = XS[n] * beta_S[s-1]';\n        }\n        if (Z[n] == 0 && D[n] == 0)\n            length = 2;\n        else if (Z[n] == 1 && D[n] == 0)\n            length = 2;\n        else if (Z[n] == 1 && D[n] == 1)\n            length = 2;\n        else if (Z[n] == 0 && D[n] == 1)\n            length = 2;\n        {\n            real log_l[length];\n            if (Z[n] == 0 && D[n] == 0) {\n                // Z:0 D:0 S:0/1\n                log_l[1] = log_prob[1] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[1]'));\n                log_l[2] = log_prob[2] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[3]'));\n            }\n            else if (Z[n] == 1 && D[n] == 0) {\n                // Z:1 D:0 S:0/2\n                log_l[1] = log_prob[1] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[2]'));\n                log_l[2] = log_prob[3] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[6]'));\n            }\n            else if (Z[n] == 1 && D[n] == 1) {\n                // Z:1 D:1 S:1/3\n                log_l[1] = log_prob[2] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[4]'));\n                log_l[2] = log_prob[4] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[8]'));\n            }\n            else if (Z[n] == 0 && D[n] == 1) {\n                // Z:0 D:1 S:2/3\n                log_l[1] = log_prob[3] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[5]'));\n                log_l[2] = log_prob[4] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[7]'));\n            }\n            target += log_sum_exp(log_l) - log_sum_exp(log_prob);\n        }\n    }\n}\ngenerated quantities {\n    vector[4] strata_prob; // the probability of being in each stratum\n    vector[8] mean_effect; // mean response\n    {\n        matrix[N, 4] log_prob;\n        vector[8] numer;\n        matrix[N, 8] expected_mean;\n        for (i in 1:N)\n            for (j in 1:8)\n                expected_mean[i, j] = inv_logit(XG[i] * beta_G[j]');\n        log_prob[:, 1] = rep_vector(0, N);\n        log_prob[:, 2:4] = XS * beta_S';\n        for (n in 1:N) {\n            log_prob[n] -= log_sum_exp(log_prob[n]);\n        }\n        for (s in 1:4) strata_prob[s] = mean(exp(log_prob[:, s]));\n        for (g in 1:8) {\n            numer[g] = mean(expected_mean[:, g] .* exp(log_prob[:, S[g]]));\n            mean_effect[g] = numer[g] / strata_prob[S[g]];\n        }\n    }\n}\n\n\nIn the simplest case, where we have no \\(X\\) design matrix, i.e. just intercepts, the observed likelihood is\n\\[\n\\begin{aligned}\nL = p(Z, D, Y) = p(Z) \\sum_s p(S = s) p(D | Z, S = s) p(Y | D, Z, S = s)\n\\end{aligned}\n\\]\nHowever, the \\(p(D | Z, S = s)\\) part is either 0 or 1, depending on whether \\((D, Z, S)\\) are consistent with each other or not. This results in a simplification to:\n\\[\n\\begin{aligned}\nL \\propto \\sum_{s \\text{ consistent with } Z \\text{ and } D} p(S=s)p(Y | Z, S = s)\n\\end{aligned}\n\\]\nwhich is what is happening in the conditional elements of the model block.\nSpecifically, the linear predictors for the multinomial S-model are implemented as the XS[n] * beta_S[s-1]' code. With a single column in the XS design matrix, an implicit uniform prior is being placed on the intercept term, being defined as the log-probability (actually it is the log-odds). Note that the reference category is set to zero for identifiability.\nFor each unit, the combination of the treatment assignment and the occurrence of the intermediate event dictate whether there are one or two contributions to the log-likelihood corresponding to the two cohorts that make up the units within each strata for this example. Without the monotonicity and exclusion restriction assumptions, all combinations result in contributions.\nFor example, when control is the assignment and the received intervention is the control, there are contributions from both the never-takes (log_prob[1]) and the compliers (log_prob[2]). Similarly, when treatment is the assignment and the received intervention is the control, there are contributions from the never-takers (again the log_prob[1]) and the defiers (log_prob[3]) and so on.\nThe two assumptions (monotonicity/ER) can be introduced to the model as follows:\n\n\ndata {\n    int&lt;lower=0&gt; N; // number of observations\n    int&lt;lower=0&gt; PS; // number of predictors for principal stratum model\n    int&lt;lower=0&gt; PG; // number of predictors for outcome model\n    array[N] int&lt;lower=0, upper=1&gt; Z; // treatment arm\n    array[N] int&lt;lower=0, upper=1&gt; D; // post randomization confounding variable\n    array[N] int&lt;lower=0, upper=1&gt; Y; // binary outcome\n    matrix[N, PS] XS; // model matrix for principal stratum model\n    matrix[N, PG] XG; // model matrix for outcome model\n}\n \ntransformed data {\n   array[4] int S;\n   S[1] = 1;\n   S[2] = 2;\n   S[3] = 2;\n   S[4] = 3;\n}\n \nparameters {\n    matrix[2, PS] beta_S; // coefficients for principal stratum model\n    matrix[4, PG] beta_G; // coefficients for outcome model\n}\n \ntransformed parameters {\n}\n \nmodel {\n    // random effect\n    // prior\n    \n    // use informative prior for intercepts\n    beta_S[:, 1] ~ normal(0, 2);\n    beta_G[:, 1] ~ normal(0, 2);\n    \n    if (PS &gt;= 2)\n        to_vector(beta_S[:, 2:PS]) ~ normal(0, 1);\n    if (PG &gt;= 2)\n        to_vector(beta_G[:, 2:PG]) ~ normal(0, 1);\n    // model\n    for (n in 1:N) {\n        int length;\n        array[3] real log_prob;\n        log_prob[1] = 0;\n        for (s in 2:3) {\n            log_prob[s] = XS[n] * beta_S[s-1]';\n        }\n        if (Z[n] == 0 && D[n] == 0)\n            length = 2;\n        else if (Z[n] == 1 && D[n] == 0)\n            length = 1;\n        else if (Z[n] == 1 && D[n] == 1)\n            length = 2;\n        else if (Z[n] == 0 && D[n] == 1)\n            length = 1;\n        {\n            array[length] real log_l;\n            if (Z[n] == 0 && D[n] == 0) {\n                // Z:0 D:0 S:0/1 never takers or compliers\n                log_l[1] = log_prob[1] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[1]'));\n                log_l[2] = log_prob[2] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[2]'));\n            }\n            else if (Z[n] == 1 && D[n] == 0) {\n                // Z:1 D:0 S:0 never takers (defiers don't exist)\n                log_l[1] = log_prob[1] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[1]'));\n            }\n            else if (Z[n] == 1 && D[n] == 1) {\n                // Z:1 D:1 S:1/2 compliers or always takers\n                log_l[1] = log_prob[2] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[3]'));\n                log_l[2] = log_prob[3] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[4]'));\n            }\n            else if (Z[n] == 0 && D[n] == 1) {\n                // Z:0 D:1 S:2 always takers\n                log_l[1] = log_prob[3] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[4]'));\n            }\n            target += log_sum_exp(log_l) - log_sum_exp(log_prob);\n        }\n    }\n}\n \ngenerated quantities {\n    vector[3] strata_prob; // the probability of being in each stratum\n    vector[4] mean_effect; // mean response\n    {\n        matrix[N, 3] log_prob;\n        vector[4] numer;\n        matrix[N, 4] expected_mean;\n        for (i in 1:N)\n            for (j in 1:4)\n                expected_mean[i, j] = inv_logit(XG[i] * beta_G[j]');\n        log_prob[:, 1] = rep_vector(0, N);\n        log_prob[:, 2:3] = XS * beta_S';\n        for (n in 1:N) {\n            log_prob[n] -= log_sum_exp(log_prob[n]);\n        }\n        for (s in 1:3) strata_prob[s] = mean(exp(log_prob[:, s]));\n        for (g in 1:4) {\n            numer[g] = mean(expected_mean[:, g] .* exp(log_prob[:, S[g]]));\n            mean_effect[g] = numer[g] / strata_prob[S[g]];\n        }\n    }\n}\n\n\nwhere for certain combinations of \\(Z\\) and \\(D\\) only one group is assumed to contribute. Specifically, for the monotonicity assumption the set of possible strata is restricted. Units are assumed to never be defiers and therefore for the cases where \\((Z=1,D=0)\\) and \\((Z=0,D=1)\\) the contribution to the Y-model is one-dimensional.\nAdditionally, an informative prior was placed on the intercepts in the above model.\nFor the exclusion restriction assumption, the number of non-zero effects is truncated, reducing the number of free parameters. This happens because we are assuming that the strata of units that never take the treatment are unable to show a treatment effect and similarly so for those that always take treatment.\nFinally, the model section increments the target via the log_sum_exp calculations for the contributions of all units. These effectively map to\n\\[\nL \\propto \\sum_{s \\text{ consistent with } Z \\text{ and } D}  \\frac{\\exp(\\beta_s)p(Y | Z, S = s)}{\\sum_{s \\text{ consistent with } Z \\text{ and } D} \\exp(\\beta_s)}   \n\\]\nThe generated quantities block (version 1 of the stan model) is used to (1) compute the expected values of the outcome for each unit for all strata, which can be used to derive the principal causal effects. In version 2 of the model, this process is simplified as there are less strata and comparisons to consider.\n\n\nApplication\nThis draws heavily on the notes from Fan Li’s (Duke Uni) lecture notes and labs on causal inference.\nMimic a two-arm trial \\(z = 0, 1\\) with two-sided non-compliance and all cause mortality as the primary outcome. Define strata where \\(D_i(z)\\) indicates the treatment received under assignment to \\(z\\):\n\nnever takers \\((0,0) = \\{i: D_i(0) = 0, D_i(1) = 0\\}\\)\ncompliant \\((0,1) = \\{i: D_i(0) = 0, D_i(1) = 1\\}\\)\nalways takers \\((1,1) = \\{i: D_i(0) = 1, D_i(1) = 1\\}\\)\n\ni.e. assume monotonicity - that defiers do not exist. Simulate strata membership using independent samples drawn with probability 0.2, 0.6, 0.2. Define baseline covariates for disease severity \\(X_1 \\sim \\mathcal{N}(0, 1)\\) and age above 60 \\(X_2 \\sim \\text{Bernoulli}(0.6)\\).\n\\[\n\\begin{aligned}\n(Y | S = (0,0), Z = z, X_1, X_2) &\\sim \\text{Bernoulli}(g^{-1}(\\eta_{s[1]})) \\\\\n(Y | S = (0,1), Z = z, X_1, X_2) &\\sim \\text{Bernoulli}(g^{-1}(\\eta_{s[2]})) \\\\\n(Y | S = (1,1), Z = z, X_1, X_2) &\\sim \\text{Bernoulli}(g^{-1}(\\eta_{s[3]})) \\\\ \\\\\n\\eta_{s[1]} &=  0.1        + 1.1 X_1 + 0.4 X_2  \\\\\n\\eta_{s[2]} &=  0   - 2 z  + 1.1 X_1 + 0.4 X_2   \\\\\n\\eta_{s[3]} &= -0.3        + 1.1 X_1 + 0.4 X_2   \\\\\n\\end{aligned}\n\\]\n\nset.seed(973589239)\n\nN &lt;- 1e4\ns &lt;- sample(1:3, N, replace = TRUE, prob = c(0.2, 0.6, 0.2))\nz &lt;- sample(c(0, 1), N, replace = TRUE, prob = c(0.5, 0.5))\nx1 &lt;- rnorm(N)\nx2 &lt;- rbinom(N, 1, 0.6)\n\nd_1 &lt;- data.table(s = s, z = z, x1 = x1, x2 = x2)\n\nd_1[s == 1, d := 0]\nd_1[s == 2, d := z]\nd_1[s == 3, d := 1]\n\nd_1[s == 1, eta := 0.1 + 1.1*x1 + 0.4*x2]\nd_1[s == 2, eta := 0  -2*z + 1.1*x1 + 0.4*x2]\nd_1[s == 3, eta := -0.3      + 1.1*x1 + 0.4*x2]\n\nd_1[, y := rbinom(N, 1, plogis(eta))]\n\n\n# expected linear predictor for control 0 + -2*0 + 1.1*0 + 0.4*0.6 = 0.24\n# expected linear predictor for control 0 + -2*1 + 1.1*0 + 0.4*0.6 = -1.76\nd_1[s == 2 & z == 0, mean(eta)]\n## [1] 0.2463996\nd_1[s == 2 & z == 1, mean(eta)]\n## [1] -1.772282\n\n\n# convert to prob scale \nd_1[s == 2 & z == 0, plogis(mean(eta))]\n## [1] 0.5612901\nd_1[s == 2 & z == 1, plogis(mean(eta))]\n## [1] 0.1452588\nd_1[s == 2 & z == 1, plogis(mean(eta))] - d_1[s == 2 & z == 0, plogis(mean(eta))]\n## [1] -0.4160314\n\n\n# average probability across strata by treatment group\nd_1[s == 2 & z == 0, mean(plogis(eta))]\n## [1] 0.5480463\nd_1[s == 2 & z == 1, mean(plogis(eta))]\n## [1] 0.1926421\nd_1[s == 2 & z == 1, mean(plogis(eta))] - d_1[s == 2 & z == 0, mean(plogis(eta))]\n## [1] -0.3554042\n\n\n# should align somewhat to the observed data\nd_1[s == 2 & z == 0, mean(y)]\n## [1] 0.5494505\nd_1[s == 2 & z == 1, mean(y)]\n## [1] 0.2080828\nd_1[s == 2 & z == 1, mean(y)] - d_1[s == 2 & z == 0, mean(y)]\n## [1] -0.3413677\n\nGiven we simulate the data, we know which strata is which so we can use g-computation to calculate the risk difference in the observed data. On average, this should be somewhere near the expected value but will not be equal to it.\n\n# compliers\nd_s2 &lt;- d_1[s == 2]\nf0 &lt;- glm(y ~ z + x1 + x2, data = d_s2, family = binomial())\n\nd_s2_0 &lt;- copy(d_s2)\nd_s2_0[, z := 0]\neta_0 &lt;- predict(f0, newdata = d_s2_0)\n\nd_s2_1 &lt;- copy(d_s2)\nd_s2_1[, z := 1]\neta_1 &lt;- predict(f0, newdata = d_s2_1)\n\nrd &lt;- plogis(mean(eta_1)) - plogis(mean(eta_0))\nrd\n## [1] -0.3965591\n\n# average risk by group, not average log-odds transformed as above\n# plogis(E[X]) \\ne E[plogis(X)] due to nonlinearity\nrd &lt;- mean(plogis(eta_1)) - mean(plogis(eta_0))\nrd\n## [1] -0.3397087\n\nCompile and fit the second implementation of the principal strata model that includes both monotonicity and ER assumptions:\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/principal-stratum-02.stan\")\n\nld &lt;- list(\n  N = nrow(d_1),\n  PS = 1, # intercept only model\n  PG = 3, # intercept plus additive x1, x2, no interaction\n  Z = d_1$z,\n  D = d_1$d,\n  Y = d_1$y,\n  XS = matrix(rep(1, nrow(d_1)), ncol = 1),\n  XG = cbind(1, d_1$x1, d_1$x2)\n  )\n\nf1 &lt;- m1$sample(data = ld, chains = 1, iter_sampling = 1000, refresh = 0,\n                adapt_delta = 0.9)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 135.4 seconds.\n\n\nWe can visualise the posterior for strata membership as shown in Figure 1.\n\nd_p1 &lt;- data.table(f1$draws(variables = \"strata_prob\", format = \"matrix\"))\nnames(d_p1) &lt;- paste0(1:ncol(d_p1))\nd_fig &lt;- melt(d_p1, measure.vars = names(d_p1), variable.name = \"strata\")\nd_fig[strata == \"1\", strata := \"n (0,0)\"]\nd_fig[strata == \"2\", strata := \"c (0,1)\"]\nd_fig[strata == \"3\", strata := \"a (1,1)\"]\n\nggplot(d_fig, aes(x = value, group = strata, col = strata)) +\n  geom_density() +\n  scale_color_discrete(\"Strata\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: Posterior proportions for strata membership\n\n\n\n\n\nAnd calculate the posterior expected mean for each arm and each strata as follows:\n\nd_p1 &lt;- data.table(f1$draws(variables = \"mean_effect\", format = \"matrix\"))\n\n# by definition and assumptions:\nN_strata &lt;- 3\nN_trt &lt;- 2\n\nd_grid &lt;- CJ(\n  s = 1:N_strata,\n  z = 0:(N_trt-1)\n)\n# strata definitions (0,0) never, (0,1)compliant, (1,1) always\nd_grid[, d := c(\n  0, 0, 0, 1, 1, 1\n)]\n# index into effect estimate for comparison\nd_grid[, g := c(\n  1, 1, 2, 3, 4, 4\n)]\n\na_out &lt;- array(NA, dim = c(N_strata, N_trt, nrow(d_p1)))\n\nfor(i in 1:N_strata){\n  for(j in 1:N_trt){\n    g_ref &lt;-  d_grid[s == i & z == (j-1), g] \n    a_out[i,j,] &lt;- d_p1[, g_ref, with = F][[1]]\n  }\n}\n\ndimnames(a_out)[[1]] &lt;- c(\"n\", \"c\", \"a\")\ndimnames(a_out)[[2]] &lt;- c(\"0\", \"1\")\n\n# summary outputs (means):\napply(a_out, c(1,2), mean)\n\n          0         1\nn 0.5708451 0.5708451\nc 0.5483657 0.2098348\na 0.5012453 0.5012453\n\n\nand then summarise treatment effects via simple comparisons between the arms:\n\nrbind(\n  n = quantile(a_out[1, 2,] - a_out[1, 1,], probs = c(0.5, 0.025, 0.975)),\n  c = quantile(a_out[2, 2,] - a_out[2, 1,], probs = c(0.5, 0.025, 0.975)),\n  a = quantile(a_out[3, 2,] - a_out[3, 1,], probs = c(0.5, 0.025, 0.975))\n)\n\n        50%       2.5%      97.5%\nn  0.000000  0.0000000  0.0000000\nc -0.338292 -0.3676863 -0.3087216\na  0.000000  0.0000000  0.0000000\n\n\nFurther testing would be beneficial to evaluate the long-run properties of this particular estimator.\n\n\n\n\n\nReferences\n\n1. Frangakis C, Rubin D. Principal stratification in causal inference. Clinical Trials. 2002;58:21–9.\n\n\n2. Page L. Principal stratification: A tool for understanding variation in program effects across endogenous subgroups. American Journal of Evaluation. 2015. https://doi.org/10.1177/1098214015594419.\n\n\n3. Mercatanti A. Do debit cards decrease cash demand?: Causal inference and sensitivity analysis using principal stratification. JRSS Applied Statistics Series C. 2017. https://doi.org/10.1111/rssc.12193.\n\n\n4. Liu B. Principal stratification analysis of noncompliance with time-to-event outcomes. Biometric Methodology. 2024. https://doi.org/10.1093/biomtc/ujad016.\n\n\n5. Hirano K. Assessing the effect of an influenza vaccine in an encouragement design. Biostatistics. 2000. https://doi.org/10.1093/biostatistics/1.1.69.\n\n\n6. Liu B, Li F. PStrata: An r package for principal stratification. Journal of Statistical Software. 2023."
  },
  {
    "objectID": "notebooks/overdispersion-1.html",
    "href": "notebooks/overdispersion-1.html",
    "title": "Overdispersion 1",
    "section": "",
    "text": "Setup and dependencies\nlibrary(data.table)\n\nlibrary(ggplot2)\nlibrary(ggh4x)\nlibrary(gt)\nsuppressPackageStartupMessages(library(cmdstanr))\nsuppressPackageStartupMessages(library(brms))\nsuppressPackageStartupMessages(library(mgcv))\n\n# devtools::install_github(\"thomasp85/patchwork\")\nlibrary(patchwork)\n\n\n\ntoks &lt;- unlist(tstrsplit(getwd(), \"/\")) \nif(toks[length(toks)] == \"maj-biostat.github.io\"){\n  prefix_stan &lt;- \"./stan\"\n} else {\n  prefix_stan &lt;- \"../stan\"\n}"
  },
  {
    "objectID": "notebooks/overdispersion-1.html#introduction",
    "href": "notebooks/overdispersion-1.html#introduction",
    "title": "Overdispersion 1",
    "section": "Introduction",
    "text": "Introduction\nFor Poisson, if \\(y\\) is the counts, we have probability density\n\\[\n\\begin{align*}\nf(y) = \\frac{\\mu^y \\exp(-\\mu)}{y!}\n\\end{align*}\n\\]\nand \\(\\mu\\) is the expected number of occurrences, which also equals the variance. Sometimes, you think about \\(\\mu\\) as a rate, e.g. average number of car crashes per 1000 population, per 1000 licensed drivers, per 10000 km travelled etc. In the latter view, rate is in terms of an exposure.\nIf \\(y_i\\) denote the number of events over exposure \\(n_i\\) for the \\(i^\\text{th}\\) covariate pattern, the expected value is\n\\[\n\\begin{align*}\nE[y_i] = \\mu_i = n_i \\lambda_i\n\\end{align*}\n\\]\nExplanatory variables are usually modelled via\n\\[\n\\begin{align*}\n\\lambda_i = \\exp(X \\beta)\n\\end{align*}\n\\]\nand so the GLM is\n\\[\n\\begin{align*}\ny_i &\\sim \\text{Pois}(\\mu_i) \\\\\n\\mu_i &= n_i \\exp(X \\beta)\n\\end{align*}\n\\]\nthe natural link is \\(\\log\\)\n\\[\n\\begin{align*}\n\\log(\\mu_i) &= \\log(n_i) + X \\beta\n\\end{align*}\n\\]\nThe inclusion of \\(\\log(n_i)\\) is the offset, a known constant whereas the \\(X\\) and \\(\\beta\\) represent the usual covariate pattern and parameters.\nWhen over-dispersion is present (can indicate something is missing from the linear predictor or independence between observations is questionable) we can switch to a Negative binomial distribution for the likelihood. This has a number of parameterisations, one of which uses the mean directly then controls the overdispersion relative to the square of the mean.\n\\[\n\\begin{align*}\ny_i &\\sim \\text{NegBin}(\\mu_i, \\phi)\n\\end{align*}\n\\]\nwith variance\n\\[\n\\begin{align*}\n\\text{Var}(y_i) = \\mu + \\frac{\\mu^2}{\\phi}\n\\end{align*}\n\\]\ni.e. \\(\\frac{\\mu^2}{\\phi}\\) is the additional variance over that for the Poisson.\nI believe this is actually the poisson gamma mixture but it is worth noting that there are other options for incorporating the excess variance. Some of those I have come across include a poisson-lognormal (https://www.sciencedirect.com/science/article/pii/S2001037025000856 and https://solomonkurz.netlify.app/blog/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/) and additive random effects (https://pmc.ncbi.nlm.nih.gov/articles/PMC4194460/)."
  },
  {
    "objectID": "notebooks/overdispersion-1.html#overdispersed-data",
    "href": "notebooks/overdispersion-1.html#overdispersed-data",
    "title": "Overdispersion 1",
    "section": "Overdispersed data",
    "text": "Overdispersed data\nSimulate overdispersed counts associated with 3 vaccine groups and where counts are represented per 35k but our interest is per million.\n\n\nSimulate overdispersed counts\n# we have 20 observations per vax, each having a count and a somewhat distinct \n# covariate pattern (which I am ignoring for now)\nn_per_group &lt;- 50\ncells_per_well &lt;- 35000\n\n# arbitrary values\ntrue_rates &lt;- c(vaxA = 5, vaxB = 10, vaxC = 15) / cells_per_well  \n# overdispersion (smaller =&gt; more dispersion)\nphi &lt;- 2  \n\n# Simulate data\nd_sim &lt;- rbindlist(lapply(names(true_rates), function(vaccine) {\n  mu &lt;- true_rates[vaccine] * cells_per_well\n  data.table(\n    vaccine = vaccine,\n    response = rnbinom(n_per_group, mu = mu, size = phi),\n    # technically this could be varying for each obs\n    cells = cells_per_well\n  )\n}))\nd_sim[, group := as.integer(factor(vaccine))]\nd_sim[, log_cells := log(cells)]\n\nhead(d_sim)\n\n\n   vaccine response cells group log_cells\n    &lt;char&gt;    &lt;num&gt; &lt;num&gt; &lt;int&gt;     &lt;num&gt;\n1:    vaxA        4 35000     1   10.4631\n2:    vaxA       12 35000     1   10.4631\n3:    vaxA        7 35000     1   10.4631\n4:    vaxA        2 35000     1   10.4631\n5:    vaxA        4 35000     1   10.4631\n6:    vaxA        2 35000     1   10.4631"
  },
  {
    "objectID": "notebooks/overdispersion-1.html#stan-model-implementation",
    "href": "notebooks/overdispersion-1.html#stan-model-implementation",
    "title": "Overdispersion 1",
    "section": "Stan model implementation",
    "text": "Stan model implementation\n\n\nCode\ncat(readLines(paste0(prefix_stan, \"/negbin-1.stan\")), sep = \"\\n\")\n\n\ndata {\n  int&lt;lower=1&gt; N;\n  int&lt;lower=1&gt; G;\n  array[N] int&lt;lower=1, upper=G&gt; group;\n  array[N] int&lt;lower=0&gt; y;\n  vector[N] log_cells;\n}\nparameters {\n  vector[G] b0;               // log(rate per cell for each group)\n  real&lt;lower=0&gt; phi;             // overdispersion parameter\n}\n\nmodel {\n  target += normal_lpdf(b0 | 0, 2);\n  target += gamma_lpdf(phi | 2, 0.1);\n  \n  for (i in 1:N){\n    target += neg_binomial_2_log_lpmf(y[i] | b0[group[i]] + log_cells[i], phi);\n  } \n}\n\ngenerated quantities {\n  vector[N] y_rep;\n  vector[G] mu;\n  \n  for (i in 1:G){\n    mu[i] = exp(b0[i]) * pow(10,6);\n  }\n  for (i in 1:N){\n    y_rep[i] = neg_binomial_2_log_rng(b0[group[i]] + log(pow(10,6)), phi);  \n  }\n  \n    \n}\n\n\nFit with a stan model, compute the expected values per million in the generated quantities block. The reference values are 143, 286, 429 for groups A, B and C.\n\n\nFit model with stan\nm1 &lt;- cmdstanr::cmdstan_model(paste0(prefix_stan, \"/negbin-1.stan\"))\n\n# Prepare Stan data\nld &lt;- list(\n  N = nrow(d_sim),\n  G = length(unique(d_sim$group)),\n  group = d_sim$group,\n  y = d_sim$response,\n  log_cells = d_sim$log_cells\n)\n\nf1 &lt;- m1$sample(\n  ld, iter_warmup = 1000, iter_sampling = 1000,\n  parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = F,\n  max_treedepth = 10)\n\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.2 seconds.\n\n\nFit model with stan\n# mu is scaled up to per 1 million cells\nf1$summary(variables = c(\"mu\"))\n\n\n# A tibble: 3 × 10\n  variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mu[1]     146.   144.  19.0  17.0  120.  178.  1.00    1011.     647.\n2 mu[2]     274.   270.  31.2  30.7  227.  326.  1.00    1129.     656.\n3 mu[3]     405.   402.  44.4  41.6  337.  488.  1.00    1041.     720.\n\n\nRepresent the posterior view of the means (per million cells). These are the expected counts, i.e. on average (central tendency) we expect blah from vaccine A, B, C.\n\n\nPosterior means\nd_post &lt;- data.table(f1$draws(variables = c(\"mu\"), format = \"matrix\"))\nd_post &lt;- melt(d_post, measure.vars = names(d_post))\n\nd_fig_1 &lt;- copy(d_post)\nd_fig_1[, group := gsub(\"mu\\\\[\", \"\", variable)]\nd_fig_1[, group := as.factor(gsub(\"\\\\]\", \"\", group))]\n\nd_fig_2 &lt;- data.table(\n  group = factor(1:3),\n  tru = true_rates * 1e6\n)\n\n\nggplot(d_fig_1, aes(x = value, group = group, col = group)) +\n  geom_vline(\n    data = d_fig_2, \n    aes(xintercept = tru, group = group, col = group),\n    lwd = 0.2\n  ) +\n  geom_density() +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 1: Posterior means per million cells by group\n\n\n\n\n\n\n\nPosterior predictive implementation\nd_post &lt;- data.table(f1$draws(variables = c(\"y_rep\"), format = \"matrix\"))\nd_post[, ix := 1:.N]\nd_post &lt;- melt(d_post, id.vars = \"ix\")\n\nix_rnd &lt;- sort(sample(1:max(d_post$ix), size = 10, replace = F))\nd_fig_1 &lt;- copy(d_post[ix %in% ix_rnd])\nd_fig_1[, id := gsub(\"y_rep\\\\[\", \"\", variable)]\nd_fig_1[, id := as.integer(gsub(\"\\\\]\", \"\", id))]\nd_fig_1[, group := d_sim[id, group]]\n\n\nggplot(d_fig_1, aes(x = value)) +\n  geom_histogram(\n    fill = \"white\", col = \"black\", bins = 17\n  ) +\n  geom_histogram(\n    data = d_sim,\n    aes(x = 1e6 * response / cells_per_well),\n    fill = \"red\", alpha = 0.3, bins = 15, lwd = 0.3\n  ) +\n  theme_bw() +\n  ggh4x::facet_grid2(ix ~ group, scales = \"free_x\", labeller = label_both)\n\n\n\n\n\n\n\n\nFigure 2: Posterior predictive (counts per million) based on 10 samples of equal size to what we observed (in red) by vaccine group"
  },
  {
    "objectID": "notebooks/mann-whitney-u.html",
    "href": "notebooks/mann-whitney-u.html",
    "title": "Mann-Whitney U",
    "section": "",
    "text": "The MWU checks a rank sum difference between two independent groups and tests whether two groups have been drawn from the same population. It is an alternative to the t-test. But it only requires that the observations from both groups are at least ordinal such that you can discern for any two observations which one is greater (or better in some sense). In the classical interpretation, the test assumes that the distributions are identical under the null hypothesis and that they are not identical under the alternative distribution. Underlying the test is an interest in the probability that a randomly selected unit from one group fairs better than a randomly selected unit from the other. More concretely, the interest is int:\n\\[\n\\text{Pr}(X &gt; Y) \\ne \\text{Pr}(Y &gt; X)\n\\]\nand if this quantity exceeds \\(0.5\\) then, all things being equal, you would prefer assignment to the \\(X\\) group.\nOne approach to the procedure involves combining all the observations and ordering the records from best to worse (keeping track of which record belongs to which group). Each row is assigned a rank, the sum of the ranks is calculated for the first group (\\(T_1\\)) and the second group (\\(T_2\\)) and then the \\(U\\) statistics are computed as:\n\\[\n\\begin{aligned}\nU_1 &= n_1 n_2 + \\frac{n_1(n_1 + 1)}{2} - T_1 \\\\\nU_2 &= n_1 n_2 + \\frac{n_2(n_2 + 1)}{2} - T_2 \\\\\n\\end{aligned}\n\\]\nthe Mann-Whitney-U is given by \\(U = \\text{min}(U_1,U_2)\\), which is the test statistic.\nFor example, say we have units 1 to 20 in the first group and units 21 to 40 in the second with all the units in the first group having a better score than the units in the second.\n\nlibrary(data.table)\nlibrary(ggplot2)\n# contrived setup to make the scores for units ascend \n# (lower values considered better)\nd &lt;- data.table(\n  i = 1:40, score = sort(rnorm(40, 0, 1)), \n  group = rep(1:2, each = 20)\n)\nd[, rank := 1:40]\nn1 &lt;- d[group == 1, .N]\nn2 &lt;- d[group == 2, .N]\n\nNow add up the ranks for both groups.\n\n(T1 = d[group == 1, sum(rank)])\n\n[1] 210\n\n(T2 = d[group == 2, sum(rank)])\n\n[1] 610\n\n\nFrom these compute \\(U_1\\) and \\(U_2\\)\n\n(U_1 = n1*n2 + n1*(n1+1)/2 - T1)\n\n[1] 400\n\n(U_2 = n1*n2 + n2*(n2+1)/2 - T2)\n\n[1] 0\n\n\nThe above is equivalent to a pairwise comparison procedure (see later). From this, we can estimate the probability of superiority as referred to earlier (which I believe, but am not certain, is assuming that the two distributions differ only in location, not in shape) as:\n\n# here the contrived setup says that the value of x_1 and x_2 also equate to their ranks\n# and that lower values are better.\nx_1 &lt;- 1:20\nx_2 &lt;- 21:40\nn_1 &lt;- length(x_1)\nn_2 &lt;- length(x_2)\nu_1 &lt;- 0\nu_2 &lt;- 0\n\nk &lt;- 1\nfor(i in 1:length(x_1)){\n  for(j in 1:length(x_2)){\n    if(x_1[i] &lt; x_2[j]) u_1 &lt;- u_1 + 1\n    else if(x_1[i] &gt; x_2[j]) u_2 &lt;- u_2 + 1\n    k &lt;- k + 1\n  }\n}\nsprintf(\"u1 = %.0f, Pr(X&gt;Y) = %.2f, u2 = %.0f, Pr(Y&gt;X) = %.2f\", u_1, u_1/(n_1*n_2), u_2, u_2/(n_1*n_2))\n\n[1] \"u1 = 400, Pr(X&gt;Y) = 1.00, u2 = 0, Pr(Y&gt;X) = 0.00\"\n\n\nThat is, for all possible pairwise combinations, we compare the value in the first group to each value in second group and add up how often X &gt; Y and compute the probability estimate of \\(\\text(Pr)(X&gt;Y)\\) by simply dividing the number of occurrences by the total number of pairs.\nFor the test, we take the following reference points. First, under no difference, the expected value for \\(U\\) is\n\\[\n\\mathbb{E}[U] = \\frac{n_1 n_2}{2}\n\\]\nwith a standard standard error of:\n\\[\n\\sigma_U = \\sqrt{\\frac{n_1 n_2 (n_1 + n_2 + 1)}{12}}\n\\]\nUsing these, you can compute a z-value using a normal approximation (for large samples &gt; 20 per group) in the usual way by taking the observed value for \\(U\\) subtracting the expected value and dividing by the standard error:\n\\[\nz = \\frac{U - \\mu_u}{\\sigma_U}\n\\]\nand obtain a p-value for the test."
  },
  {
    "objectID": "notebooks/mann-whitney-u.html#footnotes",
    "href": "notebooks/mann-whitney-u.html#footnotes",
    "title": "Mann-Whitney U",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe intuition for the above is that \\(F_e(x)\\) gives the probability that \\(X_e\\) is greater than a specific value of \\(x\\), and \\(f_c(x) dx\\) gives the probability that \\(X_c\\) falls in a small interval around \\(x\\). The product of these terms gives the probability that \\(X_e &gt; X_c\\) for that small interval and thus integrating over all possible values of \\(x\\) gives the value we are after.↩︎"
  },
  {
    "objectID": "notebooks/limits-of-per-protocol.html",
    "href": "notebooks/limits-of-per-protocol.html",
    "title": "Limitations of per-protocol analyses",
    "section": "",
    "text": "Patients in RCTs that discontinue before the endpoint often have poorer prognosis than those who continue treatment. Additionally, discontinuation is more common in the treatments that create more side effects or have less clear benefit.\nThe traditional per-protocol analyses will exclude those patients that are not completers and that deviate in some unacceptable way from the protocol (e.g. early discontinuation of treatment) and then run the primary analysis unchanged. That is, the comparison groups are defined in part by post randomisation events that are influenced by treatment group status and other factors. This can be problematic in that it violates the randomisation principle, the treatment groups can become unbalanced in either known or unknown covariates, selection bias can arise, effect estimates may be biased and external validity threatened, [1]. The following example illustrates this.\nCompare a new antihypertensive (A) to a standard treatment (B) for lowering blood pressure in patients with existing hypertension in an RCT. At 6-months patients are assessed to be either still hypertensive (0) or no longer hypertensive (1). One thousands patients are randomised, 500 to each arm.\nIt is common for trials to show differential adherence by study group. Here, assume that A causes more side effects than B and at 3 months, 20% of patients stop taking A due to side effects and 4% stop taking B for a side effect intercurrent event. We interpret this as a protocol deviation (although who knows whether this would actually be consider to be a protocol deviation in reality). Additionally, assume that the patients who experience side effects (and therefore would dropout of the traditional PP analysis) tend to have more severe hypertension and would have shown less improvement in blood pressure status irrespective of the treatment they received.\nThe traditional per-protocol analysis excludes those that deviate from the protocol and we would therefore have around 400 patients in A and 480 patients in B.\nFinally assume that the true percentage that are no longer hypertensive at 6 months is 40% and 20% in group A and B respectively.\nBelow is a simulation of the assumed data generation process.\n\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(parallel)\n\nget_data &lt;- function(\n    N = 1000\n){\n  \n  d &lt;- data.table(i = 1:N)\n  d[, trt := rep(0:1, each = N/2)]\n  d[, sev := rnorm(N, 0, 1) ]\n  \n  d[trt == 0, p_ae := plogis(qlogis(0.04) + 0.4 * sev)]\n  d[trt == 1, p_ae := plogis(qlogis(0.40) + 0.4 * sev)]\n  \n  d[, discont := rbinom(.N, 1, p_ae)]\n  \n  d[trt == 0, p_y := plogis(qlogis(0.20) - 0.3 * sev)]\n  d[trt == 1, p_y := plogis(qlogis(0.40) - 0.3 * sev)]\n  \n  d[, y := rbinom(.N, 1, p_y)]\n  \n  d\n}\n\nSimulating this data generation process a large number of times, the ITT estimate along with the PP estimates can be computed to gain some insight of their long-run properties.\n\nm_res &lt;- do.call(rbind, mclapply(1:1e4, FUN = function(i){\n  d &lt;- get_data()\n\n  # ITT analysis\n  f1 &lt;- glm(y ~ trt, data = d, family = binomial())\n  rd_f1 &lt;- diff(predict(f1, type = \"response\", newdata = data.table(trt = 0:1)))\n  \n  # Traditional PP analysis \n  f2 &lt;- glm(y ~ trt, data = d[discont == 0], family = binomial()) \n  rd_f2 &lt;- diff(predict(f2, type = \"response\", newdata = data.table(trt = 0:1)))\n  \n  # G-comp - weights per the full sample\n  f3 &lt;- glm(y ~ trt + sev, data = d[discont == 0], family = binomial()) \n  \n  d_trt_1 &lt;- copy(d)\n  d_trt_1[, trt := 1]\n  d_trt_0 &lt;- copy(d)\n  d_trt_0[, trt := 0]\n  \n  f3_eta_trt_1 &lt;- predict(f3, newdata = d_trt_1)\n  f3_eta_trt_0 &lt;- predict(f3, newdata = d_trt_0)\n  \n  rd_f3 &lt;- plogis(mean(f3_eta_trt_1))  - plogis(mean(f3_eta_trt_0))\n  \n  c(rd_f1, rd_f2, rd_f3)\n  \n}, mc.cores = 6))\n\nFigure 1 shows the results. Specifically, the distribution of the estimated treatment effect (risk difference) which is known to have a true value of 0.2 in favour of the new drug (A).\nUnder the ITT analysis (where we ignore the fact that the ICE occurred and simply proceed to analyse all the data, irrespective of whether the ICE occurred or not) the long-run expected value for the MLE estimate of the treatment effect aligns with the true value of 0.2.\nHowever, in the traditional PP analysis, we drop those patients for whom the protocol deviation applies. We fit the same model as was used in the ITT analysis to the remaining data. For this approach, we can see that the expected value for the MLE estimate is inflated.\nThe final plot shows the results from a revised approach to the per-protocol analysis where we:\n\nrun the analysis assuming that the deviations are censored, i.e. we drop anyone that had the side effects\nadd variables that are predictive of the ICE into the model\nfor the entire data set make predictions of the outcome assuming that all patients are assigned to the standard treatment (A)\nrepeat 3 for the patients assuming that all patients are assigned to the standard treatment (B)\ncompute the treatment effect (risk difference) as the difference between the means of the predicted values each transformed back to the risk scale\n\nThis final approach is aligned with a G-computation perspective and gives results similar to those that would be produced from an inverse probability of censoring weighting scheme. In both cases, we are effectively, producing a re-weighted estimate of the effect, but going about it in slightly different ways.\nUnder this revised per-protocol approach, we once again produce an estimate of the treatment effect that has a expected value close to the known true value of 0.2. Clearly, this is a very much simplified and synthetic example, but it is only intended to give an introduction.\n\nd_fig &lt;- data.table(m_res)\nnames(d_fig) &lt;- c(\"itt\", \"pp_1\", \"pp_2\")\nd_fig &lt;- melt(d_fig, measure.vars = names(d_fig))\n\nd_fig[variable == \"itt\", variable := \"ITT\"]\nd_fig[variable == \"pp_1\", variable := \"Traditional PP\"]\nd_fig[variable == \"pp_2\", variable := \"PP via G-Computation\"]\nd_fig[, variable := factor(variable, levels = c(\n  \"ITT\", \"Traditional PP\", \"PP via G-Computation\"\n))]\n\nggplot(d_fig, aes(x = value, group = variable, col = variable)) + \n  geom_density() +\n  geom_vline(data = d_fig[, mean(value), keyby = variable], \n             aes(xintercept = V1, col = variable)) +\n  scale_color_discrete(\"Analysis\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(~variable, ncol  = 1)\n\n\n\n\n\n\n\nFigure 1: Distribution of effect estimates under ITT, traditional per-protocol and g-computation under the present of ICE\n\n\n\n\n\nThe difference between the two per-protocol approaches can be emphasised by looking at the distribution of the percentage differences between the ITT effect and the PP effects as shown in Figure 2. For the traditional approach, the treatment effect is, on average, about 5% higher than it should be whereas under the revised approach, the PP approach aligns fairly well with the ITT estimate (with both being aligned with the true value).\n\nd_fig &lt;- data.table(m_res)\nnames(d_fig) &lt;- c(\"itt\", \"pp_1\", \"pp_2\")\nd_fig[, pct_diff_1 := 100 * (pp_1 - itt)/itt]\nd_fig[, pct_diff_2 := 100 * (pp_2 - itt)/itt]\n\nd_fig &lt;- d_fig[, .(pct_diff_1, pct_diff_2)]\nd_fig &lt;- melt(d_fig, measure.vars = names(d_fig))\n\nd_fig[variable == \"pct_diff_1\", variable := \"% change (traditional PP vs ITT)\"]\nd_fig[variable == \"pct_diff_2\", variable := \"% change (g-computation PP vs ITT)\"]\n\nd_fig[, variable := factor(variable, levels = c(\n  \"% change (traditional PP vs ITT)\", \n  \"% change (g-computation PP vs ITT)\"\n))]\n\n\nggplot(d_fig, aes(x = value, group = variable, col = variable)) + \n  geom_density() +\n  geom_vline(data = d_fig[, mean(value), keyby = variable], \n             aes(xintercept = V1, col = variable)) +\n  scale_color_discrete(\"Approach\") +\n  scale_x_continuous(breaks = seq(-40, 40, by = 10)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(~variable, ncol = 1)\n\n\n\n\n\n\n\nFigure 2: Distribution of percentage change from the ITT estimate - traditional per-protocol and g-computation\n\n\n\n\n\n\n\n\n\nReferences\n\n1. Greenland S. Estimating effects from randomized trials with discontinuations: The need for intent-to-treat design and g-estimation. Clinical Trials. 2008;5:5–13."
  },
  {
    "objectID": "notebooks/graphviz.html",
    "href": "notebooks/graphviz.html",
    "title": "CONSORT using graphviz",
    "section": "",
    "text": "Graphviz (short for Graph Visualization Software) is a package of open-source tools initiated by AT&T Labs Research for drawing graphs specified in DOT language scripts. Three main kinds of objects appear in the DOT language: graphs, nodes, edges. The outer most (main) graph can be directed (digraph) or undirected (graph). Within the main graph, a subgraph defines a subset of nodes and edges.\nThe following provides some basic reference examples, mostly taken from [1].\n\nlibrary(DiagrammeR)\n\n\nInitial Examples\nThe specification of a simple graph is shown in Figure 1. Nodes are created when the name appears. Edges are created when nodes are joined by the edge operator -&gt;.\nUnlike other types of quarto blocks, it is necessary to enter the control elements in the form //| echo: true rather than the usual #| echo: FALSE.\n\ndigraph G {\n  main -&gt; parse -&gt; execute;\n  main -&gt; init;\n  main -&gt; cleanup;\n  execute -&gt; make_string;\n  execute -&gt; printf\n  init -&gt; make_string;\n  main -&gt; printf;\n  execute -&gt; compare;\n}\n\n\n\n\n\n\n\nG\n\n\n\nmain\n\nmain\n\n\n\nparse\n\nparse\n\n\n\nmain-&gt;parse\n\n\n\n\n\ninit\n\ninit\n\n\n\nmain-&gt;init\n\n\n\n\n\ncleanup\n\ncleanup\n\n\n\nmain-&gt;cleanup\n\n\n\n\n\nprintf\n\nprintf\n\n\n\nmain-&gt;printf\n\n\n\n\n\nexecute\n\nexecute\n\n\n\nparse-&gt;execute\n\n\n\n\n\nmake_string\n\nmake_string\n\n\n\nexecute-&gt;make_string\n\n\n\n\n\nexecute-&gt;printf\n\n\n\n\n\ncompare\n\ncompare\n\n\n\nexecute-&gt;compare\n\n\n\n\n\ninit-&gt;make_string\n\n\n\n\n\n\n\n\nFigure 1: Example dot graph\n\n\n\n\n\nNodes and edges can be given attributes to control their representation and placement. The size of the graph can be controlled with the size operator. If the drawing is too large, it is scaled uniformly as necessary to fit.\n\n\n\n\n\n\n\n\nG\n\n\n\nmain\n\nmain\n\n\n\nparse\n\nparse\n\n\n\nmain-&gt;parse\n\n\n\n\n\ninit\n\ninit\n\n\n\nmain-&gt;init\n\n\n\n\n\ncleanup\n\ncleanup\n\n\n\nmain-&gt;cleanup\n\n\n\n\n\nprintf\n\nprintf\n\n\n\nmain-&gt;printf\n\n\n100 times\n\n\n\nexecute\n\nexecute\n\n\n\nparse-&gt;execute\n\n\n\n\n\nmake_string\n\nmake a\nstring\n\n\n\nexecute-&gt;make_string\n\n\n\n\n\nexecute-&gt;printf\n\n\n\n\n\ncompare\n\ncompare\n\n\n\nexecute-&gt;compare\n\n\n\n\n\ninit-&gt;make_string\n\n\n\n\n\n\n\n\nFigure 2: Example dot graph with attributes and comments\n\n\n\n\n\n\n\nAttributes\nSome common node shapes available include box, circle, record and plaintext. A complete list can be found at [www.graphviz.org/doc/info/shapes.html]. The modifier fixedsize=true ensures that the node’s actual size is aligned with width and height. Linework can be doubled up using peripheries=2 and orientation directed with orientation measured in degrees.\nWhile the default name of a node is its name, this can be modified with the label attribute.\n\ndigraph structs {\n  size =\"2,2\"; ratio=fill;\nnode [shape=record,fontsize=5];\n  struct1 [shape=record,label=\"&lt;f0&gt; left|&lt;f1&gt; mid\\ dle|&lt;f2&gt; right\"];\n  struct2 [shape=record,label=\"&lt;f0&gt; one|&lt;f1&gt; two\"];\n  struct3 [shape=record,label=\"hello\\nworld |{ b |{c|&lt;here&gt; d|e}| f}| g | h\"];\n  struct1 -&gt; struct2;\n  struct1 -&gt; struct3;\n}\n\n\n\n\n\n\n\nstructs\n\n\n\nstruct1\n\nleft\n\nmid dle\n\nright\n\n\n\nstruct2\n\none\n\ntwo\n\n\n\nstruct1-&gt;struct2\n\n\n\n\n\nstruct3\n\nhello\nworld\n\nb\n\nc\n\nd\n\ne\n\nf\n\ng\n\nh\n\n\n\nstruct1-&gt;struct3\n\n\n\n\n\n\n\n\nFigure 3: Example of dot graph with nested attributes and comments\n\n\n\n\n\nTo specify the minimum distance between two adjacent nodes, use nodesep and to set the minimum vertical space use ranksep.\n\n\nR Integration\nThere are a number of ways to incorporate DOT within R. One approach is to use the DiagrammeR package, specifically the grViz() function. This allows you to encode the graph using DOT notation but also pass in arguments as defined in R. Figure 4 provides a simple example.\n\nN &lt;- 24\nN_A &lt;- 12\nN_A_excl &lt;- 2\nN_A_analy &lt;- 10\nN_B &lt;- 12\nN_B_analy &lt;- 12\n\nfig &lt;- grViz(\"digraph consort {\n\n  node [fontname = Helvetica, shape = box, width = 1];\n  \n  enrolled [label = 'Enrolled \\n(n = @@1)'];  \n  allocA [label = 'Assigned A \\n(n = @@2)'];\n  allocB [label = 'Assigned B \\n(n = @@3)'];\n  \n  { rank = same; allocA allocB } \n  \n  exclA[label = 'Excl A \\n(n = @@6)'];\n  \n  analyA [label = 'Incl in analysis \\n(n = @@4)'];\n  analyB [label = 'Incl in analysis \\n(n = @@5)'];\n  \n  { rank = same; analyA analyB } \n  \n  blank1[label = '', width = 0.01, height = 0.01];\n\n  blank2[label = '', width = 0.01, height = 0.01];\n  blank3[label = '', width = 0.01, height = 0.01];\n  blank4[label = '', width = 0.01, height = 0.01];\n  /* all have same vertical position */\n  { rank = same; blank2 blank3 blank4 }\n\n  blank2 -&gt; blank3 [arrowhead=none, minlen = 5];\n  blank3 -&gt; blank4 [arrowhead=none, minlen = 5];\n  \n  blank5[label = '', width = 0.01, height = 0.01]; \n  { rank = same; blank5 exclA } \n\n  enrolled -&gt; blank1[dir = none];\n  blank1 -&gt; blank3[arrowhead=none];\n  blank2 -&gt; allocA;\n  blank4 -&gt; allocB;\n  allocA -&gt; blank5[arrowhead=none];\n  blank5 -&gt; exclA;\n  blank5 -&gt; analyA;\n  allocB -&gt; analyB;\n  \n}\n\n  [1]: N\n  [2]: N_A\n  [3]: N_B\n  [4]: N_A_analy\n  [5]: N_B_analy\n  [6]: N_A_excl\n\")\nfig\n\n\n\n\n\n\n\nFigure 4: Example of dot graph that uses arguments defined in R\n\n\n\n\n\n\n\n\n\nReferences\n\n1. Gansner E. Dot user’s manual - drawing graphs with dot. 2015."
  },
  {
    "objectID": "notebooks/door-1.html",
    "href": "notebooks/door-1.html",
    "title": "Desirability of Outcome Ranking (DOOR)",
    "section": "",
    "text": "DOOR analyses are claimed to be more patient centric. Instead of constructing summary measures by group for each outcome, the DOOR approach combines endpoints at a patient level and then creates a summary measure of the composite view for each intervention.\nThere are two approaches to a DOOR analysis, see [1]. The first approach uses the pairwise comparisons as introduced in Mann-Whitney-U. However, unlike the classical MWU, in the DOOR analysis, all the paired results are incorporated into the test statistic (this can also be done in MWU but wasn’t discussed in the earlier post). The other method used for the DOOR is a partial credit approach, but I do not really understand what that is about.\nAs a result, the DOOR analysis gives you an estimate of the probability that a randomly selected patient in the experimental group will have a better ranking than a randomly selected patient in the control group. The calculation used for the aggregated pairwise comparisons is:\n\\[\n\\begin{aligned}\n\\text{Pr}(door) = \\frac{ (n_{win} + 0.5 n_{tie}) } { n_e n_c }\n\\end{aligned}\n\\]\nwhere \\(n_{win}\\) is the number of times the units in the experimental group had better outcomes compared to the control group, \\(n_{tie}\\) is the number of ties, \\(n_e\\) is the number of units in the experimental group and \\(n_c\\) the number of units in the control group. This measure is also referred to as the probabilistic index [2] or probability of superiority, which will be cover in a separate post.\nIf there is no difference between the two arms, the probability will be close to 50%. Uncertainty intervals can be obtained via bootstrap or other means.\n\n\nLoading required package: Rcpp\n\n\nBuyseTest version 3.2.0\n\n\n\n\nTable 1: Ranking criteria for desirability of outcome for PJI\n\n\n\n\n\n\n\n\n\nRank\nAlive\nJoint Function\nTrt Success\nQoL\n\n\n\n\n1\nYes\nGood\nYes\nTiebreaker based on EQ5D5L\n\n\n2\nYes\nGood\nNo\nTiebreaker based on EQ5D5L\n\n\n3\nYes\nPoor1\nYes\nTiebreaker based on EQ5D5L\n\n\n4\nYes\nPoor\nNo\nTiebreaker based on EQ5D5L\n\n\n5\nNo\n-\n-\n-\n\n\n\n1 Good joint function is based on thresholds related to patient reported success. A successful outcome at 12-months will be defined for knee PJI with an Oxford Knee Score (OKS) at 12 months of &gt;36 or an improvement (delta) from baseline of &gt;9 and for hip PJI as a Oxford Hip Score (OHS) of &gt;38 or an improvement of &gt;12 (35).\n\n\n\n\n\n\n\n\n\n\n\nConsider a DOOR schema and ranking specification for prosthetic joint infection as per Table 1. Patients are assessed and assigned ranks based on how they align with the schema with the goal of differentiating the overall or global outcome of a patient state.\nBelow 100 people per group are simulated based on some hypothetical pair of distributions for the schema. The door probability is computed along with its confidence interval (by bootstrapping):\n\nseed &lt;- 1\nset.seed(seed)\n\nn_e &lt;- 100\nn_c &lt;- 100\np_x_e &lt;- c(0.5, 0.3, 0.1, 0.1, 0.0)\np_x_c &lt;- c(0.3, 0.2, 0.2, 0.2, 0.1)\n  \nx_e &lt;- sample(1:5, n_e, replace = T, p_x_e)\nx_c &lt;- sample(1:5, n_c, replace = T, p_x_c)  \n\nn_win &lt;- 0\nn_tie &lt;- 0\nfor(i in 1:n_e){\n  for(j in 1:n_c){\n    if(x_e[i] &lt; x_c[j]) n_win &lt;- n_win + 1\n    if(x_e[i] == x_c[j]) n_tie &lt;- n_tie + 1\n  }\n}\n\n# estimate for door\npr_door &lt;- (n_win + 0.5 * n_tie)/(n_e*n_c)\n\nboot_door &lt;- function(ix_e, ix_c){\n  \n  x_e_new &lt;- x_e[ix_e]\n  x_c_new &lt;- x_c[ix_e]\n  \n  n_win &lt;- 0\n  n_tie &lt;- 0\n  for(i in 1:n_e){\n    for(j in 1:n_c){\n      if(x_e_new[i] &lt; x_c_new[j]) n_win &lt;- n_win + 1\n      if(x_e_new[i] == x_c_new[j]) n_tie &lt;- n_tie + 1\n    }\n  }\n  \n  (n_win + 0.5 * n_tie)/(n_e*n_c)\n}\n\nn_boot &lt;- 1000\npr_door_rep &lt;- numeric(n_boot)\nfor(i in 1:n_boot){\n  ix_e &lt;- sample(1:n_e, size = n_e, replace = T)\n  ix_c &lt;- sample(1:n_c, size = n_c, replace = T)\n  pr_door_rep[i] &lt;- boot_door(ix_e, ix_c)\n}\n# \ndoor_ci &lt;- quantile(pr_door_rep, probs = c(0.025, 0.975))\n\n# c(pr_door, door_ci)\n\nFrom above, the estimate for the door probability is 0.70 with a (bootstrapped) 95% CI of 0.63, 0.77.\nThe process is simple but the procedure itself does not readily admit to complex modelling. However, Follmann proposed using a logistic regression for the probability of superiority for each determinate pair of patients \\(i\\), \\(j\\) and covariate vectors \\(\\vec{z}_{ij} = \\vec{z}_i - \\vec{z}_j\\) such that the parameters in the model correspond to the log-odds that a patient with \\(\\vec{z}_i\\) has an outcome that is better than a patient with \\(\\vec{z}_j\\) [3]. The presentation from Follmann is pretty convoluted and I lost patience with it. The exposition of probabilistic index models by De Schryver, which is analogous, if not equivalent, is much clearer and will be discussed separately, Probabilistic Index Models.\nA shiny application for door analyses can be found at DOOR although it does not give any detail on the implementation of the methods used. Under the probability-based analysis tab, the overall door and then a decomposition based on each of the dichotomous door components is shown.\nScraping the source data of the site, you can at least recreate some of the statistics. For example, the door probabilities for the ARLG CRACKLE-I demo data as detailed in the door probability-based analysis tab, are replicated below for discharge from hospital:\n\n\n\n\nTable 2: Colistin data from shiny application for DOOR\n\n\n\n\n\n\n\n\n\ntrt\ndoor_num\ndoor_txt\nN\n\n\n\n\nCAZ-AVB\n1\nDischarged home\n6\n\n\nCAZ-AVB\n2\nAlive in hosp, discharged not to home, no renal failure\n17\n\n\nCAZ-AVB\n3\nAlive in hosp, discharged not to home, renal failure\n1\n\n\nCAZ-AVB\n4\nHospital death\n2\n\n\nColistin\n1\nDischarged home\n4\n\n\nColistin\n2\nAlive in hosp, discharged not to home, no renal failure\n25\n\n\nColistin\n3\nAlive in hosp, discharged not to home, renal failure\n5\n\n\nColistin\n4\nHospital death\n12\n\n\n\n\n\n\n\n\n\n\n\nn_e &lt;- d[trt == \"CAZ-AVB\", .N]\nn_c &lt;- d[trt == \"Colistin\", .N]\n\nn_win &lt;- 0\nn_tie &lt;- 0\nfor(i in 1:n_e){\n  for(j in 1:n_c){\n    if(d[trt == \"CAZ-AVB\"][i, discharge_num] &lt; \n       d[trt == \"Colistin\"][j, discharge_num]) n_win &lt;- n_win + 1\n    if(d[trt == \"CAZ-AVB\"][i, discharge_num] == \n       d[trt == \"Colistin\"][j, discharge_num]) n_tie &lt;- n_tie + 1\n  }\n}\npr_door_colistin &lt;- (n_win + 0.5 * n_tie)/(n_e*n_c)\n\n\nboot_door &lt;- function(ix_e, ix_c){\n  \n  x_e_new &lt;- d[trt == \"CAZ-AVB\"][ix_e, discharge_num]\n  x_c_new &lt;- d[trt == \"Colistin\"][ix_c, discharge_num]\n  \n  n_win &lt;- 0\n  n_tie &lt;- 0\n  for(i in 1:n_e){\n    for(j in 1:n_c){\n      if(x_e_new[i] &lt; x_c_new[j]) n_win &lt;- n_win + 1\n      if(x_e_new[i] == x_c_new[j]) n_tie &lt;- n_tie + 1\n    }\n  }\n  \n  (n_win + 0.5 * n_tie)/(n_e*n_c)\n}\n\nn_boot &lt;- 1e3\npr_door_rep &lt;- numeric(n_boot)\nfor(i in 1:n_boot){\n  ix_e &lt;- sample(1:n_e, size = n_e, replace = T)\n  ix_c &lt;- sample(1:n_c, size = n_c, replace = T)\n  pr_door_rep[i] &lt;- boot_door(ix_e, ix_c)\n}\npr_door_colistin_ci &lt;- quantile(pr_door_rep, probs = c(0.025, 0.975))\n\nGiving 0.57 and 95% CI of 0.49, 0.66.\nSimilarly, for renal failure:\n\nd[, .N, keyby = .(trt, renal_num, renal_txt)]\n\nKey: &lt;trt, renal_num, renal_txt&gt;\n        trt renal_num renal_txt     N\n     &lt;char&gt;     &lt;int&gt;    &lt;char&gt; &lt;int&gt;\n1:  CAZ-AVB         0        No    25\n2:  CAZ-AVB         1       Yes     1\n3: Colistin         0        No    39\n4: Colistin         1       Yes     7\n\nn_win &lt;- 0\nn_tie &lt;- 0\nfor(i in 1:n_e){\n  for(j in 1:n_c){\n    if(d[trt == \"CAZ-AVB\"][i, renal_num] &lt; \n       d[trt == \"Colistin\"][j, renal_num]) n_win &lt;- n_win + 1\n    if(d[trt == \"CAZ-AVB\"][i, renal_num] == \n       d[trt == \"Colistin\"][j, renal_num]) n_tie &lt;- n_tie + 1\n  }\n}\npr_door_colistin &lt;- (n_win + 0.5 * n_tie)/(n_e*n_c)\n\nwhich gives 0.56 aligning with the shiny app results.\n\nGeneralised pairwise comparisons\nGPC is a related method and frankly it seems a bit better thought out than DOOR, but I am not sure that it is as popular [4]. The outcomes of interest are first ranked in terms of importance and the pairwise comparison is run progressively on each outcome for all pairs. For the ties under each outcome, the procedure moves on to the outcome that has the next highest priority and so on.\nWhile GPC can be used to produce a range of summary measures, the original paper used net treatment benefit (NTB).\n\\[\n\\begin{aligned}\nNTB = \\frac{ (n_{win} - n_{loss}) } { n_{win} + n_{loss} + n_{tie} }\n\\end{aligned}\n\\]\nwhere \\(n_{win} + n_{loss} + n_{tie}\\) is typically equal to the total number of pairwise comparisons.\nUnlike the DOOR approach, GPC allows for component level contribution and event level correlation. In contrast to the Win Ratio, the net treatment benefit incorporates ties.\nAs an example, consider a situation where we have outcomes, as above, for death, joint function, treatment success and QoL. The procedure first runs pairwise comparisons for all units on death and the number of wins, draws and losses recorded, demonstration below.\n\nset.seed(seed)\nN &lt;- 100\nd &lt;- data.table(\n  id = 1:(2*N),\n  # expt is 1\n  trt = rep(1:0, each = N)\n)\nd[, death := rbinom(.N, 1, prob = 0.4 - 0.2 * trt)]\nd[, jf := rbinom(.N, 1, prob = 0.6 - 0 * trt)]\nd[, success := rbinom(.N, 1, prob = 0.65 + 0.15 * trt)]\nd[, qol := rnorm(.N, 0 + 0.4 * trt, 1)]\n\nn_e &lt;- d[trt == 1, .N]\nn_c &lt;- d[trt == 0, .N]\nn_win &lt;- numeric(4)\nn_loss &lt;- numeric(4)\nn_tie &lt;- numeric(4)\n\n# create a grid to compute all comparisons (quicker than looping)\nsetkey(d, id)\nd_all &lt;- CJ(i = 1:100, j = 100 + (1:100))\n# death\nd_all[, death_i := d[i, death]]\nd_all[, death_j := d[j, death]]\n# note sign direction differs dependent on context of comparison\nd_all[death_i &lt; death_j, death_res := 1]\nd_all[death_i &gt; death_j, death_res := -1]\nd_all[death_i == death_j, death_res := 0]\n# jf\nd_all[, jf_i := d[i, jf]]\nd_all[, jf_j := d[j, jf]]\nd_all[jf_i &gt; jf_j, jf_res := 1]\nd_all[jf_i &lt; jf_j, jf_res := -1]\nd_all[jf_i == jf_j, jf_res := 0]\n# success\nd_all[, success_i := d[i, success]]\nd_all[, success_j := d[j, success]]\nd_all[success_i &gt; success_j,  success_res := 1]\nd_all[success_i &lt; success_j,  success_res := -1]\nd_all[success_i == success_j, success_res := 0]\n# success\nd_all[, qol_i := d[i, qol]]\nd_all[, qol_j := d[j, qol]]\nd_all[qol_i &gt;  qol_j, qol_res := 1]\nd_all[qol_i &lt;  qol_j, qol_res := -1]\nd_all[qol_i == qol_j, qol_res := 0]\nhead(d_all)\n\nKey: &lt;i, j&gt;\n       i     j death_i death_j death_res  jf_i  jf_j jf_res success_i success_j\n   &lt;int&gt; &lt;num&gt;   &lt;int&gt;   &lt;int&gt;     &lt;num&gt; &lt;int&gt; &lt;int&gt;  &lt;num&gt;     &lt;int&gt;     &lt;int&gt;\n1:     1   101       0       1         1     1     0      1         1         1\n2:     1   102       0       0         0     1     1      0         1         0\n3:     1   103       0       0         0     1     1      0         1         0\n4:     1   104       0       1         1     1     1      0         1         0\n5:     1   105       0       1         1     1     1      0         1         1\n6:     1   106       0       0         0     1     0      1         1         0\n   success_res    qol_i      qol_j qol_res\n         &lt;num&gt;    &lt;num&gt;      &lt;num&gt;   &lt;num&gt;\n1:           0 1.293674  1.0744410       1\n2:           1 1.293674  1.8956548      -1\n3:           1 1.293674 -0.6029973       1\n4:           1 1.293674 -0.3908678       1\n5:           0 1.293674 -0.4162220       1\n6:           1 1.293674 -0.3756574       1\n\n\nGPC calculations:\n\n# ntb on death is as follows:\nntb &lt;- numeric(4)\nnames(ntb) &lt;- c(\"death\", \"jf\", \"success\", \"qol\")\nd_res &lt;- d_all[, .N, keyby = death_res]\nd_res[, pct := N / nrow(d_all)]\n\nntb[\"death\"] &lt;- (d_res[death_res == 1, N] - d_res[death_res == -1, N]) /  nrow(d_all)\n\n# for the ties on death, compute jf:\nd_res &lt;- d_all[death_res == 0, .N, keyby = jf_res]\nd_res[, pct := N / nrow(d_all)]\nntb[\"jf\"] &lt;- (d_res[jf_res == 1, N] - d_res[jf_res == -1, N]) /  nrow(d_all)\n\n# for comparisons on all pairs, don't condition:\n# d_res &lt;- d_all[, .N, keyby = jf_res]\n# d_res[, pct := N / nrow(d_all)]\n# d_res\n# (d_res[jf_res == 1, N] - d_res[jf_res == -1, N]) /  nrow(d_all)\n\n# for the ties on death and jf, compute success:\nd_res &lt;- d_all[death_res == 0 & jf_res == 0, .N, keyby = success_res]\nd_res[, pct := N / nrow(d_all)]\nntb[\"success\"] &lt;- (d_res[success_res == 1, N] - d_res[success_res == -1, N]) / nrow(d_all)\n\n# for the ties on death, jf and success, compute qol:\nd_res &lt;- d_all[death_res == 0 & jf_res == 0 & success_res == 0, .N, keyby = qol_res]\nd_res[, pct := N / nrow(d_all)]\nntb[\"qol\"] &lt;- (d_res[qol_res == 1, N] - d_res[qol_res == -1, N]) / nrow(d_all)\n\nNote that for all endpoints, we use the total number of pairwise comparisons as the denominator and not the number of ties left over from the previous outcome.\nThe resulting net treatment benefit reported on each outcome:\n\nntb\n\n  death      jf success     qol \n 0.2300  0.0527  0.0574  0.0455 \n\n\nTaking the cumulative sum, progresses from the effect of each component through to an overall effect:\n\ncumsum(ntb)\n\n  death      jf success     qol \n 0.2300  0.2827  0.3401  0.3856 \n\n\nThe NTB is absolute measure ranging from -1 to 1 with zero being no effect. It estimates the probability that a random unit on the expt arm will do better than a random unit on the control arm minus the probability that a random unit on the control arm will do better than a random unit on the expt arm. For example, if \\(Pr(E&gt;C) = 0.7\\), then \\(Pr(E&lt;C) = 0.3\\) and \\(NTB = 0.7 - 0.3 = 0.4\\).\nYou can compute the overall effect directly with the following:\n\nn_win &lt;- d_all[death_res == 1, .N] + d_all[death_res == 0 & jf_res == 1, .N] +\n   + d_all[death_res == 0 & jf_res == 0 & success_res == 1, .N] +\n   + d_all[death_res == 0 & jf_res == 0 & success_res == 0 & qol_res == 1, .N]\n\nn_loss &lt;- d_all[death_res == -1, .N] + d_all[death_res == 0 & jf_res == -1, .N] +\n   + d_all[death_res == 0 & jf_res == 0 & success_res == -1, .N] +\n   + d_all[death_res == 0 & jf_res == 0 & success_res == 0 & qol_res == -1, .N]\n\n# n_ties &lt;- d_all[death_res == 0 & jf_res == 0 & success_res == 0, .N] \n\n(n_win - n_loss) / nrow(d_all)\n\n[1] 0.3856\n\n\nThe NTB is known to be the inverse of the number needed to treat, i.e. 1/ number of pt you need to trt to avoid one bad outcome. For large samples, inference can again be conducted via bootstrap. R provides the BuyseTest package that allows for stratification (beyond the implicit treatment level stratification).\n\nff1 &lt;- trt ~ bin(death, operator = \"&lt;0\") + bin(jf) + bin(success) + cont(qol)\nf1 &lt;- BuyseTest(ff1, data = d, trace = 0)\ns_f1 &lt;- summary(f1)\n\n       Generalized pairwise comparisons with 4 prioritized endpoints\n\n - statistic       : net treatment benefit  (delta: endpoint specific, Delta: global) \n - null hypothesis : Delta == 0 \n - confidence level: 0.95 \n - inference       : H-projection of order 1 after atanh transformation \n - treatment groups: 1 (treatment) vs. 0 (control) \n - neutral pairs   : re-analyzed using lower priority endpoints\n - results\n endpoint total(%) favorable(%) unfavorable(%) neutral(%) uninf(%)  delta\n    death   100.00        33.20          10.20      56.60        0 0.2300\n       jf    56.60        15.30          10.03      31.27        0 0.0527\n  success    31.27         9.86           4.12      17.29        0 0.0574\n      qol    17.29        10.92           6.37       0.00        0 0.0455\n  Delta CI [2.5% ; 97.5%]    p.value    \n 0.2300    [0.106;0.3469] 0.00032703 ***\n 0.2827   [0.1355;0.4177] 0.00022230 ***\n 0.3401   [0.1882;0.4761] 2.2250e-05 ***\n 0.3856     [0.2326;0.52] 2.6463e-06 ***\n\n\nIn the results, the totals, wins, loss and ties are presented as percentages rather than counts. For example, the total column effectively represents the proportion of pairs that carry over from one outcome to the next; for death there were 5660 pairs that carried over to joint function there were 3127 pairs that carried over to the treatment success outcome and so on. These can be visualised as:\n\nd_fig &lt;- data.table(s_f1)\nd_fig &lt;- d_fig[, 1:5]\nnames(d_fig) &lt;- c(\n  \"endpoint\", \"total\", \"wins\", \"losses\", \"tie\"\n)\nd_fig &lt;- melt(d_fig, id.vars = \"endpoint\")\nd_fig &lt;- d_fig[variable != \"total\"]\n\nggplot(d_fig, aes(x = endpoint, y = value, fill = variable)) +\n  geom_bar(stat='identity') +\n  scale_fill_discrete(\"\") +\n  scale_y_continuous(\"Percentage\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: Contribution by each endpoint\n\n\n\n\n\nInference can be conducted on all pairs for all outcomes by indicating that the hierarchical perspective is not required:\n\nf2 &lt;- BuyseTest(ff1, hierarchical = FALSE, data = d, trace = 0)\nsummary(f2)\n\n       Generalized pairwise comparisons with 4 endpoints\n\n - statistic       : net treatment benefit  (delta: endpoint specific, Delta: global) \n - null hypothesis : Delta == 0 \n - confidence level: 0.95 \n - inference       : H-projection of order 1 after atanh transformation \n - treatment groups: 1 (treatment) vs. 0 (control) \n - results\n endpoint weight total(%) favorable(%) unfavorable(%) neutral(%) uninf(%)\n    death   0.25      100        33.20          10.20      56.60        0\n       jf   0.25      100        26.64          17.64      55.72        0\n  success   0.25      100        30.80          13.80      55.40        0\n      qol   0.25      100        64.02          35.98       0.00        0\n  delta  Delta CI [2.5% ; 97.5%]    p.value    \n 0.2300 0.0575   [0.0272;0.0877] 0.00020121 ***\n 0.0900 0.0800    [0.037;0.1227] 0.00026848 ***\n 0.1700 0.1225   [0.0699;0.1744] 5.4724e-06 ***\n 0.2804 0.1926    [0.1296;0.254] 3.3839e-09 ***\n\n\n\n\nReferences\n\n\n1. Chamberlain J. Desirability of outcome ranking for status epilepticus. Research Methods in Neurology. 2023;101.\n\n\n2. De Schryver M. A tutorial on probabilistic index models: Regression models for the effect size p(Y1 &gt; Y2). American Psychological Association. 2019;24.\n\n\n3. Follmann D. Regression analysis based on pairwise ordering of patients’ clinical histories. Statistics in Medicine. 2002;21.\n\n\n4. Buyse M. Generalized pairwise comparisons of prioritized outcomes in the two-sample problem. Statistics in Medicine. 2010;29."
  },
  {
    "objectID": "notebooks/custom-distribution-stan.html",
    "href": "notebooks/custom-distribution-stan.html",
    "title": "User-defined Probability Distributions in Stan",
    "section": "",
    "text": "Overview\nSome of this material can be found in the stan user guide and this is solely to serve as a reference in my own words.\nTo implement, you just need to provide a function to increment the total log-probability appropriately.\n\n\n\n\n\n\nNote\n\n\n\nWhen a function with the name ending in *_lpdf* or *_lpmf* is defined, the stan compiler automatically makes a *_lupdf* or lupmf version. Only normalised custom distributions are permitted.\n\n\nAssume that we want to create a custom distribution per:\n\\[\n\\begin{aligned}\nf(x) &= (1-a) x^{-a}\n\\end{aligned}\n\\]\ndefined for \\(a \\in [0,1]\\) and \\(x \\in [0,1]\\) with cdf:\n\\[\n\\begin{aligned}\nF_x &= x^{a-1}\n\\end{aligned}\n\\]\nWe can generate draws from this distribution using the inverse cdf method:\n\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.9.0\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/mark/.cmdstan/cmdstan-2.36.0\n\n\n- CmdStan version: 2.36.0\n\nf_x &lt;- function(x, a){\n  if(a &lt; 0 | a &gt; 1) stop(\"only defined for a in [0,1]\")\n  if(any(x &lt; 0 | x &gt; 1)) stop(\"only defined for x in [0,1]\")\n  (1-a) * x ^ -a\n}\nF_x &lt;- function(x, a){\n  if(a &lt; 0 | a &gt; 1) stop(\"only defined for a in [0,1]\")\n  if(any(x &lt; 0 | x &gt; 1)) stop(\"only defined for x in [0,1]\")\n  x^(1-a)\n}\nF_inv_x &lt;- function(u, a){\n  if(a &lt; 0 | a &gt; 1) stop(\"only defined for a in [0,1]\")\n  if(any(u &lt; 0 | u &gt; 1)) stop(\"only defined for x in [0,1]\")\n  u ^ (1 / (1-a))\n}\n\na &lt;- 0.35\nx &lt;- seq(0, 1, len = 1000)\nd_fig &lt;- data.table(x = x, y = f_x(x, a))\nd_sim &lt;- data.table(\n  y_sim = F_inv_x(runif(1e6), a)\n)\n\nggplot(d_fig, aes(x = x, y = y)) +\n  geom_histogram(data = d_sim, aes(x = y_sim, y = ..density..),\n               inherit.aes = F, fill = 1, alpha = 0.2,\n               binwidth = density(d_sim$y_sim)$bw) + \n  geom_line() +\n  theme_bw()\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nfunctions {\n  real custom_lpdf(vector x, real alpha) {\n    \n    int n_x = num_elements(x);\n    vector[n_x] lpdf;\n    for(i in 1:n_x){\n      \n      lpdf[i] = log1m(alpha) - alpha * log(x[i]);\n    }  \n    return sum(lpdf);\n  }\n}\ndata {\n  int N;\n  vector[N] y;\n}\n\nparameters {\n  real&lt;lower=0, upper = 1&gt; a;\n}\nmodel {\n  target += exponential_lpdf(a | 1);\n  target += custom_lpdf(y | a);   \n}\n\n\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/custom-dist-1.stan\")\n\nld = list(\n  N = 1000, \n  y = d_sim$y_sim[1:1000]\n)\n\n\nf1 &lt;- m1$sample(\n  ld, iter_warmup = 1000, iter_sampling = 1000,\n  parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = F,\n  max_treedepth = 10)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.2 seconds.\n\nf1$summary(variables = c(\"a\"))\n\n# A tibble: 1 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 a        0.336  0.337 0.0223 0.0226 0.297 0.370  1.01     339.     504.\n\npost &lt;- data.table(f1$draws(variables = \"a\", format = \"matrix\"))\nhist(post$a)\n\n\n\n\n\n\n\n\n\n\nReferences"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "maj-biostat.github.io",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nWednesday 30 Jul 2025\n\n\nRole of DSMB for adaptive trials\n\n\n7 min\n\n\n\n\nWednesday 16 Jul 2025\n\n\nICH Adaptive designs for clinical trials E20\n\n\n12 min\n\n\n\n\nWednesday 9 Jul 2025\n\n\nOverdispersion 1\n\n\n5 min\n\n\n\n\nSaturday 5 Jul 2025\n\n\nSplines 1\n\n\n21 min\n\n\n\n\nThursday 13 Mar 2025\n\n\nStan implementation of AFT survival model with log-logistic event times\n\n\n12 min\n\n\n\n\nThursday 13 Mar 2025\n\n\nPower analysis by simulation\n\n\n4 min\n\n\n\n\nWednesday 20 Nov 2024\n\n\nDiscrete parameters in stan\n\n\n5 min\n\n\n\n\nWednesday 13 Nov 2024\n\n\nCOVID-19 Analyses\n\n\n5 min\n\n\n\n\nMonday 21 Oct 2024\n\n\nCONSORT using graphviz\n\n\n4 min\n\n\n\n\nThursday 10 Oct 2024\n\n\nPrincipal Stratification\n\n\n15 min\n\n\n\n\nFriday 4 Oct 2024\n\n\nMultinomial regression\n\n\n10 min\n\n\n\n\nThursday 3 Oct 2024\n\n\nLimitations of per-protocol analyses\n\n\n7 min\n\n\n\n\nMonday 30 Sep 2024\n\n\nProbabilistic Index Models\n\n\n4 min\n\n\n\n\nFriday 27 Sep 2024\n\n\nDesirability of Outcome Ranking (DOOR)\n\n\n12 min\n\n\n\n\nWednesday 25 Sep 2024\n\n\nRobust errors for estimating proportion\n\n\n4 min\n\n\n\n\nWednesday 25 Sep 2024\n\n\nMann-Whitney U\n\n\n14 min\n\n\n\n\nWednesday 25 Sep 2024\n\n\nUser-defined Probability Distributions in Stan\n\n\n2 min\n\n\n\n\nWednesday 18 Sep 2024\n\n\nRandom walk priors\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Biostatistician working mostly in the area of Bayesian adaptive clinical trials using R and stan. The site contains various posts/reminders on topics that are relevant to my day-to-day work."
  },
  {
    "objectID": "about.html#repository-status",
    "href": "about.html#repository-status",
    "title": "About",
    "section": "Repository status",
    "text": "Repository status\n\nlibrary(git2r)\nrepo &lt;- git2r::repository(path = \".\")\nsummary(repo)\n\nLocal:    main /Users/mark/Documents/project/website/src/maj-biostat.github.io\nRemote:   main @ origin (https://github.com/maj-biostat/maj-biostat.github.io)\nHead:     [1eb95c4] 2025-07-31: Update date\n\nBranches:         1\nTags:             0\nCommits:         87\nContributors:     2\nStashes:          0\nIgnored files:    3\nUntracked files: 51\nUnstaged files:   4\nStaged files:     0\n\nLatest commits:\n[1eb95c4] 2025-07-31: Update date\n[50cae60] 2025-07-31: Add two new posts (moved from regulatory that will be retired)\n[1d159c4] 2025-07-10: Update site\n[85795b3] 2025-07-10: Update splines to show sparsity model\n[ad8616f] 2025-07-10: Update site"
  },
  {
    "objectID": "notebooks/covid-19-vaccine-efficacy.html",
    "href": "notebooks/covid-19-vaccine-efficacy.html",
    "title": "COVID-19 Analyses",
    "section": "",
    "text": "library(data.table)\nlibrary(ggplot2)\nlibrary(gt)\n\nVaccine efficacy is defined as:\n\\[\nVE = 1\\left(1 - \\frac{\\pi_v}{\\pi_p} \\right)\n\\]\nwhere \\(\\pi_v\\), \\(\\pi_p\\) are the probabilities of infection under the vaccine and placebo (i.e. the risk of infection without vaccination), generally expressed as a percentage.\nThe lower the risk of infection on receipt of the vaccine relative to the risk of infection under placebo, the higher the VE with VE capped at 100%, \\(\\lim_{\\pi_v \\to 0} VE = 1\\). For the FDA to approve a vaccine, the VE must be at least 0.3 (30%).\nIf we can reasonably assume that \\(\\pi_v \\le \\pi_p\\) (implying the risk of infection is going to be lower on receipt of the vaccine than on placebo) then the lower bound of VE will be constrained to zero. However, when we cannot make this assumption and \\(\\pi_v &gt; \\pi_p\\) then VE may become negative.\nThe VE is given various representations via the risk-ratio, incidence rate ratio and hazard ratio, specifically:\n\\[\n\\begin{aligned}\nVE &= 1 - RR = 1 - \\frac{c_v/N_v}{c_p/N_p} \\\\\nVE &= 1 - IRR = 1 - \\frac{c_v/T_v}{c_p/T_p}  \\\\\nVE &= 1 - HR = 1 - \\frac{\\lambda_v}{\\lambda_p}\n\\end{aligned}\n\\]\nwhere \\(c_.\\) are the cases and \\(N_.\\) are the total number of participants, \\(T_.\\) is person-time and \\(\\lambda_.\\) are hazard rates.\nIf we take the second representation and set \\(r = T_v/T_p\\) as the ratio of person-time, then a few manipulations give us:\n\\[\n\\frac{c_v}{c_p} = r (1 - VE)\n\\]\nfrom which we can form a case proportion - the proportion of the total cases in the vaccinated group:\n\\[\n\\begin{aligned}\n\\theta &= \\frac{r (1 - VE)}{1 + r(1 - VE)} \\\\\n  &= \\frac{  \\frac{c_v}{c_p}   }{    \\frac{c_v}{c_p} + 1 } \\\\\n  &= \\frac{  \\frac{c_v}{c_p}   }{    \\frac{c_v}{c_p} + \\frac{c_p}{c_p} } \\\\\n  &= \\frac{  c_v   }{   c_v + c_p  }\n\\end{aligned}\n\\]\nwhich is a proportion between 0 and 1, and thus might plausibly admit to a beta distributional assumption.\nSome more manipulations gives a reparameterisation of VE:\n\\[\n\\begin{aligned}\n\\frac{1 + r(1 - VE)}{r (1 - VE)} &= \\frac{1}{\\theta} \\\\\n\\frac{1}{r (1 - VE)} + 1 &= \\frac{1}{\\theta} \\\\\n                         &= \\frac{1-\\theta}{\\theta}\\\\\nr (1 - VE)   &=  \\frac{\\theta}{1-\\theta} \\\\\n1 - VE &= \\frac{\\theta}{r(1-\\theta)} \\\\\nVE - 1 &= - \\frac{\\theta}{r(1-\\theta)} \\\\\n       &=   \\frac{\\theta}{r(\\theta-1)} \\\\\nVE  &= 1 + \\frac{\\theta}{r(\\theta-1)}\n\\end{aligned}\n\\]\nand therefore we can do inference on \\(\\theta\\) and derive VE from that.\nAccording to this paper by Polack, there were 8 cases in the vaccine group and 162 in the placebo group used in the Pfizer vaccine efficacy analysis. Additionally, the follow up times were 2.214 and 2.222 in 1000 person-years in the vaccine and placebo groups respectively.\nTherefore, we can compute the posterior on the case proportion as \\(\\theta \\sim \\text{Beta}(8 + 0.700102, 162 + 1)\\) and derive VE. By simulation:\n\nN_ptcl &lt;- 1e6\n\n# person time followup ratio\nr &lt;- 2.214 / 2.222\n\n# prior\ntheta_pri &lt;- rbeta(N_ptcl, 0.700102, 1)\nmu_theta_pri &lt;- 0.700102 / (1 + 0.700102)\nVE_pri &lt;- 1 + theta_pri / (1 * (theta_pri - 1))\n\n# posterior\ntheta &lt;- rbeta(N_ptcl, 8 + 0.700102, 162 + 1)\nVE &lt;- 1 + theta / (r * (theta - 1))\n\n\n# results\nd_theta &lt;- data.table(par = \"theta\", prior = theta_pri, posterior = theta)\nd_VE &lt;- data.table(par = \"VE\", prior = VE_pri, posterior = VE)\n\nFigure 1 shows the prior and posterior on the case proportion, Figure 2 shows the implied VE and Table 1 gives a summary of the posterior on the VE parameter.\nThe prior on \\(\\theta\\), the case proportion, implies a prior probability that VE is greater than 0.3 equal to 0.54. However, the posterior probability that VE is greater than 0.3 is 1.00.\n\n\n\n\n\n\n\n\nFigure 1: Prior and posterior density on case proportion\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Prior and posterior density on VE\n\n\n\n\n\n\n\n\nTable 1: Summary of posterior on VE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior (mean, 95% CrI)\n\n\n\nmean\n2.5%\n97.5%\n\n\n\n\nVE\n0.946\n0.903\n0.976"
  },
  {
    "objectID": "notebooks/discrete-parameters-stan.html",
    "href": "notebooks/discrete-parameters-stan.html",
    "title": "Discrete parameters in stan",
    "section": "",
    "text": "library(cmdstanr)\n\nThis is cmdstanr version 0.9.0\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/mark/.cmdstan/cmdstan-2.36.0\n\n\n- CmdStan version: 2.36.0\n\nlibrary(data.table)\nlibrary(ggplot2)\n\nHamiltonian Monte Carlo is usually used to estimate continuous parameters. However, sometimes we are interested in discrete parameters.\nHMC runs on log-probabilities. Therefore, as long as we can increment the log-probability associated with our model (whatever the model is), we can code it up, even if it uses discrete parameters. For models with discrete parameter, we have to ‘marginalise out’ the discrete parameter(s) and increment the log-density by the marginal density.\nBen Lambert has an example:\nConsider a series of \\(K\\) experiments where we get someone flips a coin a fixed number of times, \\(n\\), for each experiment, but we are never told \\(n\\). The individual uses the same coin across all the experiments and the probability of heads remains constant \\(\\text{Pr}(X = \\text{heads}) = \\theta\\). We define \\(X_k\\) as the number of heads obtained in each experiment \\(k\\) yielding \\((X_1, X_2, \\dots X_K)\\). Again, both \\(\\theta\\) and \\(n\\) are unknown to us; \\(\\theta\\) is continuous, but \\(n\\) is discrete.\nWe want to be able to make inference on both \\(\\theta\\) and \\(n\\), i.e. talk about the probability intervals for theta and the probability that \\(n\\) takes on certain values.\nAssume that we know \\(K = 10\\) experiments were run and the following data was observed \\(X_k = (2, 4, 3, 3, 3, 3, 3, 3, 4, 4)\\).\nWe adopt independent priors on \\(\\theta\\) and \\(n\\):\n\\[\n\\begin{aligned}\nn &\\sim \\text{Discrete-Unif}(5, 8) \\\\\n\\theta &\\sim \\text{Unif}(0, 1)\n\\end{aligned}\n\\]\nWe can write down the joint posterior of \\(\\theta\\) and \\(n\\), i.e. \\(\\text{Pr}(\\theta, n | X)\\) and then marginalise out the discrete parameter, \\(n\\). Once we have an expression that excludes the \\(n\\), then we can get stan to use that expression to conduct the sampling we want it to do. We have:\n\\[\n\\begin{aligned}\n\\text{Pr}(\\theta | X) &= \\sum_{n=5}^8 \\text{Pr}(\\theta, n | X)\n\\end{aligned}\n\\]\nStan runs on the log probability so we need to think in those terms:\n\\[\n\\begin{aligned}\n\\log(\\text{Pr}(\\theta | X)) &= \\log \\left(\\sum_{n=5}^8 \\text{Pr}(\\theta, n | X) \\right) \\\\\n&= \\log \\left(\\sum_{n=5}^8 \\exp( \\log( \\text{Pr}(\\theta, n | X))) \\right) \\\\\n\\end{aligned}\n\\]\nwhere the second line is to ensure we are dealing with log probabilities for both terms.\nIn stan, the above can be achieved in a mathematically stable way via the log-sum-exp function.\n\\[\n\\begin{aligned}\n\\log(\\text{Pr}(\\theta | X)) &= \\text{log\\_sum\\_exp}_{n=5}^8 \\left( \\log( \\text{Pr}(\\theta, n | X) ) \\right)\n\\end{aligned}\n\\]\nUnfortunately, we do not have \\(\\text{Pr}(\\theta, n | X)\\) but we can use Bayes rule to determine what it is:\n\\[\n\\begin{aligned}\n\\text{Pr}(\\theta, n | X) &\\propto \\text{Pr}(X | \\theta, n) \\text{Pr}(\\theta, n) \\\\\n&= \\text{Pr}(X | \\theta, n) \\text{Pr}(n)\\text{Pr}(\\theta)\n\\end{aligned}\n\\]\nwhere the second line comes from the fact that we use independent priors. Taking logs, we get:\n\\[\n\\begin{aligned}\n\\log(\\text{Pr}(\\theta, n | X)) &\\propto \\log(\\text{Pr}(X | \\theta, n)) +  \\log(\\text{Pr}(n)) + \\log(\\text{Pr}(\\theta)) \\\\\n\\end{aligned}\n\\]\n\nThe first term on the RHS is the likelihood for which we use the binomial distribution.\nThe second term is the log of the discrete uniform distribution that we defined earlier. For a given \\(n \\in \\{ 5,6,7,8 \\}\\) this is \\(\\log(1/4)\\) as we assume each option has equal probability.\nFinally, the third term is standard uniform. However, given that the third term does not contain \\(n\\), we do not actually need to include it in the expression for the joint distribution (although we do still include it in the model block as a standard uniform).\n\nThe above allows us to estimate models with discrete parameters by marginalising them out of the joint density. But, what if we want to do inference on the discrete parameters?\nAnswer; write down the unnormalised density of \\(n\\) conditional on \\(X\\) and estimate via MCMC:\n\\[\n\\begin{aligned}\nq(n | x) \\approx \\frac{1}{B} \\sum_{i = 1}^B q(n, \\theta_i | X)\n\\end{aligned}\n\\]\nwhere \\(B\\) is the number of MCMC samples and \\(\\theta_i\\) are the posterior samples. Essentially, this is averaging over \\(\\theta_i\\).\nTo get the normalised version, we need to form a simplex (sum of elements is 1 and elements are non-negative and less than 1) across the four possible values for \\(n\\) giving probabilities for \\(n = 5\\), \\(n = 6\\), \\(n = 7\\) and \\(n = 8\\). We can obtain this from:\n\\[\n\\begin{aligned}\np(n | x) &\\approx \\frac{q(n|X)}{\\sum_{n = 5}^8 q(n, | X)} \\\\\n&= \\frac{\\exp( \\log( q(n | x) ) )}{ \\exp( \\text{log\\_sum\\_exp} (\\log( q(n | X))) )}\n\\end{aligned}\n\\]\nand in stan, we would write this as:\n\\[\n\\begin{aligned}\np(n | x) = \\exp\\left[  \\log(q(n | X)) - \\text{log\\_sum\\_exp}(\\log( q(n | X)) ) \\right]\n\\end{aligned}\n\\]\nAn implementation for the above discussion is shown below:\n\n\n\n\ndata {\n  // num expt\n  int&lt;lower=0&gt; K;\n  array[K] int X;\n}\ntransformed data{\n  array[4] int n;\n  // these are the permissible values of n, \n  // i.e. 5, 6, 7, 8\n  for(i in 1:4){\n    n[i] = 4 + i;\n  }\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; theta;\n}\ntransformed parameters{\n  // unnormalised density\n  vector[4] lq;\n  for(i in 1:4){\n    // record the unnormalised density for every possible value \n    // of n\n    \n    // log pmf for the array of values in X conditional on a given n[i]\n    // and the parameter theta PLUS the prior on the given n, which is \n    // a discrete uniform, i.e. for each n, the prior is 0.25\n    lq[i] = binomial_lpmf(X | n[i], theta) + log(0.25);\n  }\n}\nmodel {\n  target += uniform_lpdf(theta | 0, 1);\n  \n  // marginalise out the troublesome n\n  target += log_sum_exp(lq);\n  \n}\ngenerated quantities{\n  // probability of n given X, i.e. the distribution of n | x\n  vector[4] p_n_X;\n  p_n_X = exp(lq - log_sum_exp(lq));\n  \n}\n\n\nRunning the model with the assumed data gives us our parameter estimates for both \\(\\theta\\) and \\(n\\).\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/discrete-param-1.stan\")\n\nld &lt;- list(\n  K = 10, X = c(2, 4, 3, 3, 3, 3, 3, 3, 4, 4)\n)\n\nf1 &lt;- m1$sample(\n    ld, iter_warmup = 1000, iter_sampling = 1000,\n    parallel_chains = 4, chains = 4, refresh = 0, show_exceptions = F,\n    max_treedepth = 10)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nf1$summary()\n\n# A tibble: 10 × 10\n   variable     mean     median    sd      mad       q5     q95  rhat ess_bulk\n   &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lp__     -14.9    -14.7      0.614 0.358    -1.61e+1 -14.4    1.00    1640.\n 2 theta      0.571    0.583    0.102 0.108     3.89e-1   0.723  1.00    1065.\n 3 lq[1]    -14.7    -13.7      2.19  0.852    -1.95e+1 -13.1    1.00    1299.\n 4 lq[2]    -15.6    -14.9      1.75  1.19     -1.92e+1 -14.0    1.00    1787.\n 5 lq[3]    -18.1    -16.9      3.61  2.98     -2.55e+1 -14.6    1.00    1414.\n 6 lq[4]    -21.7    -20.4      5.89  6.01     -3.31e+1 -15.1    1.00    1060.\n 7 p_n_X[1]   0.579    0.692    0.366 0.415     5.64e-3   0.993  1.00    1065.\n 8 p_n_X[2]   0.224    0.184    0.175 0.219     6.73e-3   0.508  1.00    1761.\n 9 p_n_X[3]   0.120    0.0230   0.157 0.0341    9.44e-6   0.436  1.00    1158.\n10 p_n_X[4]   0.0771   0.000653 0.166 0.000968  4.53e-9   0.501  1.00    1065.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nObviously, the above is somewhat contrived. We gnerally do not know the bounds on the discrete parameters. For example, how did we know that the bounds of \\(n\\) were 5 and 8? How would we have modified the model to account for observing a 6 in the data?"
  },
  {
    "objectID": "notebooks/dsmb-adaptive.html",
    "href": "notebooks/dsmb-adaptive.html",
    "title": "Role of DSMB for adaptive trials",
    "section": "",
    "text": "This is a summary from a youtube podcast presented by Scott Berry of Berry Consultants with guest Roger Lewis. The full presentation can be found here. The summary is my interpretation of the discussion and should not be taken as a complete representation of the discussion that actually occurred.\nThe role of the DSMB:\nGenerally groups of experts that are asked to keep an eye on the trial in order to keep participants from avoidable risk during the period of time that the trial is enrolling, which is also the period of time that the clinical investigators/research team are distanced from the trial data in order to ensure trial integrity.\nThe DSMB has three different levels of responsibility:\n\nprevent avoidable risk to the participants in the trial, including risks that could not been forseen when the trial was being designed\nprovide assurance that the scientific integrity of the trial is maintained while protecting trial participants\noperationalise the sponsor goals, for example, stopping for futility in full understanding of what the sponsor goals were, e.g. by familiarisation with documentation\n\nFor example, in an adaptive trial, there are specific rules in place that must be followed if the trial is to have the operating characteristics that we have claimed it should have.\nIn a classical trial, the DSMB is mainly monitoring data for safety signals or operational challenges that were not anticipated. In an adaptive trial, the role of the DSMB expands to ensure that the trial is conducted in the way that was intended as long as that continues to be ethically and scientifically appropriate. It is one thing to understand how a traditional trial is to be run but it is qualitatively more complicated to understand how an adaptive trial is to be conducted."
  },
  {
    "objectID": "notebooks/dsmb-adaptive.html#introduction",
    "href": "notebooks/dsmb-adaptive.html#introduction",
    "title": "Role of DSMB for adaptive trials",
    "section": "",
    "text": "This is a summary from a youtube podcast presented by Scott Berry of Berry Consultants with guest Roger Lewis. The full presentation can be found here. The summary is my interpretation of the discussion and should not be taken as a complete representation of the discussion that actually occurred.\nThe role of the DSMB:\nGenerally groups of experts that are asked to keep an eye on the trial in order to keep participants from avoidable risk during the period of time that the trial is enrolling, which is also the period of time that the clinical investigators/research team are distanced from the trial data in order to ensure trial integrity.\nThe DSMB has three different levels of responsibility:\n\nprevent avoidable risk to the participants in the trial, including risks that could not been forseen when the trial was being designed\nprovide assurance that the scientific integrity of the trial is maintained while protecting trial participants\noperationalise the sponsor goals, for example, stopping for futility in full understanding of what the sponsor goals were, e.g. by familiarisation with documentation\n\nFor example, in an adaptive trial, there are specific rules in place that must be followed if the trial is to have the operating characteristics that we have claimed it should have.\nIn a classical trial, the DSMB is mainly monitoring data for safety signals or operational challenges that were not anticipated. In an adaptive trial, the role of the DSMB expands to ensure that the trial is conducted in the way that was intended as long as that continues to be ethically and scientifically appropriate. It is one thing to understand how a traditional trial is to be run but it is qualitatively more complicated to understand how an adaptive trial is to be conducted."
  },
  {
    "objectID": "notebooks/dsmb-adaptive.html#skillset-and-process",
    "href": "notebooks/dsmb-adaptive.html#skillset-and-process",
    "title": "Role of DSMB for adaptive trials",
    "section": "Skillset and process",
    "text": "Skillset and process\nIt is absolutely necessary to have people on the DSMB that fully understand how the statistics are supposed to work and how they will work if some of the assumptions that were made in the design process do not play out. For example, in some settings, enrolment rates will be important on the design characteristics but in other settings variation from the assumed enrolment rates may not result in significant changes to the operating characteristics.\nIn terms of process, it is quite reasonable and actually very useful for the DSMB to have open discussions with the design team, the sponsor and regulatory agencies in order to understand the considerations made in the design, how it was developed, how it is supposed to work and how it was evaluated. These conversations can happen at any time before the DSMB has seen the unblinded data; after that time, the DSMB can only interact with the investigator team and sponsor through well-established lines of communication that are intended to maintain safety and validity of the participants and trial. Clearly, the process requires much more preparation up-front than for monitoring for a traditional trial design.\nThe bottom line is that you need people that understand the general theory but also the specific application of the design that is being overseen."
  },
  {
    "objectID": "notebooks/dsmb-adaptive.html#interactions-with-the-dsmb",
    "href": "notebooks/dsmb-adaptive.html#interactions-with-the-dsmb",
    "title": "Role of DSMB for adaptive trials",
    "section": "Interactions with the DSMB",
    "text": "Interactions with the DSMB\nDuring the period of open discussion with the DSMB, it is quite possible that they will be able to provide valuable insights (this might feel like criticism). However, if individuals on the DSMB are absolutely against the design, then consideration should be given to whether these people continue to serve on the DSMB."
  },
  {
    "objectID": "notebooks/dsmb-adaptive.html#dsmb-members",
    "href": "notebooks/dsmb-adaptive.html#dsmb-members",
    "title": "Role of DSMB for adaptive trials",
    "section": "DSMB members",
    "text": "DSMB members\nAdaptive trials are new and with this carry a number of abstract technical complexities that DSMB members will need to be familiarise themselves with. As a result, in contrast to traditional fixed trials, much more preparation is required so that a DSMB member can make relevant contributions. Additionally, this preparation phase occurs before the meetings begin, before any data is seen and before each meeting is much greater than a traditional trial and thus a much greater demand is placed on DSMB members. Consequently, but somewhat counter-intuitively, more junior professionals may be more appropriate for DSMB membership as they have more time available to them to do the preparation stage."
  },
  {
    "objectID": "notebooks/dsmb-adaptive.html#rules-vs-guidelines",
    "href": "notebooks/dsmb-adaptive.html#rules-vs-guidelines",
    "title": "Role of DSMB for adaptive trials",
    "section": "Rules vs guidelines",
    "text": "Rules vs guidelines\nIn group sequential studies, stopping rules often amounted to guidelines and so the DSMB had latitude to override stopping rules. However, in the context of contempoary adaptive designs, in most cases, the stopping rules are rules that must be adhered to in order to maintain the operating characteristics of the trials. If these rules are to be broken, there must be very explicit reasons for doing so and these must be discussed with the sponsor. For example, there may be an unanticipated safety signal that is directly relevant to the rule.\nConsider a futility stopping rule and the trial is evaluating treatments from a chronic degenerative disease. The first cohort of participants enrolled might be from a group with relatively long standing disease and that have been waiting for the trial to open up. This cohort may have systematically different (harder to treat) that people with new diagnosed disease. Here, a futility trigger may be appropriately interpreting the data, but we are considering the wrong population and later cohorts may have more favourable prognosis. This is a difficult situation to deal with, but is indicative of where a DSMB might face and might reasonably and justifiably be able to question the design rule.\nA similar scenario may come up in surgical settings where patients are enrolled globally and there is heterogeneity in how procedures work in the different regions. Again, the DSMB might become suspicious that the rule is missing an important consideration present in the data."
  },
  {
    "objectID": "notebooks/dsmb-adaptive.html#interactions-between-dsmb-and-statistical-analysis-committee",
    "href": "notebooks/dsmb-adaptive.html#interactions-between-dsmb-and-statistical-analysis-committee",
    "title": "Role of DSMB for adaptive trials",
    "section": "Interactions between DSMB and statistical analysis committee",
    "text": "Interactions between DSMB and statistical analysis committee\nThe statistical analysis committee (aka unblinded analytic group) are the group that perform analyses and present the results based on data provided from the trial data centres. The data have usually gone through initial quality control and cleaning but the data are not locked. The statistical analysis committee has the responsibility to perform an implement the specified analyses (in good faith effort given the state of the data) to evaluate and drive the decision rules. As a consequence of the detailed inspection of the data, the SAC become aware of aspects of the data that are complete, internally consistent, credible and so on, but also what problems exist and what parts of the data are impacted.\nAs an concrete example, the SAC may be running interim 3 and note that some of the participants that were present in interim 2 have had their treatment assignment changed (or even their outcome). This is important information for the DSMB to get a sense of how confident they can be in the data quality, especially in relation to marginal decision triggers."
  },
  {
    "objectID": "notebooks/dsmb-adaptive.html#blinding-the-dsmb",
    "href": "notebooks/dsmb-adaptive.html#blinding-the-dsmb",
    "title": "Role of DSMB for adaptive trials",
    "section": "Blinding the DSMB",
    "text": "Blinding the DSMB\nA question often arises as to whether the DSMB should be blinded to treatment assignment.\nFundamentally, the DSMB is tasked with balancing efficacy and safety, but these considerations are nearly never symmetric. For example, a DSMB will want to continue a trial when it is looking like a new treatment is helpful, but will not need anywhere near the same level of certainty to make a safety decision. As such, all information in the reports should be labelled explicitly with the true treatment assignments (no A vs B, no switching etc, no obfuscation whatsoever). For a detailed discussion of this point, see Masked monitoring in Clinical Trials, NEJM (1998).\nAs a final example, consider a study where the first interim for efficacy is at one-year but a first meeting is run at 6-months for an initial check of the data. The question arises whether the DSMB should see efficacy at this point and the answer to this is found in considering the fundamental role of the DSMB - to balance efficacy and safety. Given this fundamental role, the DSMB should still be considering the efficacy/safety, even at the initial look at the data and should therefore see both efficacy and safety. The level of safety considerations that the DSMB will tolerate is directly related to the amount of benefit that the participants may receive."
  },
  {
    "objectID": "notebooks/ich-adaptive-designs-e20.html",
    "href": "notebooks/ich-adaptive-designs-e20.html",
    "title": "ICH Adaptive designs for clinical trials E20",
    "section": "",
    "text": "Per the ICH blurb\nThe E20 EWG is working on the development of a new E20 Guideline on “Adaptive Clinical Trials” on the design, conduct, analysis, and interpretation of adaptive clinical trials that provides a transparent and harmonized set of principles for the regulatory review of these studies in a global drug development program. These principles should also provide the flexibility to evaluate / discuss innovative approaches to clinical trial design throughout the development process.\nStep 2b of the guideline was approved in June 2025 and step 3 is open for public consultation. The focus of the guideline is on the principles that should be applied in planning, conduct, analysis and interpretation. The principles are relevant to all phases of development but it does not discuss specific statistical methods.\nThis set of notes provides a summary of key points, it is not aiming to be a substitute for the guidance; so if you want to know the full scoop, RTFM. Some familiarity with adaptive designs is assumed. AD are a type of clinical trial that allow for prospectively planned modifications to one or more aspects of the trial based on interim data.\nAdvantages of ADs include improved flexibility and the ability to guard against uncertain initial assumptions, they can offer some benefit to the participants (ethical) and they can improve efficiency. However, these designs pose a number of challenges.\nADs can make maintaining confidentiality difficult and they can be challenging to plan, which can result in higher costs and longer lead times. One of the key points made is that excessive design complexity can lead to issues with trial integrity, e.g. interpretation the results. The designs are also not suitable when you have fast enrolment because there is no time to evaluate the data and make adaptations before the target sample size is reached. Other issues raised include type-i error control and the potential for biased treatment effects. For any design, there needs to be a compelling justification, i.e. you don’t just do a platform trial or introduce RAR because it seems like it might be a nice idea, you need to propose how the design addresses a need and the approach should be contrasted to alternative designs, including non-adaptive ones."
  },
  {
    "objectID": "notebooks/ich-adaptive-designs-e20.html#introduction",
    "href": "notebooks/ich-adaptive-designs-e20.html#introduction",
    "title": "ICH Adaptive designs for clinical trials E20",
    "section": "",
    "text": "Per the ICH blurb\nThe E20 EWG is working on the development of a new E20 Guideline on “Adaptive Clinical Trials” on the design, conduct, analysis, and interpretation of adaptive clinical trials that provides a transparent and harmonized set of principles for the regulatory review of these studies in a global drug development program. These principles should also provide the flexibility to evaluate / discuss innovative approaches to clinical trial design throughout the development process.\nStep 2b of the guideline was approved in June 2025 and step 3 is open for public consultation. The focus of the guideline is on the principles that should be applied in planning, conduct, analysis and interpretation. The principles are relevant to all phases of development but it does not discuss specific statistical methods.\nThis set of notes provides a summary of key points, it is not aiming to be a substitute for the guidance; so if you want to know the full scoop, RTFM. Some familiarity with adaptive designs is assumed. AD are a type of clinical trial that allow for prospectively planned modifications to one or more aspects of the trial based on interim data.\nAdvantages of ADs include improved flexibility and the ability to guard against uncertain initial assumptions, they can offer some benefit to the participants (ethical) and they can improve efficiency. However, these designs pose a number of challenges.\nADs can make maintaining confidentiality difficult and they can be challenging to plan, which can result in higher costs and longer lead times. One of the key points made is that excessive design complexity can lead to issues with trial integrity, e.g. interpretation the results. The designs are also not suitable when you have fast enrolment because there is no time to evaluate the data and make adaptations before the target sample size is reached. Other issues raised include type-i error control and the potential for biased treatment effects. For any design, there needs to be a compelling justification, i.e. you don’t just do a platform trial or introduce RAR because it seems like it might be a nice idea, you need to propose how the design addresses a need and the approach should be contrasted to alternative designs, including non-adaptive ones."
  },
  {
    "objectID": "notebooks/ich-adaptive-designs-e20.html#topics",
    "href": "notebooks/ich-adaptive-designs-e20.html#topics",
    "title": "ICH Adaptive designs for clinical trials E20",
    "section": "Topics",
    "text": "Topics\nAfter the preamble, the guideline is made of up four sections\n\nKey principles\nTypes of adaptations\nSpecial topics\nDocumentation\n\nThe key principles sections are essentially “must-haves” in any adaptive trial that ensure the results are reliable and can be interpreted. Types of adaptations covers the most common adaptations that are used in ADs. Special topics a miscellaneous grab bag of items but includes discussion of simulation. Documentation covers a small set of the topics that should be addressed in documenting an AD."
  },
  {
    "objectID": "notebooks/ich-adaptive-designs-e20.html#key-principles",
    "href": "notebooks/ich-adaptive-designs-e20.html#key-principles",
    "title": "ICH Adaptive designs for clinical trials E20",
    "section": "Key principles",
    "text": "Key principles\nThese are critical considerations in any adaptive trial.\n\nAdequacy within the development program\nTrials should build upon evidence moving from exploratory to confirmatory stages. In the confirmatory setting, adaptations shoud be limited. The key idea is that adaptation should not be used to completely replace the exploratory phase. For example, if dose-ranging has not been conducted then dose selection within a confirmatory setting is not well founded.\n\n\nAdequacy of planning\nPredictably, the planning considerations that need to be specified for adaptive trials include:\n\nnumber of interim analyses\ntiming of interim analyses\ntypes of adaptations\nmethods to be used for interim analyses\nrules governing adaptations\nmethods for the primary analysis aligned to the targeted estimand\napproaches to maintain trial integrity\n\nUnblinded adaptation is raised as a particular risk that needs to be managed and simulations are raised, especially their documentation. An interesting point is that the planning stage should involve the IDMC to confirm their understanding and support so that the panel is able to review interim results and make adaptation recommendations. A key point is specification of whether rules are to be binding or not. Non-binding rules are potentially useful where flexibility is needed to deal with safety issues, but the trial specification needs to be clear about how permissive deviations can be and what leads to them. Again, clarity in the specification is emphasised.\n\n\nMinimise errors\nThe section deals with error control, mainly minimising false positive efficacy conclusions within a frequentist framework but also false negatives (related to power). The discussion raises both type-i and multiplicity adjustments and refers the reader on to ICH E9 for consideration of when non-frequentist approaches can be used. Nothing much further of note.\n\n\nReliability\nThis section basically considers with bias and variability of the effect estimates.\nOne of the considerations that is pointed out relates to selecting treatment arms from a multi-arm design on the basis of the largest effects as this can lead to over estimation of the effect. Another point is how secondary endpoints may be biased in the presence of adaptation, but there wasn’t any detail on this.\n\n\nTrial integrity\nThis section is quite a bit longer than the others. The central goal of trial integrity is to ensure that the objectives are met in a reliable, ethical and timely manner.\nAllocation concealment and blinding are raised, as is ensuring that summaries of accumalating data are controlled so that expectations and behaviours are not impacted. As an example, enrolment rates might be impacted through careless handling of data, especially if effect sizes based on the accruing data are small (and therefore interpreted as no effect). Ideally, everyone is blinded to individually treatment assignment and accumulating summary level data. If blinding is not possible for everyone then the personnel that have access to accumulating data should at least be independent in the sense that they do not have any role in trial activities."
  },
  {
    "objectID": "notebooks/ich-adaptive-designs-e20.html#types-of-adaptation",
    "href": "notebooks/ich-adaptive-designs-e20.html#types-of-adaptation",
    "title": "ICH Adaptive designs for clinical trials E20",
    "section": "Types of adaptation",
    "text": "Types of adaptation\nThis section deals with some of the key adaptations that might be encountered.\n\nEarly stopping\nThe main points that are made in this section are:\n\nappropriate control of type-i error\ntiming analyses so that if stopping occurs, the results would be sufficiently persuasive\nlogistical considerations (DSMC availability)\nensuring that sufficient data accrues for meaningful secondary and safety outcomes (and subgroups)\n\nThese all point to ensuring that interim analyses do not start too early and having solid justification for the stopping rule (e.g. ethics).\nWith regards to the use of the data, the guidance indicates that two versions of the final analysis should be presented, the first based on the data to the time that the decision was triggered and the second that is based on that data plus any that continued to accrue due to delays between the decision being reached and suspension of enrolment. Clearly, this leads to the possibility of contradicting or revised results and these instances need to be assessed and explained in the reporting to ensure that the results are interpretable.\nFutility rules are recommended to be non-binding and thus type-i should be controlled in independent of these rule types.\n\n\nSample size adaptation\nSample size adapatation is where the initial sample size calculation is revisited based on the accruing data. Many times people think early stopping is sample size adaptation, it isn’t. The goal with sample size adapatation is to ensure that sufficient power is available to address the trial goals, in light of the uncertainty that is resolved on design parameters. In practice, for researcher led trials, this isn’t a very practical adapatation element.\nWithin this adaptation, trial integrity should be considered such that re-estimation is undertaken blinded to treatment assignment.\n\n\nPopulation selection\nOtherwise known as enrichment, conceives an adaptation rule based on treatment effect heterogeneity across population strata. For example, if effects are identified to be present in older but not younger cohorts, we might focus subsequent enrolment on the older cohort. Care needs to be taken against bias as cohorts are based on treatment effects and that the approach is based on a scientific rationale for the existence of heterogeneity.\nThis adaptation has implications on the analysis approach, which should shift to be primarily focused on the enriched population, although all of the data should still be used (pooled).\n\n\nTreatment selection\nMulti-arm trials may involve considerations to adapt based on arm performance with only better arms continuing over the entire study (e.g. as in dose selection). In this setting flexibile rules are recommended to allow for the full scope of information to be assessed. The guidance indicates that methods to reduce bias should be considered, but is silent on what methods might be used.\n\n\nParticipant allocation\nThe section focuses on RAR and covariate adapted allocation. The challenges of RAR are discussed, particularly inflation of type-i and the potential for bias in the presence of time trends. For example, a RAR design would be more likely to show a false positive if earlier-enrolled participants are both more likely to be assigned to control and to have a poor prognosis (e.g., because of changes in background care or participant characteristics over time) than later-enrolled participants. An additional concern is how sample size might be overly restricted in some arms to the point where inference is challenging.\nIf RAR is to be used, the specific RAR procedure should be specified and justified which includes accounting for time trends. One approach is to restrict RAR to a single or small number of interim analyses.\nThe logistical and trial integrity considerations are disussed. Specifically, allocation updates should be maintained confidentially so that information on treatment effects is not revealed.\nFinally, deterministic rules for RAR are strongly discouraged."
  },
  {
    "objectID": "notebooks/ich-adaptive-designs-e20.html#special-topics",
    "href": "notebooks/ich-adaptive-designs-e20.html#special-topics",
    "title": "ICH Adaptive designs for clinical trials E20",
    "section": "Special topics",
    "text": "Special topics\n\nData monitoring\nThe IDMC for adaptive designs should have the expertise required to make informed decisions for these kinds of trials. They need to have at least one statistician that are experienced in the area of adaptive designs and a charter needs to be produced in order to guide the IDMC (meeting schedule, logistics, confidentiality procedures, process for making recommendations).\nWithin the trial team, the set of analysts responsible for implementing the interim analyses should be carved out and these should not be members of the monitoring committee. Ideally, these analysts would be independent from the sponsor.\nWhen the IDMC makes a recommendation then information may be communicated to the sponsor to allow them to make the final call on whether the trial should be stopped. Again, any unblinding should be carefully considered.\n\n\nSimulations\nThis appeared to be the largest section in the document.\nWhat simulations are and their intended goals are discussed along with how much detail (e.g. addressing missingness or not) should be implemented and their merit in comparing adaptive versus fixed designs.\nIn terms of setting up a simulation, the guidance indicates that the focus and objectives should be thought through up front. This includes the selection of a benchmark design and analysis approach that should be a well-understood design.\nOptions for variations across simulations may look at number and timing of interim analyses, types of adaptations, stopping rules and testing procedures. Nuisance parameters are raised as a key consideration.\nThe operating characteristics to be reported include type-i, expected sample size, trial duration, power, coverage, bias, mean squared error of treatment effect, cumulative stopping probabilities. Different summary statistics for each might have merit.\nThe assumptions for the simulations should be founded ideally on real data (e.g. published sources) rather than opinion.\nThe implementation details for the simulations need to be described and justified, which includes the data-generating process, model specification number of simulations per scneario sufficient for the desired precision of summary statistics. The design, results and conclusions of the simulation study need to be documented. Include:\n\nKey questions to be addressed\nClinical trial design and analysis options to be evaluated\nChoice of operating characteristics to be assessed\nExisting knowledge used to inform simulations\nRange of parameter configurations used along with justification\nImplementation details (DGP, number of trials per simulation)\nSoftware and code used along with detailed instructions on how to recreate simulations\nSummary of results, interpretation and conclusions\nExample trials\nLimitations of the simulation results\nClinical discussion on the extent to which the simulations address key questions\n\nThe guidance strongly emphasises the documentation requirements.\n\n\nBayesian methods\nThe guidance doesn’t include anything surprising under this section. Borrowing information is explicitly discussed, specifically, the use of informative priors, the use of external data. Priors are still a point of contention and the need for justification is strongly stated. Sensitivity analyses are recommended.\nStragely, partial-pooling and the use of hierarchical models isn’t mentioned.\n\n\nTime to event studies\nFor time to event studies, the focus of events rather than number of participants is a key consideration. As such, adaptive designs based on time to event endpoints might (and probably should) be evaluated with respect to events rather than target sample sizes. This flows over into the types of adaptations that might be considered; sample size re-estimation becomes number-of-events re-estimation etc. Adaptations should rely on the primary endpoint (not secondary markers which can violate the independence assumption). In general, the participants that inform interim analyses should only be those that have reached their endpoint.\n\n\nExploratory trials\nThe section highlights that exploratory trials can (relative to confirmatory trials) incorporate more adaptations. This is specifically geared towards early stage trials.\n\n\nOperational aspects\nOne of the main aspects of this section disussed the need to minimise information transfer so that things don’t creep out of the interim results and thereby impact trial integrity. Another aspect is the requirement to be clear with participants about the possibility of adaptation, why they might happen and the impacts on them.\nInfrastructure requirements is another key point."
  },
  {
    "objectID": "notebooks/ich-adaptive-designs-e20.html#documentation",
    "href": "notebooks/ich-adaptive-designs-e20.html#documentation",
    "title": "ICH Adaptive designs for clinical trials E20",
    "section": "Documentation",
    "text": "Documentation\nThe range of documentation is discussed such that the design can be adequately evaluated.\n\nPre-trial documentation\nNothing particularly surprising in this section. The documentation needs to incorporate\n\nrationale (clinical and statistical)\nproposed adaptations and their specification\nmethods\nprocedures (who will do what)\nconfidentiality considerations\nsimulations\n\nEstimands should be documented in the protocol.\n\n\nMarketing applications\nThere wasn’t much in here that is currently relevant to me. Basically, it is just about how you justify your results such that regulatory approval can be obtained."
  },
  {
    "objectID": "notebooks/log-logistic-aft-in-stan.html",
    "href": "notebooks/log-logistic-aft-in-stan.html",
    "title": "Stan implementation of AFT survival model with log-logistic event times",
    "section": "",
    "text": "Code\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(gt)\nsuppressPackageStartupMessages(library(survival))\nsuppressPackageStartupMessages(library(flexsurv))\noptions(scipen=999)\n\n\nIn a parametric AFT model, the effect of covariates is to speed or slow down time.\n\\[\n\\begin{aligned}\n\\log(T) = X\\gamma + \\text{error}\n\\end{aligned}\n\\]\nWhere:\n\n\\(T\\) is the survival time\n\\(X\\gamma\\) is the linear predictor\n\nand the error term is made up of a scale parameter \\(\\sigma\\) and a random variable \\(W\\) with a specific distribution. In the usual setup, we observe the event/censoring indicator and the associated event or censoring time \\(C\\), with the event and censoring process assumed to be independent.\nFor the log-logistic model, the residual distribution is determined by the shape parameter. If \\(\\log(T) = X\\gamma + \\sigma W\\) where \\(W\\) has a logistic distribution then \\(T\\) follows a log-logistic distribution with scale parameter \\(\\alpha = \\exp(X\\gamma)\\) and shape parameter \\(\\beta = 1/\\sigma\\). For further reference see section 2.2.4 in [1], chapter 13 of [2], chapter 6 of [3] (possibly the clearest explanation) and [4].\nThe hazard function associated with log-logistic event times is hump-shaped, a bit like the log normal case but with longer tails. It initially increases, reaches a maximum and then decreases toward 0 as lifetimes become larger and larger. Definitions for the density function can be found in the stan docs: https://mc-stan.org/docs/functions-reference/positive_continuous_distributions.html#log-logistic-distribution and in the flexsurv help file, see ?flexsurv::dllogis. Unlike lognormal, the log-logistic has a closed form hazard function.\n\\[\n\\begin{aligned}\nf = \\frac{(\\beta/\\alpha)(t/\\alpha)^{\\beta-1}}{(1 + (t/\\alpha)^\\beta)^2}\n\\end{aligned}\n\\]\nwith shape parameter \\(\\beta &gt;0\\) and scale parameter \\(\\alpha &gt;0\\). The cumulative distribution function is\n\\[\n\\begin{aligned}\nF = \\frac{1}{1 + (t/\\alpha)^{-\\beta}}\n\\end{aligned}\n\\]\nthe survival function is \\(1 - F\\):\n\\[\n\\begin{aligned}\nS &= 1 - \\frac{1}{1 + (t/\\alpha)^{-\\beta}} \\\\\n  &= \\frac{1 + (t/\\alpha)^{-\\beta}}{1 + (t/\\alpha)^{-\\beta}} - \\frac{1}{1 + (t/\\alpha)^{-\\beta}} \\\\\n  &= \\frac{(t/\\alpha)^{-\\beta}}{1 + (t/\\alpha)^{-\\beta}} \\\\\n  &= \\frac{1}{(t/\\alpha)^\\beta (1 + (t/\\alpha)^{-\\beta})} \\\\\n  &= \\frac{1}{1 + (t/\\alpha)^{\\beta}} \\\\\n\\end{aligned}\n\\]\nthe hazard function is \\(f/S\\):\n\\[\n\\begin{aligned}\nh &= \\frac{\\frac{(\\beta/\\alpha)(t/\\alpha)^{\\beta-1}}{(1 + (t/\\alpha)^\\beta)^2}}{\\frac{1}{1 + (t/\\alpha)^{\\beta}}} \\\\\n  &= \\frac{(\\beta/\\alpha)(t/\\alpha)^{\\beta-1} (1 + (t/\\alpha)^{\\beta}) }{(1 + (t/\\alpha)^\\beta)^2} \\\\\n  &= \\frac{(\\beta/\\alpha)(t/\\alpha)^{\\beta-1}}{1 + (t/\\alpha)^\\beta} \\quad \\text{cancelling similar terms}\n\\end{aligned}\n\\]\nSay that we want to simulate data where there was 10% cumulative incidence by day 360, e.g. in the first 360 days of life about 10% of infants will experience a medical attendance for RSV, a respiratory illness.\nWe want \\(S(360) = \\frac{1}{1 + (360/\\alpha)^\\beta} = \\pi = 0.9\\). As \\(\\alpha\\) is the scale parameter, which is usually modelled as a linear function of parameters (treatment effects etc), assume that \\(\\beta\\) is known and solve for \\(\\alpha\\)\n\\[\n\\begin{aligned}\n\\alpha = \\frac{360}{((1/\\pi) - 1)^{1/\\beta}}\n\\end{aligned}\n\\]\nFor example, say \\(\\beta = 2\\), this implies \\(\\alpha = \\frac{360}{((1/0.9) - 1)^{1/2}} = 1080\\) and gives the functional forms as shown below. Setting the survival probability to 0.5 and solving for time gives the median survival time under these parameters, i.e \\(1080 \\times 1^{1/\\beta} = 1080\\). Obviously, these values are just for demonstration and can be calibrated to subject matter expertise as necessary for simulating trial designs etc.\n\nCode\n# log-logistic parameters\n# shape parameter\nb &lt;- 2\n# scale\na &lt;- 360 / ( (1/0.9)-1 )^(1/b)    \n\n# Create a data.table with days from 1 to 360\ndt &lt;- data.table(day = 1:1080)\n\n# Compute the survival function S(t) = 1 / (1 + (t/a)^b)\ndt[, survival := 1 / (1 + (day / a)^b)]\n\n# Compute the density f(t) = (gamma/alpha) * (t/alpha)^(gamma-1) / [1 + (t/alpha)^gamma]^2\ndt[, density := (b / a) * (day / a)^(b - 1) / (1 + (day / a)^b)^2]\n\n# Compute the hazard function h(t) = f(t) / S(t)\ndt[, hazard := density / survival]\n\n# Plot the survival curve\nggplot(dt, aes(x = day, y = survival)) +\n  geom_line(color = \"blue\", lwd = 0.4) +\n  geom_vline(xintercept = 360, lwd = 0.2) +\n  labs(title = \"Survival Curve for RSV (Log-Logistic Model)\",\n       x = \"Day of Life\", y = \"Survival Probability S(t)\") +\n  scale_y_continuous(\"Survival S(t)\", limits = c(0.5, 1), seq(0.5, 1, by = 0.1)) +\n  theme_minimal()\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nCode\n# Plot the hazard function\nggplot(dt, aes(x = day, y = hazard)) +\n  geom_line(color = \"darkgreen\", lwd = 0.4) +\n  geom_vline(xintercept = 360, lwd = 0.2) +\n  labs(title = \"Hazard Function for RSV (Log-Logistic Model)\",\n       x = \"Day of Life\", y = \"Hazard h(t)\") +\n  theme_minimal()\n# Plot the density function\nggplot(dt, aes(x = day, y = density)) +\n  geom_line(color = \"red\", lwd = 0.4) +\n  geom_vline(xintercept = 360, lwd = 0.2) +\n  labs(title = \"Density Function for RSV (Log-Logistic Model)\",\n       x = \"Day of Life\", y = \"Density f(t)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn an AFT model, which is appropriate when we are more concerned with direct assessment of event times (AFT is also a way to work around non-proportional hazards) rather than a focus on instantaneous risk, the scale parameter is allowed to vary with the covariates, such as:\n\\[\n\\begin{aligned}\n\\alpha_i &= \\exp(\\mu_i) \\\\\n\\mu &= \\gamma_0 + \\gamma_1 x_1 + \\dots\n\\end{aligned}\n\\]\nThe density for observation \\(i\\) is then:\n\\[\n\\begin{aligned}\nf(t_i) &= \\frac{\\beta}{\\exp(\\mu_i)} \\left( \\frac{t_i}{\\exp(\\mu_i)}\\right)^{\\beta - 1} \\left[  1 + \\left(\\frac{t_i}{\\exp(\\mu_i)} \\right)^\\beta \\right]^{-2}\n\\end{aligned}\n\\]\ntaking logs of this gives the log-likelihood for observation \\(i\\):\n\\[\n\\begin{aligned}\n\\log f(t_i) &= \\log(\\beta) - \\mu_i + (\\beta - 1)\\left[ \\log(t_i) - \\mu_i \\right]   -2 \\log \\left( 1 + (t_i/\\exp(\\mu_i))^\\beta    \\right)\n\\end{aligned}\n\\]\nfor the right censored records, the survival function is used:\n\\[\n\\begin{aligned}\nS = \\frac{1}{1 + (t_i/\\exp(\\mu_i))^\\beta}\n\\end{aligned}\n\\]\ntaking logs:\n\\[\n\\begin{aligned}\n\\log S &= 0 + \\log \\left[ 1 + \\left( \\frac{t_i}{\\exp(\\mu_i)} \\right)^\\beta        \\right]\n\\end{aligned}\n\\]\nImplement stan model:\n\n\n// Log-logistic AFT model\ndata {\n  int&lt;lower=0&gt; N;             // Number of observations\n  int&lt;lower=0&gt; P;             // Number of predictors\n  matrix[N, P] X;             // Predictor matrix X[, 1] is intercept\n  vector&lt;lower=0&gt;[N] y;       // Observed survival times\n  vector&lt;lower=0, upper=1&gt;[N] event;  // Event indicator (1=event, 0=censored)\n  \n  int N_pred;\n  vector[N_pred] t_surv;    // time to predict survival at\n  \n  // prior\n  vector[2] mu0_gamma;   // e.g. c(5, 0)\n  vector[2] sd0_gamma;   // e.g. c(2, 2)\n  \n  real rho_shape;        // e.g. 0.5 to 1\n  \n}\n\nparameters {\n  vector[P] gamma;             // Regression coefficients for scale\n  real&lt;lower=0&gt; shape;        // Shape parameter (b in the formula)\n}\n\ntransformed parameters {\n  // Location parameter (log-scale)\n  vector[N] mu;      \n  \n  mu = X * gamma;\n}\n\nmodel {\n  // Priors - arbitrary at the moment\n  target += normal_lpdf(gamma[1] | 5, 3);\n  target += normal_lpdf(gamma[2] | 0, 2);\n  target += exponential_lpdf(shape | 0.5);\n  \n  // Likelihood\n  for (i in 1:N) {\n    if (event[i] == 1) {\n      // For observed events, use the log-logistic density\n      target += log(shape) - mu[i] + (shape - 1) * (log(y[i]) - mu[i]) - \n                2 * log1p(pow(y[i] / exp(mu[i]), shape));\n    } else {\n      // For censored observations, use the log survival function\n      target += -log1p(pow(y[i] / exp(mu[i]), shape));\n    }\n  }\n}\n\ngenerated quantities {\n  vector[N_pred] surv0;\n  vector[N_pred] surv1;\n  \n  real scale0;\n  real scale1;\n  \n  // these equate to the median survival time\n  scale0 = exp(gamma[1]);\n  scale1 = exp(gamma[1] + gamma[2]);\n  \n  for(i in 1:N_pred){\n    surv0[i] =  1 / (1 + pow(t_surv[i]/exp(gamma[1]),  shape));\n    surv1[i] =  1 / (1 + pow(t_surv[i]/exp(gamma[1] + gamma[2]),  shape));\n  }\n  \n  \n}\n\n\nRunning the model with the assumed data gives parameter estimates.\n\n\nCode\nmod_01 &lt;- cmdstanr::cmdstan_model(\"stan/log-logistic-aft-01.stan\")\n\n# Simulation parameters\nN &lt;- 2000 \n\ngamma_true &lt;- c(log(1080), 1)  \n# True shape parameter\nshape_true &lt;- 2  \n\n# Simulate covariates\nsimulate_data &lt;- function(\n    N = 2000,\n    gamma_true = c(log(1080), 0.3)  ,\n    shape_true = 2  ,\n    t_cen = 360\n    ) {\n  \n  d &lt;- data.table(\n    trt = rep(0:1, length  = N)\n  )\n  \n  d[trt == 0, scale := exp(gamma_true[1])]\n  d[trt == 1, scale := exp(gamma_true[1] + gamma_true[2])]\n  \n  d[, t_evt := flexsurv::rllogis(.N, shape = shape_true, scale = scale)]\n  d[, evt := as.numeric(t_evt &lt;= t_cen)]\n  d[evt == 1, t_evt_obs := t_evt]\n  # Assume everyone is followed up to 360 days. If you are doing this \n  # incrementally then you need to consider the minimum of the censoring\n  # or follow up time.\n  d[evt == 0, t_evt_obs := t_cen]\n\n  # d[, .N, keyby = .(trt, evt)]\n  d\n}\n\n# Simulate data\nd_sim &lt;- simulate_data()\n# d_sim[, .N, keyby = .(trt, evt)]\n\n# Prepare data for Stan\nld &lt;- list(\n  N = nrow(d_sim),\n  P = 2,\n  X = cbind(1, d_sim$trt),\n  y = d_sim$t_evt_obs,\n  event = d_sim$evt,\n  N_pred = 361,\n  t_surv = 0:360,\n  mu0_gamma = c(5, 0),\n  sd0_gamma = c(2, 2),\n  rho_shape = 0.5\n)\n\n\n# d_stan_gamma &lt;- function(x, a, b){\n#   (b^a / gamma(a)) * x^(a-1) * exp(-b * x)\n# }\n# \n# xx &lt;- seq(0, 100, len = 1000)\n# yy &lt;- d_stan_gamma(xx, 1, 0.1)\n# plot(xx, yy, type = \"l\")\n\n\n# Fit the Stan model - sink to remove the noise\n# snk &lt;- capture.output(\n  m1 &lt;- mod_01$sample(\n      ld, iter_warmup = 1000, iter_sampling = 1000,\n      parallel_chains = 4, chains = 4, refresh = 0, show_exceptions = F,\n      max_treedepth = 10)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 4.1 seconds.\nChain 3 finished in 4.1 seconds.\nChain 4 finished in 4.1 seconds.\nChain 2 finished in 4.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4.2 seconds.\nTotal execution time: 4.5 seconds.\n\n\nCode\n# )\n\n\nExtract the parameters that we are interested in:\n\n\nCode\nd_post &lt;- data.table(\n  m1$draws(variables = c(\"gamma\", \"shape\", \"scale0\", \"scale1\"),\n           format = \"matrix\")\n)\nd_post &lt;- melt(d_post, measure.vars = names(d_post))\n\nd_tbl &lt;- d_post[, .(\n  mu = mean(value),\n  q_025 = quantile(value, prob = 0.025),\n  q_975 = quantile(value, prob = 0.975)\n), keyby = variable]\n\ngt(d_tbl) |&gt;\n  cols_align(\n    align = \"left\",\n    columns = c(\"variable\")\n    ) |&gt;\n  fmt_number(columns = everything(), decimals = 2) |&gt;\n  tab_options(\n      table.font.size = \"80%\"\n    ) \n\n\n\n\nTable 1: Parameters estimated by mcmc\n\n\n\n\n\n\n\n\n\nvariable\nmu\nq_025\nq_975\n\n\n\n\ngamma[1]\n7.17\n6.95\n7.42\n\n\ngamma[2]\n0.25\n0.06\n0.45\n\n\nshape\n1.80\n1.53\n2.08\n\n\nscale0\n1,306.48\n1,048.21\n1,672.04\n\n\nscale1\n1,685.66\n1,282.56\n2,261.94\n\n\n\n\n\n\n\n\n\n\nPathfinder variational inference is a bit quicker but coarser approximation:\n\n\nCode\nm3 &lt;- mod_01$pathfinder(\n  ld, \n  init = function() {list(\n    gamma = c(runif(1, 5, 10), runif(1, -1, 1)),\n    shape = runif(1, 0, 4)\n    )},\n  num_paths=4, single_path_draws=250,\n  history_size=50, max_lbfgs_iters=100,\n  refresh = 0, draws = 1000)\n\n\nFinished in  0.6 seconds.\n\n\n\n\nCode\nd_post &lt;- data.table(\n  m3$draws(variables = c(\"gamma\", \"shape\", \"scale0\", \"scale1\"),\n           format = \"matrix\")\n)\nd_post &lt;- melt(d_post, measure.vars = names(d_post))\n\nd_tbl &lt;- d_post[, .(\n  mu = mean(value),\n  q_025 = quantile(value, prob = 0.025),\n  q_975 = quantile(value, prob = 0.975)\n), keyby = variable]\n\ngt(d_tbl) |&gt;\n  cols_align(\n    align = \"left\",\n    columns = c(\"variable\")\n    ) |&gt;\n  fmt_number(columns = everything(), decimals = 2) |&gt;\n  tab_options(\n      table.font.size = \"80%\"\n    ) \n\n\n\n\nTable 2: Parameters estimated by pathfinder algorithm\n\n\n\n\n\n\n\n\n\nvariable\nmu\nq_025\nq_975\n\n\n\n\ngamma[1]\n7.18\n6.97\n7.42\n\n\ngamma[2]\n0.24\n0.06\n0.44\n\n\nshape\n1.79\n1.56\n2.05\n\n\nscale0\n1,315.97\n1,066.54\n1,660.76\n\n\nscale1\n1,685.52\n1,324.35\n2,255.59\n\n\n\n\n\n\n\n\n\n\nAnd by way of a sanity check, run the equivalent model using the flexsurv package.\n\n\nCode\nm2 &lt;- flexsurvreg(Surv(t_evt_obs, evt) ~ trt, data = d_sim, dist = \"llogis\")\nprint(m2)\n\n\nCall:\nflexsurvreg(formula = Surv(t_evt_obs, evt) ~ trt, data = d_sim, \n    dist = \"llogis\")\n\nEstimates: \n       data mean  est        L95%       U95%       se         exp(est) \nshape         NA     1.8116     1.5517     2.1150     0.1431         NA\nscale         NA  1269.7367  1007.2309  1600.6571   150.0420         NA\ntrt       0.5000     0.2455     0.0569     0.4342     0.0962     1.2783\n       L95%       U95%     \nshape         NA         NA\nscale         NA         NA\ntrt       1.0585     1.5437\n\nN = 2000,  Events: 154,  Censored: 1846\nTotal time at risk: 699744.3\nLog-likelihood = -1426.038, df = 3\nAIC = 2858.076\n\n\nOther options for model implementation might be through brms with a custom family (if that is possible).\n\n\nCode\nd_sim[, censored := 1-evt]\n# brms is backwords - \n# for cens, specify 0 to indicate no censoring and 1 to indicate right censoring\n\nbrms::make_stancode(t_evt_obs | cens(censored) ~ trt, data = d_sim, family = lognormal())\nbrms::make_stancode(t_evt_obs | cens(censored) ~ trt, data = d_sim, family = weibull())\n\n\nExponentiating the \\(\\gamma_2\\) parameter gives the acceleration factor associated with the treatment effect. For example, if \\(\\gamma_2 &gt; 0\\) we can say that change from the control to treatment arm is associated with survival times being multiplied by a factor of \\(\\exp(\\gamma_2)\\), indicating prolonged survival/delayed events. Similarly, if \\(\\gamma_2 &lt; 0\\) we have a reduction in survival (the time to event speeds up).\nIn a log-logistic AFT model with the current parameterisation, the median survival time for an individual with covariates \\(x_i\\) is given by \\(\\exp( \\gamma x_i') = \\alpha_i = \\text{scale}_\\text{i}\\). Median survival is a common measure used to contrast groups.\nProduce a posterior for the survival curve:\n\n\nCode\nd_post &lt;- data.table(m1$draws(variables = c(\"surv0\", \"surv1\"), format = \"matrix\"))\nd_post &lt;- melt(d_post, measure.vars = names(d_post))\nd_post[variable %like% \"surv0\", trt := 0]\nd_post[variable %like% \"surv1\", trt := 1]\nd_fig &lt;- copy(d_post)\n\nd_fig[, x := gsub(\".*\\\\[\", \"\", variable)]\nd_fig[, x := gsub(\"\\\\]\", \"\", x)]\nd_fig[, x := as.numeric(x)]\n\nd_fig &lt;- d_fig[\n  , .(mu = mean(value),\n      q_025 = quantile(value, prob = 0.025),\n      q_975 = quantile(value, prob = 0.975)), keyby = .(trt, x)]\nd_fig[, trt := factor(trt, levels = 0:1, labels = c(\"ctl\", \"trt\"))]\n\nggplot(d_fig, aes(x = x, y = mu, group = trt, col = trt)) +\n  geom_ribbon(aes(ymin = q_025, ymax = q_975, fill = trt), alpha = 0.1, col = NA) +\n  geom_line() + \n  scale_y_continuous(limits = c(0.7, 1), breaks = seq(0.7, 1, by = 0.1)) +\n  scale_color_discrete(\"\") +\n  scale_fill_discrete(\"\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nPosterior on the median survival time, the time at which 50% of the cohort has experienced the occurrence of the event, e.g. a medical attendance for RSV ARI.\n\n\nCode\nd_post &lt;- data.table(m1$draws(variables = c(\"scale0\", \"scale1\"), format = \"matrix\"))\n\nd_fig &lt;- melt(d_post, measure.vars = names(d_post), variable.name = \"trt\")\n\nd_fig[, trt := factor(trt, levels = c(\"scale0\", \"scale1\"), labels = c(\"ctl\", \"trt\"))]\n\nggplot(d_fig, aes(x = value, group = trt, col = trt)) +\n  geom_density() +\n  scale_x_continuous(\"Median survival time\") +\n  scale_color_discrete(\"Treatment\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nEstimate the posterior for restricted mean survival time (by treatment group) by integrating under the survival curve function for each draw from the posterior.\nThe RMST can be interpreted as the average survival time (i.e. time without the event, here being occurrence of RSV) during a defined time period ranging from time 0 to a specific follow-up time point.\n\n\nCode\n# Define the function to integrate\nintegrand_1 &lt;- function(\n    x, mu, shape) {\n  \n  a = exp(mu)\n  S = 1 / (1 + (x/a)^shape)\n  S\n  \n}\n\n\nd_post &lt;- data.table(m1$draws(variables = c(\"gamma\", \"shape\"), format = \"matrix\"))\nnames(d_post) &lt;- c(paste0(\"gamma\", 1:2), \"shape\")\ni &lt;- 1\nm_rmst &lt;- matrix(NA, ncol = 2, nrow = nrow(d_post))\nfor(i in 1:nrow(d_post)){\n  m_rmst[i, 1] &lt;- integrate(\n    integrand_1, lower = 0, upper = 360,\n    mu = d_post$gamma1[i], \n    shape = d_post$shape[i])$value  \n  m_rmst[i, 2] &lt;- integrate(\n    integrand_1, lower = 0, upper = 360,\n    mu = d_post$gamma1[i] + d_post$gamma2[i], \n    shape = d_post$shape[i])$value  \n}\n\n\n\nd_rmst &lt;- data.table(m_rmst)\nnames(d_rmst) &lt;- paste0(0:1)\n\nrmst_diff &lt;- d_rmst$`1` -  d_rmst$`0`\n\n\nd_fig &lt;- melt(d_rmst, measure.vars = names(d_rmst), variable.name = \"trt\")\n\nd_fig[, trt := factor(trt, levels = 0:1, labels = c(\"ctl\", \"trt\"))]\n\nggplot(d_fig, aes(x = value, group = trt, col = trt)) +\n  geom_density() +\n  scale_x_continuous(\"RMST\") +\n  scale_color_discrete(\"Treatment\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\nnames(d_rmst) &lt;- paste0(\"rmst\", 0:1)\nd_rmst[, diff := rmst1 - rmst0]\n\n\nFrom here, we could evaluate differences in the RMST between groups considering what level of improvement in the mean survival to 360 days would be warranted to decide on adopting the treatment over the control.\nNote that I have assumed a log-logistic parametric assumption here, primarily because I wanted something similar to a log-normal but more tractable. Other distributional assumptions might be more suitable. For example, if the data have a peaked hazard followed by a decline, then standard log-logistic or generalized log-logistic may work well. However, if the hazard function is more complex (e.g. bathtub shape, non-monotonic tail behavior etc), the Generalized F or Burr distributions might be better. Weibull or gamma models are simpler if only a monotonically increasing or decreasing hazard is required. All of these are reasonably straight forward to code up.\n\n\n\n\n\n\nReferences\n\n1. Sun J. Statistical analysis of interval censored failure time data. Springer; 2006.\n\n\n2. Christensen R. Bayesian ideas and data analysis. CRC Press; 2011.\n\n\n3. Collett D. Modelling survival data in medical research. CRC Press; 2015.\n\n\n4. Cleves M. Introduction to survival analysis using stata. Stata Press; 2010."
  },
  {
    "objectID": "notebooks/multinomial-regression.html",
    "href": "notebooks/multinomial-regression.html",
    "title": "Multinomial regression",
    "section": "",
    "text": "The multinomial distribution is a generalisation of the binomial distribution, [1]. The binomial counts the successes in a fixed number of trials each classed as success or failure. The multinomial distribution keeps track of trials whose outcomes have multiple categories, e.g. agree, neutral, disagree.\nWe have \\(N\\) objects, each is independently placed into one of \\(K\\) categories. An object is placed in category \\(k\\) with probability \\(p_k\\) where \\(\\sum_{k=1}^K p_k = 1\\) and \\(p_k \\ge 0\\) for all \\(k\\). If we let \\(Y_1\\) be the count of category 1 objects, \\(Y_2\\) be the count for category 2 etc so that \\(Y_1 + \\dots + Y_K = N\\) then \\(\\mathbf{Y} = (Y_1, \\dots, Y_K)\\) is said to have a multinomial distribution with parameters \\(N\\) and \\(\\mathbf{p} = (p_1, \\dots, p_K)\\). This can be written as \\(\\mathbf{Y} \\sim \\text{Mult}_K(N, \\mathbf{p})\\) where the \\(\\mathbf{Y}\\) is referred to as a random vector as it is a vector of random variables.\nIf \\(\\mathbf{Y} \\sim \\text{Mult}_K(N, \\mathbf{p})\\) then the joint PMF is:\n\\[\n\\begin{aligned}\n\\text{Pr}(Y_1 = y_1, \\dots, Y_K = y_K) &= \\frac{N!}{y_1!y_2!\\dots y_K!} p_1^{y_1}p_2^{y_2}\\dots  p_K^{y_K} \\\\\n&= \\begin{pmatrix}\nN \\\\\ny_1, \\dots, y_K\n\\end{pmatrix} \\prod_{k=1}^K p_k^{y_k}\n\\end{aligned}\n\\]\nThe marginal distribution of a multinomial are all binomial with \\(Y_k \\sim Bin(N, p_k)\\).\nLumping categories together will form multinomial distribution with the revised \\(K^\\prime\\) representing the new (smaller) set of categories and both counts and probabilities for the lumped groups being additive."
  },
  {
    "objectID": "notebooks/multinomial-regression.html#setup",
    "href": "notebooks/multinomial-regression.html#setup",
    "title": "Multinomial regression",
    "section": "Setup",
    "text": "Setup\nAssume that for a city, it is well known that the dominant modes of travel (to work) are walk/bike, public transport and car with a distribution provided in Table 1. Note that these are mutually exclusive and exhaustive of the possible modes of transport. Now say we want to investigate ways to move people away from cars. Interventions may span from city wide information on the benefits of using public transport, to financial discounts to restrictions on parking or taxes. Assume we want to evaluate whether an information mail-out versus financial discounts on public transport achieves greater transition to public transport.\n\n\n\n\nTable 1: Distribution of dominant mode of transport used to get to work\n\n\n\n\n\n\n\n\n\n\n\n\n\nMode of transport\nProportion\n\n\n\n\nWalk/bike\n0.07\n\n\nPublic transport\n0.33\n\n\nCar\n0.60\n\n\n\n\n\n\n\n\n\n\n\nget_data &lt;- function(\n    N = 250,\n    p0 = c(0.07, 0.33, 0.6),\n    p1 = c(0.10, 0.50, 0.4)){\n  \n  d &lt;- data.table(id = 1:N)\n  d[, trt := rep(0:1, each = N/2)]\n  \n  d[trt == 0, y := sample(seq_along(p0), .N, replace = T, prob = p0)]\n  d[trt == 1, y := sample(seq_along(p1), .N, replace = T, prob = p1)]\n  \n  d\n}\n\nWe take a random sample of adult working residents from the city. The sample is randomised 1:1 to a monthly email lasting for 3-months that details the benefit of using public transport versus a 3-month discount for the all public transport networks within the city. At 3-months, the sample cohort are surveyed to determine their dominant mode of transport in the last month."
  },
  {
    "objectID": "notebooks/multinomial-regression.html#contingency-table",
    "href": "notebooks/multinomial-regression.html#contingency-table",
    "title": "Multinomial regression",
    "section": "Contingency table",
    "text": "Contingency table\nOne approach to the analysis is via a contingency table posing the research hypothesis Do dominant modes of transport differ under the two interventions? The Chi-squared test of independence is run on the observed counts of each cell as follows:\n\\[\n\\begin{aligned}\nX^2 &= \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(O_{ij} - e_{ij})^2}{e_{ij}}\n\\end{aligned}\n\\]\nwhere the expected counts are based on the row and column totals (i.e. assuming independence across groups):\n\\[\n\\begin{aligned}\ne_{ij} = \\frac{O_{i.} O_{.j}}{O_{..}}\n\\end{aligned}\n\\]\nwhere \\(O_{i.}\\) denotes the row totals, \\(O_{.j}\\) the column totals and \\(O_{..}\\) denotes the grand total.\nIf the null hypothesis is true then the observed and the expected frequencies will be similar to one another and \\(\\chi^2\\) will be small. If \\(\\chi^2\\) is too large then the null would be rejected suggesting that the treatment and dominant modes of travel are not independent. Specifically, we would reject the null if the test statistic was found to be greater than or equal to a \\(\\chi^2\\) distribution that has \\((r-1)(c-1)\\) degrees of freedom, i.e. reject null if \\(X^2 \\ge \\chi^2_{(r-1)(c-1);\\alpha}\\) where \\(\\alpha\\) is the significance level.\nA sketch of the approach is shown below.\n\nset.seed(1)\nd &lt;- get_data()\n\n# observed\nm_obs &lt;- as.matrix(dcast(d[, .N, keyby = .(trt, y)],\n                      trt ~ y, value.var = \"N\"))\n\ntot_cols &lt;- colSums(m_obs[, 2:4])\ntot_rows &lt;- rowSums(m_obs[, 2:4])\n\n# expected\nm_e &lt;- rbind(\n  tot_cols  * tot_rows[1] / sum(tot_rows),\n  tot_cols  * tot_rows[2] / sum(tot_rows)\n)\n\n# chisqured statistic vs critical value\n\nv_stats &lt;- c(\n  chisq_obs = sum(  ((m_obs[, -1] - m_e)^2)/m_e ) ,\n  chisq_crit = qchisq(0.95, 2 * 1)\n)\n\nround(c(\n  v_stats, \n  p_value = pchisq(v_stats[1], 2 * 1, lower.tail = F)), 3)\n\n        chisq_obs        chisq_crit p_value.chisq_obs \n            6.053             5.991             0.048 \n\n\nSince the observed test statistic is greater than the critical value, we would reject the null in this case. The above can be automatically with the built-in function:\n\nres &lt;- chisq.test(m_obs[, -1])\nres\n\n\n    Pearson's Chi-squared test\n\ndata:  m_obs[, -1]\nX-squared = 6.0528, df = 2, p-value = 0.04849"
  },
  {
    "objectID": "notebooks/multinomial-regression.html#multi-logit-regression",
    "href": "notebooks/multinomial-regression.html#multi-logit-regression",
    "title": "Multinomial regression",
    "section": "Multi-logit regression",
    "text": "Multi-logit regression\nTo implement the multi-logit regression a long format is usually adopted so that we have each unit \\(i\\) has an outcome \\(Y_i\\) equal to one of the possible categories.\nA simple sum-to-zero implementation is shown below.\n\n\ndata {\n  // Each unit has an outcome corresponding to one of K categories\n  int K;\n  // Number of units\n  int N;\n  // Number of terms in linear predictor (all lp have the \n  // same terms here). Includes intercept.\n  int D;\n  // Outcome variable (one of the categories)\n  array[N] int y;\n  // design matrix\n  matrix[N, D] x;\n}\nparameters {\n  vector[K-1] a_raw;\n  vector[K-1] b_raw;\n}\ntransformed parameters {\n  //            k1  k2  k3 etc\n  // intercept  -   -   -\n  //         x  -   -   -\n  matrix[D, K] b;\n  \n  b[1, ] = append_row(a_raw, -sum(a_raw))';\n  b[2, ] = append_row(b_raw, -sum(b_raw))';\n}\nmodel {\n  matrix[N, K] x_beta = x * b;\n\n  to_vector(a_raw) ~ normal(0, 5);\n  to_vector(b_raw) ~ normal(0, 5);\n\n  for (n in 1:N) {\n    y[n] ~ categorical_logit(x_beta[n]');\n  }\n}\ngenerated quantities{\n  \n  matrix[K, 2] p;\n  \n  vector[K] l0 = to_vector(b[1, ]);\n  vector[K] l1 = to_vector(b[1, ] + b[2, ]);\n  \n  p[, 1] = softmax(l0);\n  p[, 2] = softmax(l1);\n  \n}\n\n\nAnd a fixed pivot implementation is\n\n\ndata {\n  // Each unit has an outcome corresponding to one of K categories\n  int K;\n  // Number of units\n  int N;\n  // Number of terms in linear predictor (all lp have the \n  // same terms here). Includes intercept.\n  int D;\n  // Outcome variable (one of the categories)\n  array[N] int y;\n  // design matrix\n  matrix[N, D] x;\n}\nparameters {\n  //            k1  k2  k3 etc\n  // intercept  -   -   -\n  //         x  -   -   -\n  matrix[D, K-1] b_raw;\n}\ntransformed parameters {\n  //            k1  k2  k3 etc\n  // intercept  -   -   -\n  //         x  -   -   -\n  matrix[D, K] b;\n  \n  b[, 1:(K-1)] = b_raw;\n  b[, K] = rep_vector(0.0, D);\n}\nmodel {\n  matrix[N, K] x_beta = x * b;\n\n  to_vector(b_raw) ~ normal(0, 5);\n\n  for (n in 1:N) {\n    y[n] ~ categorical_logit(x_beta[n]');\n  }\n}\ngenerated quantities{\n  \n  matrix[K, 2] p;\n  \n  vector[K] l0 = to_vector(b[1, ]);\n  vector[K] l1 = to_vector(b[1, ] + b[2, ]);\n  \n  p[, 1] = softmax(l0);\n  p[, 2] = softmax(l1);\n  \n}\n\n\nFit both models to the data\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/multi-logit-01.stan\")\nm2 &lt;- cmdstanr::cmdstan_model(\"stan/multi-logit-02.stan\")\n\nld &lt;- list(\n  N = nrow(d),\n  K = length(unique(d$y)),\n  y = d$y,\n  D = 2,\n  x = cbind(1, d$trt)\n)\n\nf1 &lt;- m1$sample(data = ld, chains = 1, iter_sampling = 1000, refresh = 0)\nf2 &lt;- m2$sample(data = ld, chains = 1, iter_sampling = 1000, refresh = 0)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.9 seconds.\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.6 seconds.\n\n\nBoth approaches faithfully recover the empirical proportions for the two treatment groups as shown in Table 2. The pivot model gives the most direct path to interpreting the model parameters. However, note that the models are structurally different and cannot be considered completely equivalent since different prior weights enter the models.\n\n\n\n\nTable 2: Observed and modeled distribution for mode of transport\n\n\n\n\n\n\n\n\n\ntrt\ny\nN\ntot\np\nmu_f1\nmu_f2\n\n\n\n\n0\n1\n7\n125\n0.056\n0.055\n0.058\n\n\n0\n2\n46\n125\n0.368\n0.368\n0.369\n\n\n0\n3\n72\n125\n0.576\n0.576\n0.573\n\n\n1\n1\n12\n125\n0.096\n0.097\n0.097\n\n\n1\n2\n60\n125\n0.480\n0.479\n0.478\n\n\n1\n3\n53\n125\n0.424\n0.425\n0.425\n\n\n\n\n\n\n\n\n\n\nThe parameter estimates from the linear predictors from the two models are summarised below. The terms (treatment effects) of interest are those associated with the second model f2, specifically, b[2,1] and b[2,2]. These suggesting that (1) units in the treatment group are more likely to have walking/bike than car as their dominant mode of transport and (2) units in the treatment group are more likely to have public transport than car as their dominant mode of transport.\n\nf1$summary(variables = \"b\")\n\n# A tibble: 6 × 10\n  variable    mean  median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b[1,1]   -1.46   -1.44   0.269 0.267 -1.92  -1.04   1.01     181.     170.\n2 b[2,1]    0.408   0.405  0.348 0.347 -0.162  0.965  1.01     177.     200.\n3 b[1,2]    0.503   0.494  0.172 0.172  0.227  0.794  1.02     180.     319.\n4 b[2,2]    0.0822  0.0948 0.225 0.213 -0.312  0.444  1.01     164.     285.\n5 b[1,3]    0.955   0.949  0.154 0.149  0.707  1.22   1.00     338.     314.\n6 b[2,3]   -0.490  -0.487  0.211 0.216 -0.810 -0.134  1.00     453.     512.\n\nf2$summary(variables = \"b\")\n\n# A tibble: 6 × 10\n  variable   mean median    sd   mad       q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b[1,1]   -2.35  -2.32  0.374 0.375 -3.00    -1.79   1.00     499.     403.\n2 b[2,1]    0.834  0.835 0.492 0.492  0.00937  1.63   1.00     456.     588.\n3 b[1,2]   -0.445 -0.443 0.185 0.174 -0.759   -0.145  1.01     643.     527.\n4 b[2,2]    0.563  0.561 0.260 0.260  0.133    0.982  1.00     619.     540.\n5 b[1,3]    0      0     0     0      0        0     NA         NA       NA \n6 b[2,3]    0      0     0     0      0        0     NA         NA       NA \n\n\nThese results are consistent with the results from the contingency table analysis in that they suggest the public transport discount intervention may increase the likelihood of people taking this form of transport to work.\nUnlike the contingency analysis, the the multi-logit approach offers the usual regression benefit of being able to adjust for covariates that may be relevant."
  },
  {
    "objectID": "notebooks/multinomial-regression.html#poisson-regression",
    "href": "notebooks/multinomial-regression.html#poisson-regression",
    "title": "Multinomial regression",
    "section": "Poisson regression",
    "text": "Poisson regression\nMultinomial logit models can also be fit using an equivalent log-linear model and a series of Poisson likelihoods. This is mathematically equivalent to the multinomial and computationally the Poisson approach can be easier, see McElreath2020 p365.\nAn implementation is shown below\n\n\ndata {\n  // Each unit has an outcome corresponding to one of K categories\n  int K;\n  // Number of units\n  int N;\n  // Outcome variable (one of the categories)\n  // y1= walk/bike, y2=public transport, y3=car\n  array[N] int y1;\n  array[N] int y2;\n  array[N] int y3;\n  // design matrix\n  vector[N] x;\n}\nparameters {\n  vector[K] a;\n  vector[K] b;\n}\nmodel {\n\n  to_vector(a) ~ normal(0, 5);\n  to_vector(b) ~ normal(0, 5);\n  \n  target += poisson_log_lpmf(y1 | a[1] + x * b[1]);\n  target += poisson_log_lpmf(y2 | a[2] + x * b[2]);\n  target += poisson_log_lpmf(y3 | a[3] + x * b[3]);\n  \n}\ngenerated quantities{\n  \n  matrix[K, 2] p;\n  \n  p[1, 1] = exp(a[1]) * inv( exp(a[1]) + exp(a[2]) + exp(a[3]));\n  p[2, 1] = exp(a[2]) * inv( exp(a[1]) + exp(a[2]) + exp(a[3]));\n  p[3, 1] = exp(a[3]) * inv( exp(a[1]) + exp(a[2]) + exp(a[3]));\n  \n  p[1, 2] = exp(a[1] + b[1]) * inv( exp(a[1] + b[1]) + exp(a[2] + b[2]) + exp(a[3] + b[3]));\n  p[2, 2] = exp(a[2] + b[2]) * inv( exp(a[1] + b[1]) + exp(a[2] + b[2]) + exp(a[3] + b[3]));\n  p[3, 2] = exp(a[3] + b[3]) * inv( exp(a[1] + b[1]) + exp(a[2] + b[2]) + exp(a[3] + b[3]));\n\n}\n\n\nIn order to fit the model, it is necessary to first create an indicator variable for the occurrence of each category.\n\n# need to create a binary indicator for each category:\nd[, y1 := ifelse(y == 1, 1, 0)]\nd[, y2 := ifelse(y == 2, 1, 0)]\nd[, y3 := ifelse(y == 3, 1, 0)]\n\nm3 &lt;- cmdstanr::cmdstan_model(\"stan/poisson-01.stan\")\n\nld &lt;- list(\n  N = nrow(d),\n  K = length(unique(d$y)),\n  y1 = d$y1, \n  y2 = d$y2,\n  y3 = d$y3,\n  x = d$trt\n)\n\nf3 &lt;- m3$sample(data = ld, chains = 1, iter_sampling = 1000, refresh = 0)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.4 seconds.\n\nf3$summary(variables = \"b\")\n\n# A tibble: 3 × 10\n  variable   mean median    sd   mad      q5     q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b[1]      0.568  0.567 0.509 0.526 -0.259   1.43    1.00     800.     576.\n2 b[2]      0.272  0.266 0.188 0.187 -0.0329  0.581   1.00     719.     701.\n3 b[3]     -0.318 -0.318 0.178 0.179 -0.630  -0.0241  1.00     792.     837.\n\n\nThe combined set of results are shown below, again the empirical proportions for each group are captured by the poisson implementation of the model.\n\n\n\n\nTable 3: Observed and modeled distribution for mode of transport (multi-logit and poisson)\n\n\n\n\n\n\n\n\n\ntrt\ny\nN\ntot\np\nmu_f1\nmu_f2\nmu_f3\n\n\n\n\n0\n1\n7\n125\n0.056\n0.055\n0.058\n0.056\n\n\n0\n2\n46\n125\n0.368\n0.368\n0.369\n0.367\n\n\n0\n3\n72\n125\n0.576\n0.576\n0.573\n0.577\n\n\n1\n1\n12\n125\n0.096\n0.097\n0.097\n0.096\n\n\n1\n2\n60\n125\n0.480\n0.479\n0.478\n0.482\n\n\n1\n3\n53\n125\n0.424\n0.425\n0.425\n0.422"
  },
  {
    "objectID": "notebooks/multinomial-regression.html#extensions",
    "href": "notebooks/multinomial-regression.html#extensions",
    "title": "Multinomial regression",
    "section": "Extensions",
    "text": "Extensions\nOne of the natural extensions of multinomial regression is the use of cluster level effects that are correlated across categories. For more detail, see [4]."
  },
  {
    "objectID": "notebooks/multinomial-regression.html#footnotes",
    "href": "notebooks/multinomial-regression.html#footnotes",
    "title": "Multinomial regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that if we have three categories and assign category 3 as the reference then even though our logits are in terms of \\(\\log(p_1/p_3)\\) and \\(\\log(p_2/p_3)\\), we can still compute a logit for \\(\\log(p_1/p_2)\\) by subtracting the logit for \\(\\log(p_2/p_3)\\) from that for \\(\\log(p_1/p_3)\\).↩︎"
  },
  {
    "objectID": "notebooks/power-analysis-by-sim.html",
    "href": "notebooks/power-analysis-by-sim.html",
    "title": "Power analysis by simulation",
    "section": "",
    "text": "library(data.table)\nlibrary(ggplot2)\nlibrary(gt)\n\nPower analyses inform us as to chances an experiment has of identifying a treatment differences based on our criteria, various assumptions about the design, sample size and over the effect sizes we imagine might arise. Power analyses can be implemented via simulation. The following methods provide a basic demonstration of the power analysis for an experiment with a single analysis where we consider the binary outcome under differing treatment regimes at a fixed sample and a range of effect sizes. For the sake of the example, logistic regression is used as the analysis model although one might reasonable select from a range of analysis models, with some being more efficient than others. For example, non-parametric approaches may yield lower power, but might make less assumptions, depending on the particular method chosen.\nAssume:\n\na baseline probability of response in the standard treatment that equals 0.2\neffect sizes from -0.25 to 0.25 on the risk scale\nsample size of 100 per treatment type\n\nMethod 1 creates a fine grid over the entire effect size range of interest and then performs a single trial for each of these effect sizes based on data simulated at this effect size. For each trial, we will either detect an effect or not based on the criteria we are using to make a decision (i.e. reject a null hypothesis). The probability of detecting an effect for each simulated trial equates to the power at that effect size. If we interpolate over these indicator values as to whether an effect was detected or not, we can produce an approximation of the power curve over the entire range.\n\nn_sim &lt;- 10000\np_0 &lt;- 0.5\nrd &lt;- seq(-0.25, 0.25, len = n_sim)\nwin &lt;- numeric(n_sim)\n\ny_0 &lt;- rbinom(n_sim, 100, p_0)\ny_1 &lt;- rbinom(n_sim, 100, p_0 + rd)\n\ni &lt;- 1\nfor(i in 1:n_sim){\n  \n  y &lt;- rbind(\n    c(y_0[i], 100 - y_0[i]),\n    c(y_1[i], 100 - y_1[i])\n  )\n  x &lt;- c(0, 1)\n  \n  f1 &lt;- glm(y ~ x, family = binomial)\n  # assume a typical frequentist critria\n  win[i] &lt;- summary(f1)$coef[2, 4] &lt; 0.025\n  \n}\n\n\nd_fig_1 &lt;- data.table(\n  x = rd,\n  win = win\n)\n\nMethod 2 simulates a large number of trials at each effect size of interest. The power at each effect size is computed as the proportion of trials where the effect was identified based on the criteria we are using to make a decision. This process is done for a range of effect sizes below. Method 2 is clearly more computationally demanding than method 1.\n\nn_sim &lt;- 5000\np_0 &lt;- 0.5\np_1 &lt;- p_0 + c(-0.2, -0.1, 0.0, 0.1, 0.2)\n\npwr &lt;- numeric(length(p_1))\n\n\nfor(i in 1:length(p_1)){\n  \n  y_0 &lt;- rbinom(n_sim, 100, p_0)\n  y_1 &lt;- rbinom(n_sim, 100, p_1[i])\n  \n  win &lt;- numeric(n_sim)\n  \n  for(j in 1:n_sim){\n  \n    y &lt;- rbind(\n      c(y_0[j], 100 - y_0[j]),\n      c(y_1[j], 100 - y_1[j])\n    )\n    x &lt;- c(0, 1)\n    \n    f1 &lt;- glm(y ~ x, family = binomial)\n    win[j] &lt;- summary(f1)$coef[2, 4] &lt; 0.025\n    \n  } \n  \n  pwr[i] &lt;- mean(win)\n}\n\n\nd_fig_2 &lt;- data.table(\n  x = p_1 - p_0,\n  pwr = pwr\n)\n\n\n\n\n\n\n\n\n\nFigure 1: Power curves characterising power by effect size at a sample size of 100 per arm in two group study with binary outcome (points show results from method 2)"
  },
  {
    "objectID": "notebooks/probabilistic-index.html",
    "href": "notebooks/probabilistic-index.html",
    "title": "Probabilistic Index Models",
    "section": "",
    "text": "The probabilistic index (PI), also known as the probability of superiority, refers to the probability that the outcome of a randomly selected subject in the treatment group exceeds the outcome of another randomly selected subject on the control group [1]. Thus a PI of 50% indicates that a patient on the experimental treatment is as likely to be better or worse as compared with a patient on the control treatment.\nThe PI is the metric associated with the Mann-Whitney-U test (aka Wilcoxon-Mann-Whitney). When modelled under a regression framework with conditioning on the covariate values of both subjects, we have a probabilistic index model (PIM).\nIn notational terms, if we let \\(y\\) denote a univariate outcome and \\(\\vec{x}\\) a vector of covariates for a unit, then the PI is given by \\(\\text{Pr}(y_i &lt; y_j | \\vec{x_i}, \\vec{x_j})\\) where \\(i\\) and \\(j\\) denote distinct units.\nTwo things are clear from this definition:\n\nThe PI does not provide information on the magnitude of the difference between two populations.\nThe measure is always comparing two different subjects, it does not give the probability that a single patient will benefit from a given treatment as compared with the conventional treatment.\n\nIn a two-sample setting we can use the MWU to compute the probabilistic index. However, when our treatment is continuous or we wish to condition on a set of covariates then it is desirable to embed the PI into a regression model. The approach is to model the conditional PI directly as a function of covariates:\n\\[\n\\begin{aligned}\n\\text{Pr}(Y_i &lt; Y_j | X_i, X_j) = m(X_i, X_j, \\beta)\n\\end{aligned}\n\\]\nwhere \\(m(.)\\) is some user-specified function and \\((Y_i, X_i)\\) are iid and \\(X_i\\), \\(X_j\\) and \\(\\beta\\) are vector quantities. It is convenient to choose \\(m\\) as:\n\\[\n\\begin{aligned}\nm(X_i, X_j, \\beta) = g^{-1}[(X_j - X_i)^\\top \\beta]\n\\end{aligned}\n\\]\nTo interpret the regression coefficient, consider two subjects \\(i\\) and \\(j\\) with covariate patterns \\(X^\\top = (Z_1, Z_2)\\) with \\(\\beta^\\top = (\\beta_1, \\beta_2)\\) (for a bivariate case). Say, subject \\(i\\) has covariate values \\((z_1, z_2)\\) and subject \\(j\\) has values \\((z_1 + 1, z_2)\\) so that both have the same value for \\(Z_2\\) but where \\(Z_1\\) differs by one unit. It follows that:\n\\[\n\\begin{aligned}\n\\text{Pr}(Y_i &lt; Y_j | Z_{1i} = z_1, Z_{1j} = z_1 + 1, Z_{2i} = Z_{2j}) = g^{-1}{\\beta_1}\n\\end{aligned}\n\\]\nso that \\(g^{-1}{\\beta_1}\\) gives the probability that a randomly selected subject with covariate value \\(z_1\\) for \\(Z_1\\) will have a lower outcome as compared with a randomly selected subject with a covariate value that is higher by one and where \\(Z_2\\) is the same for both subjects.\n\n\n\n\n\n\nNote\n\n\n\nNotice the absence of an intercept in these models. This means that when the covariate patterns are the same, the probability that \\(Y_i\\) is less than \\(Y_j\\) is 0.5 and vice versa.\n\n\nAs might be clear, the above can be handled via a logistic regression applied to the transformed binary outcome \\(I_{ij} = I(Y_i &lt; Y_j)\\) and predictors \\(X_{ij} = X_j - X_i\\). From this, the MLE give consistent estimates for \\(\\beta\\), [2]. However, despite the fact that \\((Y_i, X_i)\\) are mutually independent, the transformed data \\(I_{ij}\\) and \\(X_{ij}\\) are not. This is obviously the case if we consider \\(I_{ij} = I(Y_i &lt; Y_j)\\) and \\(I_{ik} = I(Y_i &lt; Y_k)\\) since both share \\(Y_i\\) making them no longer independent. Moreover, the \\(I_{ij}\\) have a correlation structure that is different from the typical block correlation structure in multi-level data. Thas introduced a sandwich estimator for the standard errors (frequentist setting) implemented in the R package pim, [2] and [3]. However, a bootstrap approach could also be used - simply take \\(B\\) bootstrap samples (with replacement) of size \\(n\\) and then repeat whatever estimation process was used.\n\n\n\n\n\n\nNote\n\n\n\nThe above specification can be modified to deal with ties by modifying the transformed outcome to \\(I(Y_i &lt; Y_j) + 0.5I(Y_i = Y_j)\\).\n\n\n\nReferences\n\n\n1. De Schryver M. A tutorial on probabilistic index models: Regression models for the effect size p(Y1 &gt; Y2). American Psychological Association. 2019;24.\n\n\n2. Thas O. Probabilistic index models. Journal of the Royal Statistical Society Series B Methodological. 2012;74:623–71.\n\n\n3. Meys J. Pim fit probabilistic index models. 2017."
  },
  {
    "objectID": "notebooks/robust-errors.html",
    "href": "notebooks/robust-errors.html",
    "title": "Robust errors for estimating proportion",
    "section": "",
    "text": "The goto approach for estimating an uncertainty interval for a proportion is to use the normal approximation of the binomial distribution:\n\\[\n\\begin{aligned}\n\\hat{p} \\pm z_{(1 - \\alpha)/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\end{aligned}\n\\]\nwhere \\(n\\) is the sample size, \\(\\hat{p}\\) is the observed sample proportion and \\(z\\) is the standard normal quantile (and typically set to \\(\\approx 2\\)).\nSay we had multiple estimates of the proportion, e.g. number of times we observe antimicrobial resistance out of the positive blood cultures we collected over the previous year. These estimates might come from differing hospitals and some might include repeat tests on individuals. This means that we have multiple levels of variation to deal with. One approach is to use a robust (sometimes call a Hubert White or sandwich) estimator for the standard errors.\nThis can be achieved by fitting a standard glm and then making an adjustment to the standard errors using the tools provided in the R sandwich package.\nSimulate data for 500 patients from 10 sites, some patients having repeat measures.\n\nlibrary(sandwich)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(marginaleffects)\nlibrary(lme4)\n\nLoading required package: Matrix\n\n# library(gee)\nlibrary(\"geepack\")\n\n# N unique pts on which we have multiple obs, each pt nested within one of\n# the 10 sites\nN &lt;- 500\nN_site &lt;- 10\np_site &lt;- as.numeric(extraDistr::rdirichlet(1, rep(1, 10)))\nsites &lt;- sample(1:N_site, N, replace = T, prob = p_site)\nd_pt &lt;- data.table(\n  id_pt = 1:N,\n  site = sort(sites)\n)\n# number obs per pt - inflated here to make a point\nn_obs &lt;- rpois(N, 2)\nd &lt;- d_pt[unlist(lapply(1:N, function(x){\n  rep(x, n_obs[x])\n}))]\nd[, id_obs := 1:.N, keyby = id_pt]\n\n# about 60% (plogis(0.4)) resistant but with site and subject level\n# variability beyond the natural sampling variability due to varying \n# number of subjects per site\nnu &lt;- rnorm(N, 0, 0.5)\n# treat site as true fixed effect\nrho &lt;- rnorm(N_site, 0, 0.7)\nd[, eta := 0.4 + rho[site] + nu[id_pt]]\n\nd[, y := rbinom(.N, 1, plogis(eta))]\nd[, site := factor(site)]\nd[, id_pt := factor(id_pt)]\n\np_obs &lt;- d[, sum(y)/.N]\n\n# d[, .(y = sum(y), n = .N)]\n# distribution of frequency of observations on a pt\n# hist(d[, .N, keyby = id_pt][, N])\n\nThe raw numbers of observations at each site are shown in Figure 1.\n\nd_fig &lt;- copy(d)\nd_fig[y == 0, resp := \"Susceptible\"]\nd_fig[y == 1, resp := \"Resistant\"]\nggplot(d_fig, aes(x = site, fill = resp)) +\n  geom_bar() +\n  scale_fill_discrete(\"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: Observations by site\n\n\n\n\n\nOverall, the observed proportion resistant to antibiotics is 0.747. Various ways exist to estimate the uncertainty.\nThe wald estimate for the uncertainty interval is calculated as:\n\n# wald (normal approximation)\nse_wald &lt;- sqrt(p_obs * (1-p_obs) / nrow(d))\np_0_lb &lt;- p_obs - qnorm(0.975) * se_wald\np_0_ub &lt;- p_obs + qnorm(0.975) * se_wald\n\nA GLM with only an intercept term will give the same prediction as the observed proportion and we can calculate the naive estimate of uncertainty as:\n\n# standard glm, not accounting for pt\nf1 &lt;- glm(y ~ 1, family = binomial, data = d)\n\npredict(f1, type = \"response\")[1]\n\n        1 \n0.7467337 \n\n# get naive standard errors\ns_f1 &lt;- summary(f1)$coef\n\n# model uncertainty naive\nlo_1 &lt;- qlogis(p_obs)\n# se from intercept term, i.e. we are just looking at the 'average' or\n# typical pt, over which we would expect heterogeneity\np_1_lb &lt;- plogis(lo_1 - qnorm(0.975) * s_f1[1, 2])\np_1_ub &lt;- plogis(lo_1 + qnorm(0.975) * s_f1[1, 2])\n\nWe can use the sandwich estimator to adjuste for heterogeneity as:\n\n# adjusted to account for heterogeneity due to site (we did not \n# adjust for site in the model) and repeat measure for pt\nsw_se &lt;- sqrt(vcovCL(f1, cluster = d[, .(site, id_pt)], type = \"HC1\")[1,1])\np_2_lb &lt;- plogis(lo_1 - qnorm(0.975) * sw_se)\np_2_ub &lt;- plogis(lo_1 + qnorm(0.975) * sw_se)\n\n\nf2 &lt;- glmer(y ~ (1|site) + (1|id_pt), data = d, family = binomial)\n\nNote that in the glmer model (with a non-linear link function) the predictions are first made on the link scale, averaged, and then back transformed. This means that the average prediction may not be exactly identical to the average of predictions.\nYou’ll note that the point estimate from the glmer deviates from the observed proportion. This can be for all of the following reasons:\n\nThe GLMM provides subject-specific estimates, conditional on the random effects.\nGLMMs involve shrinkage, where estimates for groups with less data are pulled towards the overall mean.\nLarger random effects can lead to bigger differences.\nThe GLMM estimate is a model-based estimate that accounts for the hierarchical structure of the data and provides a framework for inference.\n\nIn theory, a GEE could also be used but in R the GEE framework is not particularly well set up for multiple levels of clustering.\n\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_0_lb, p_0_ub)\n\n[1] \"0.7467 (0.7197, 0.7738)\"\n\n# Model based adjusting for site.\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_1_lb, p_1_ub)\n\n[1] \"0.7467 (0.7188, 0.7728)\"\n\n# Adjusted - account for heterogeneity due to site (we did not \n# adjust for site) and repeat measure for pt\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_2_lb, p_2_ub)\n\n[1] \"0.7467 (0.6330, 0.8344)\"\n\n# Finally a random effects model\navg_predictions(f2, re.form = NA, type = \"link\")\n\n\n Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n     1.04      0.291 3.57   &lt;0.001 11.4 0.468   1.61\n\nType: link\n\navg_predictions(f2, re.form = NA, type = \"response\")\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    0.739     0.0563 13.1   &lt;0.001 128.4 0.628  0.849\n\nType: response\n\n## Step 1\npred &lt;- predictions(f2, type = \"link\", re.form = NA)$estimate\n## Step 2: average\nplogis(mean(pred))\n\n[1] 0.7386312\n\n\n\nReferences"
  },
  {
    "objectID": "notebooks/ich-estimands-e9-r1.html",
    "href": "notebooks/ich-estimands-e9-r1.html",
    "title": "Addendum on estimands and sensitivity analysis in clinical trials E9(R1)",
    "section": "",
    "text": "Code\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(gt)\noptions(scipen=999)"
  },
  {
    "objectID": "notebooks/ich-estimands-e9-r1.html#introduction",
    "href": "notebooks/ich-estimands-e9-r1.html#introduction",
    "title": "Addendum on estimands and sensitivity analysis in clinical trials E9(R1)",
    "section": "Introduction",
    "text": "Introduction\nThe ICH efficacy guidelines deal with the design, conduct, safety and reporting of clinical trials. E9 is about the statistical methdology applied to (later phase) clinical trials for marketing applications. E9 (R1) is an addendum to E9 that introduces the estimand franework with a focus on the statistical priniciples related to estimands and sensitivity analyses.\nThere are six sections in the addendum:\n\nPurpose and scope\nFramework introduction\nEstimand definitions\nImpacts of framework on design and conduct\nImpact on analyses\nDocumentation"
  },
  {
    "objectID": "notebooks/ich-estimands-e9-r1.html#purpose",
    "href": "notebooks/ich-estimands-e9-r1.html#purpose",
    "title": "Addendum on estimands and sensitivity analysis in clinical trials E9(R1)",
    "section": "Purpose",
    "text": "Purpose\nThe adendum is about achieving greater clarity about what treatment effects imply. To do so the estimands are introduced, which are just a precise description reflecting the clinical question posed by the trial. At a fundamental level, estimands assume a causal inference perspective:\n“An estimand … summarises at a population level what the outcomes would be in the same patients under different treatment conditions being compared.” (ICH E9(R1) training materials.\nWhile this is clearly drawing on concepts from the potential outcomes framework, the guidance never mentions this term explicitly.\nThe stated purpose of the guidance is to address (1) the limitations of ITT with respect to its utility in decision making, (2) distinguishing missingness from intercurrent events (post randomisation events such as discontinuation of treatment, use of rescue medication, death etc) considerations especially when ICE impact the interpretaion of later measures (3) which analysis set should be referenced and how per protocol can be addressed and (4) the use of sensitivity analyses.\nThe adendum (i.e. the use of the estimand framework) is relevant irrespective of whether the study is an RCT and whatever data type is considered."
  },
  {
    "objectID": "notebooks/ich-estimands-e9-r1.html#framework-introduction",
    "href": "notebooks/ich-estimands-e9-r1.html#framework-introduction",
    "title": "Addendum on estimands and sensitivity analysis in clinical trials E9(R1)",
    "section": "Framework introduction",
    "text": "Framework introduction\nThe maintains a waterfall (linear/sequential) perspective on the development of clinical trials. Specifically, objectives should be translated into clinical questions, which would then be used to define estimands.\nOnly once the estimands have been formulated can a method of estimation be chosen and then sensitivity analyses (targeting the same estimand) are used to explore how robust the inference is.\nThe guideline wants the estimands to be the main determinant of the trial design, conduct and anlaysis. Doing this the other way around will effectively dictate the estimand from the selected analysis."
  },
  {
    "objectID": "notebooks/ich-estimands-e9-r1.html#estimand-definitions",
    "href": "notebooks/ich-estimands-e9-r1.html#estimand-definitions",
    "title": "Addendum on estimands and sensitivity analysis in clinical trials E9(R1)",
    "section": "Estimand definitions",
    "text": "Estimand definitions"
  },
  {
    "objectID": "notebooks/ich-estimands-e9-r1.html#impacts-of-framework-on-design-and-conduct",
    "href": "notebooks/ich-estimands-e9-r1.html#impacts-of-framework-on-design-and-conduct",
    "title": "Addendum on estimands and sensitivity analysis in clinical trials E9(R1)",
    "section": "Impacts of framework on design and conduct",
    "text": "Impacts of framework on design and conduct"
  },
  {
    "objectID": "notebooks/ich-estimands-e9-r1.html#impact-on-analyses",
    "href": "notebooks/ich-estimands-e9-r1.html#impact-on-analyses",
    "title": "Addendum on estimands and sensitivity analysis in clinical trials E9(R1)",
    "section": "Impact on analyses",
    "text": "Impact on analyses"
  },
  {
    "objectID": "notebooks/ich-estimands-e9-r1.html#documentation",
    "href": "notebooks/ich-estimands-e9-r1.html#documentation",
    "title": "Addendum on estimands and sensitivity analysis in clinical trials E9(R1)",
    "section": "Documentation",
    "text": "Documentation"
  },
  {
    "objectID": "notebooks/ich-estimands-e9-r1.html#estimands",
    "href": "notebooks/ich-estimands-e9-r1.html#estimands",
    "title": "Addendum on estimands and sensitivity analysis in clinical trials E9(R1)",
    "section": "Estimands",
    "text": "Estimands\nThe estimand captures a very precise description of the treatment effect that reflects the clinical question.\nICEs are events that happen after treatment and impact the interpretation or the existence of the measurements used to answer the clinical question. To fully understand the treatment effect, we need to consider ICEs.\nFor example, if two participants are assigned to the treatment arm of a trial but one receives additional medicine, then the information on the treatment differs between the two participants. Additionally, whether a participant needs to take additional medication and whether or not they can continue taking the assigned treatment might depend on which treatment arm they are exposed to. What needs to be decided is whether these events should be considered explicitly when defining the clinical question.\nThe guidance does not go into much detail for what ICEs it has considered so Table 1 gives a (somewhat) generic list:\n\n\nCode\nd_tbl &lt;- fread(\"data/intercurrent_events_list.csv\")\nd_tbl[, id_type := .GRP, by = type]\n\n# Order the data\nsetorder(d_tbl, id_type)\n\n# Blank out repeated 'type' and 'subtype' values for better visual grouping\nd_tbl[, my_lag := shift(type, type = \"lag\")]\nd_tbl[, type_display := ifelse(my_lag != type | is.na(my_lag), type, \"\")]\nd_tbl[, my_lag := NULL]\n\n\nd_tbl[, my_lag := shift(subtype, type = \"lag\")]\nd_tbl[, subtype_display := ifelse(my_lag != subtype | is.na(my_lag), subtype, \"\")]\nd_tbl[, my_lag := NULL]\n\nd_tbl[, .(type, subtype_display, element)] |&gt;\n  gt(groupname_col = \"type\") |&gt;\n  gt::text_transform(\n    locations = cells_row_groups(),\n    fn = function(x) {\n      lapply(x, function(x) {\n        gt::md(paste0(\"**\", x, \"**\"))\n      })\n    }\n  ) |&gt;\n  cols_label(\n    # type = \"Type\",\n    subtype_display = \"Subtype\",\n    element = \"Description\"\n  ) |&gt;\n  cols_width(\n    subtype_display ~ pct(25),\n    element ~ pct(75)\n  ) |&gt;\n  # tab_style(\n  #   style = cell_text(indent = px(20)),\n  #   locations = cells_body(columns = subtype_display)\n  # ) |&gt;\n  fmt_markdown(columns = everything()) |&gt;\n  tab_options(\n    table.font.size = \"x-small\"\n  )\n\n\n\n\nTable 1: Examples of possible ICEs\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubtype\nDescription\n\n\n\n\nParticipant-Level Events\n\n\nTreatment Discontinuation\nDiscontinuation due to adverse events\n\n\n\nDiscontinuation due to lack of efficacy\n\n\n\nDiscontinuation due to participant choice\n\n\n\nDiscontinuation due to investigator decision\n\n\nTreatment Switching / Change in Assigned Treatment\nSwitch to another active treatment\n\n\n\nSwitch to placebo/control arm\n\n\n\nEscalation or de-escalation of dose outside of protocol-defined rules\n\n\nInitiation of Rescue or Concomitant Medication\nUse of rescue medication for symptom relief\n\n\n\nUse of non-protocol permitted concomitant medication\n\n\n\nUse of alternative (non-protocol) therapy for the condition under study\n\n\nNon-Adherence to Treatment Regimen\nPartial compliance (e.g., missed doses)\n\n\n\nTaking incorrect dose or frequency\n\n\n\nUse of alternative medicine or supplements that interact with treatment on outcome\n\n\nWithdrawal from Follow-up\nParticipant withdrawal of consent for follow-up\n\n\n\nLoss to follow-up (cannot be contacted)\n\n\n\nWithdrawal due to logistical/administrative issues (system error, data loss)\n\n\nMortality\nDeath due to study treatment\n\n\n\nDeath due to disease under study\n\n\n\nDeath due to unrelated causes\n\n\n\nDeath with unknown attribution\n\n\nClinical and Health Events\n\n\nOccurrence of a Clinical Event that Alters Outcome Interpretation\nIntercurrent illness affecting outcome measurement (e.g. contracting COVID-19)\n\n\n\nComplication or comorbidity unrelated to treatment\n\n\n\nPregnancy (in trials not focused on pregnancy)\n\n\nDiagnosis of a New Condition\nDiagnosed with a comorbidity during follow-up\n\n\n\nPsychiatric diagnosis altering engagement or assessment\n\n\nHospitalisation or Institutionalisation\nEmergency hospitalisation unrelated to study treatment\n\n\n\nAdmission to rehabilitation, palliative, or psychiatric care\n\n\nMeasurement and Assessment Events\n\n\nMissing Outcome Data Due to Operational Issues\nMissed clinic visit or window for assessment\n\n\n\nData entry failure, corrupted files, lost records\n\n\nChange in Method of Outcome Measurement\nDifferent device, temporary staff replacement, non-trial method used\n\n\n\nIncomplete outcome data (data entry mishaps)\n\n\nUnblinding of Treatment Allocation\nAccidental unblinding by staff or participant (clinician unblinded impacting subsequent assessment and care)\n\n\n\nProtocol-mandated unblinding\n\n\nParticipant Non-Cooperation During Assessment\nRefusal to undergo a test or procedure\n\n\n\nInability to perform task due to unrelated cause\n\n\nProtocol and Trial Conduct Issues\n\n\nProtocol Deviations / Violations\nIncorrect treatment administration\n\n\n\nEnrolment of ineligible participant\n\n\n\nMissed or out-of-window assessments\n\n\nTrial Discontinuation Events\nStudy site closure\n\n\n\nSponsor decision to stop trial early\n\n\n\nRegulatory action halting the study\n\n\nRandomisation Error\nMisallocation to incorrect arm\n\n\n\nSystem malfunction during randomisation\n\n\nContextual and External Events\n\n\nEnvironmental or Societal Disruptions\nNatural disaster, pandemic, conflict affecting follow-up\n\n\n\nTravel restrictions preventing visits\n\n\nChange in Standard of Care\nIntroduction of new competing treatment during the trial\n\n\n\nPublic access to investigational treatment\n\n\nInsurance or Access Issues\nLoss of access to trial medication or assessments due to coverage\n\n\n\nChanges in healthcare system or site practices\n\n\n\n\n\n\n\n\n\n\nICEs can have a range of impacts on the outcome and/or treatment:\n\nDiscontinuation of the assigned treatment or the use of additional or alternative therapy can impact the interpretation of outcome measures and thus treatment effect.\nDeath may lead to the outcome measure being impossible to observe.\n\nSome events will have permanent impacts on the outcome, others may be temporary.\nICEs may also fully (or partially) define principal strata. For example, some participants may never complete any of the treatments that they may be assigned.\nThe guideline offers several strategies for handling intercurrent events:\n\nTreatment policy\nHypothetical\nComposite\nWhile on treatment\nPrincipal stratum\n\n\nTreatment policy\nWithin TP, ICE are considered irrelevant to the treatment effect of interest and thus the ICE becomes part of the treatment regime under comparison. In this sense, TP essentially replicates the ITT principle. However, TP cannot be implemented for terminal events such as death as the outcome variable will not exist.\n\n\nHypothetical\nThe hypothetical estimand envisages a situation where the event would not occur. However, this estimand might be difficult to implement. For example, if discontinuation of medication is the event in question, we might consider what would be the effect of the drug if all participants did adhere. One of the issues is that the adherence status of participants on other treatment arms had they been assigned to the drug in question was not observed.\nAs another example, we might consider the hypothetical estimand of the effect of a new inhaler were rescue medication inhaled. In this situation, the approach is implausible as it is unrealistic to implement and ethically problematic.\n\n\nComposite\nThe composite strategy treats the occurrence of an ICE as being informative of the patients outcome. Death is the classic example, but discontinuation because of toxicity is also a good example of a form of treatment failure.\n\n\nWhile on treatment\nWhile on treatment is primarily for situations where the outcome variable is measure repeatedly and we are considering the effect of treatment up to the point where the ICE occurs.\n\n\nPrincipal stratum\nUnder principal stratum we consider the stratum where the ICE either occurs or does not and the clinical question relates to the effects only within the strata. For example, we might be interested in the effect of treatment on severity of the infection in the patients that become infected after infection.\nThe guideline is very clear on the importance of distinguishing principal stratification (based on potential intercurrent events) from subsetting based on actual ICEs because the subset of participants who experience an ICE on the test treatment will likely be different from the subset that experience the same ICE on control. A naive perspective will lead to treatment effects being defined by comparing outcomes in subsets that confound the effects of the different treatments with the differences possibly being due to the different participant characteristics rather than any real effect.\n\n\nComponents of estimands\nTo define an estimand, the treatment condition, the popoulation (entire trial population, subgroup, principal stratum etc), the endpoint to be obtained for each patient and population summary need to be considered and precisely specified.\nThe guideline notes the important that when defining the treatment effect of interest, the definition identifies an effect due to treatment and not due to potential confounders such as differences in the duration of observation or participant characteristics.\n\n\nConsiderations\nThe main emphasis of the guideline is to specify estimands that are useful to the decision maker under consideration. For example, a hypothetical estimand that evaluates what the variable of interest would have been in absence of rescue medication might be important in certain scenarios but might be completely unrealistic in others.\nMore to come…"
  }
]