[
  {
    "objectID": "notebooks/robust-errors.html",
    "href": "notebooks/robust-errors.html",
    "title": "Robust errors for estimating proportion",
    "section": "",
    "text": "The goto approach for estimating an uncertainty interval for a proportion is to use the normal approximation of the binomial distribution:\n\\[\n\\begin{aligned}\n\\hat{p} \\pm z_{(1 - \\alpha)/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\end{aligned}\n\\]\nwhere \\(n\\) is the sample size, \\(\\hat{p}\\) is the observed sample proportion and \\(z\\) is the standard normal quantile (and typically set to \\(\\approx 2\\)).\nSay we had multiple estimates of the proportion, e.g. number of times we observe antimicrobial resistance out of the positive blood cultures we collected over the previous year. These estimates might come from differing hospitals and some might include repeat tests on individuals. This means that we have multiple levels of variation to deal with. One approach is to use a robust (sometimes call a Hubert White or sandwich) estimator for the standard errors.\nThis can be achieved by fitting a standard glm and then making an adjustment to the standard errors using the tools provided in the R sandwich package.\nSimulate data for 500 patients from 10 sites, some patients having repeat measures.\n\nlibrary(sandwich)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(marginaleffects)\nlibrary(lme4)\n\nLoading required package: Matrix\n\n# library(gee)\nlibrary(\"geepack\")\n\n# N unique pts on which we have multiple obs, each pt nested within one of\n# the 10 sites\nN &lt;- 500\nN_site &lt;- 10\np_site &lt;- as.numeric(extraDistr::rdirichlet(1, rep(1, 10)))\nsites &lt;- sample(1:N_site, N, replace = T, prob = p_site)\nd_pt &lt;- data.table(\n  id_pt = 1:N,\n  site = sort(sites)\n)\n# number obs per pt - inflated here to make a point\nn_obs &lt;- rpois(N, 2)\nd &lt;- d_pt[unlist(lapply(1:N, function(x){\n  rep(x, n_obs[x])\n}))]\nd[, id_obs := 1:.N, keyby = id_pt]\n\n# about 60% (plogis(0.4)) resistant but with site and subject level\n# variability beyond the natural sampling variability due to varying \n# number of subjects per site\nnu &lt;- rnorm(N, 0, 0.5)\n# treat site as true fixed effect\nrho &lt;- rnorm(N_site, 0, 0.7)\nd[, eta := 0.4 + rho[site] + nu[id_pt]]\n\nd[, y := rbinom(.N, 1, plogis(eta))]\nd[, site := factor(site)]\nd[, id_pt := factor(id_pt)]\n\np_obs &lt;- d[, sum(y)/.N]\n\n# d[, .(y = sum(y), n = .N)]\n# distribution of frequency of observations on a pt\n# hist(d[, .N, keyby = id_pt][, N])\n\nThe raw numbers of observations at each site are shown in Figure 1.\n\nd_fig &lt;- copy(d)\nd_fig[y == 0, resp := \"Susceptible\"]\nd_fig[y == 1, resp := \"Resistant\"]\nggplot(d_fig, aes(x = site, fill = resp)) +\n  geom_bar() +\n  scale_fill_discrete(\"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: Observations by site\n\n\n\n\n\nOverall, the observed proportion resistant to antibiotics is 0.591. Various ways exist to estimate the uncertainty.\nThe wald estimate for the uncertainty interval is calculated as:\n\n# wald (normal approximation)\nse_wald &lt;- sqrt(p_obs * (1-p_obs) / nrow(d))\np_0_lb &lt;- p_obs - qnorm(0.975) * se_wald\np_0_ub &lt;- p_obs + qnorm(0.975) * se_wald\n\nA GLM with only an intercept term will give the same prediction as the observed proportion and we can calculate the naive estimate of uncertainty as:\n\n# standard glm, not accounting for pt\nf1 &lt;- glm(y ~ 1, family = binomial, data = d)\n\npredict(f1, type = \"response\")[1]\n\n        1 \n0.5909091 \n\n# get naive standard errors\ns_f1 &lt;- summary(f1)$coef\n\n# model uncertainty naive\nlo_1 &lt;- qlogis(p_obs)\n# se from intercept term, i.e. we are just looking at the 'average' or\n# typical pt, over which we would expect heterogeneity\np_1_lb &lt;- plogis(lo_1 - qnorm(0.975) * s_f1[1, 2])\np_1_ub &lt;- plogis(lo_1 + qnorm(0.975) * s_f1[1, 2])\n\nWe can use the sandwich estimator to adjuste for heterogeneity as:\n\n# adjusted to account for heterogeneity due to site (we did not \n# adjust for site in the model) and repeat measure for pt\nsw_se &lt;- sqrt(vcovCL(f1, cluster = d[, .(site, id_pt)], type = \"HC1\")[1,1])\np_2_lb &lt;- plogis(lo_1 - qnorm(0.975) * sw_se)\np_2_ub &lt;- plogis(lo_1 + qnorm(0.975) * sw_se)\n\n\nf2 &lt;- glmer(y ~ (1|site) + (1|id_pt), data = d, family = binomial)\n\nNote that in the glmer model (with a non-linear link function) the predictions are first made on the link scale, averaged, and then back transformed. This means that the average prediction may not be exactly identical to the average of predictions.\nYou’ll note that the point estimate from the glmer deviates from the observed proportion. This can be for all of the following reasons:\n\nThe GLMM provides subject-specific estimates, conditional on the random effects.\nGLMMs involve shrinkage, where estimates for groups with less data are pulled towards the overall mean.\nLarger random effects can lead to bigger differences.\nThe GLMM estimate is a model-based estimate that accounts for the hierarchical structure of the data and provides a framework for inference.\n\nIn theory, a GEE could also be used but in R the GEE framework is not particularly well set up for multiple levels of clustering.\n\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_0_lb, p_0_ub)\n\n[1] \"0.5909 (0.5609, 0.6209)\"\n\n# Model based adjusting for site.\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_1_lb, p_1_ub)\n\n[1] \"0.5909 (0.5606, 0.6205)\"\n\n# Adjusted - account for heterogeneity due to site (we did not \n# adjust for site) and repeat measure for pt\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_2_lb, p_2_ub)\n\n[1] \"0.5909 (0.5093, 0.6678)\"\n\n# Finally a random effects model\navg_predictions(f2, re.form = NA, type = \"link\")\n\n\n Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n    0.445      0.181 2.46    0.014 6.2 0.0899    0.8\n\nType: link\n\navg_predictions(f2, re.form = NA, type = \"response\")\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    0.609     0.0431 14.1   &lt;0.001 148.4 0.525  0.694\n\nType: response\n\n## Step 1\npred &lt;- predictions(f2, type = \"link\", re.form = NA)$estimate\n## Step 2: average\nplogis(mean(pred))\n\n[1] 0.609387\n\n\n\nReferences"
  },
  {
    "objectID": "notebooks/probabilistic-index.html",
    "href": "notebooks/probabilistic-index.html",
    "title": "Probabilistic Index Models",
    "section": "",
    "text": "The probabilistic index (PI), also known as the probability of superiority, refers to the probability that the outcome of a randomly selected subject in the treatment group exceeds the outcome of another randomly selected subject on the control group [1]. Thus a PI of 50% indicates that a patient on the experimental treatment is as likely to be better or worse as compared with a patient on the control treatment.\nThe PI is the metric associated with the Mann-Whitney-U test (aka Wilcoxon-Mann-Whitney). When modelled under a regression framework with conditioning on the covariate values of both subjects, we have a probabilistic index model (PIM).\nIn notational terms, if we let \\(y\\) denote a univariate outcome and \\(\\vec{x}\\) a vector of covariates for a unit, then the PI is given by \\(\\text{Pr}(y_i &lt; y_j | \\vec{x_i}, \\vec{x_j})\\) where \\(i\\) and \\(j\\) denote distinct units.\nTwo things are clear from this definition:\n\nThe PI does not provide information on the magnitude of the difference between two populations.\nThe measure is always comparing two different subjects, it does not give the probability that a single patient will benefit from a given treatment as compared with the conventional treatment.\n\nIn a two-sample setting we can use the MWU to compute the probabilistic index. However, when our treatment is continuous or we wish to condition on a set of covariates then it is desirable to embed the PI into a regression model. The approach is to model the conditional PI directly as a function of covariates:\n\\[\n\\begin{aligned}\n\\text{Pr}(Y_i &lt; Y_j | X_i, X_j) = m(X_i, X_j, \\beta)\n\\end{aligned}\n\\]\nwhere \\(m(.)\\) is some user-specified function and \\((Y_i, X_i)\\) are iid and \\(X_i\\), \\(X_j\\) and \\(\\beta\\) are vector quantities. It is convenient to choose \\(m\\) as:\n\\[\n\\begin{aligned}\nm(X_i, X_j, \\beta) = g^{-1}[(X_j - X_i)^\\top \\beta]\n\\end{aligned}\n\\]\nTo interpret the regression coefficient, consider two subjects \\(i\\) and \\(j\\) with covariate patterns \\(X^\\top = (Z_1, Z_2)\\) with \\(\\beta^\\top = (\\beta_1, \\beta_2)\\) (for a bivariate case). Say, subject \\(i\\) has covariate values \\((z_1, z_2)\\) and subject \\(j\\) has values \\((z_1 + 1, z_2)\\) so that both have the same value for \\(Z_2\\) but where \\(Z_1\\) differs by one unit. It follows that:\n\\[\n\\begin{aligned}\n\\text{Pr}(Y_i &lt; Y_j | Z_{1i} = z_1, Z_{1j} = z_1 + 1, Z_{2i} = Z_{2j}) = g^{-1}{\\beta_1}\n\\end{aligned}\n\\]\nso that \\(g^{-1}{\\beta_1}\\) gives the probability that a randomly selected subject with covariate value \\(z_1\\) for \\(Z_1\\) will have a lower outcome as compared with a randomly selected subject with a covariate value that is higher by one and where \\(Z_2\\) is the same for both subjects.\n\n\n\n\n\n\nNote\n\n\n\nNotice the absence of an intercept in these models. This means that when the covariate patterns are the same, the probability that \\(Y_i\\) is less than \\(Y_j\\) is 0.5 and vice versa.\n\n\nAs might be clear, the above can be handled via a logistic regression applied to the transformed binary outcome \\(I_{ij} = I(Y_i &lt; Y_j)\\) and predictors \\(X_{ij} = X_j - X_i\\). From this, the MLE give consistent estimates for \\(\\beta\\), [2]. However, despite the fact that \\((Y_i, X_i)\\) are mutually independent, the transformed data \\(I_{ij}\\) and \\(X_{ij}\\) are not. This is obviously the case if we consider \\(I_{ij} = I(Y_i &lt; Y_j)\\) and \\(I_{ik} = I(Y_i &lt; Y_k)\\) since both share \\(Y_i\\) making them no longer independent. Moreover, the \\(I_{ij}\\) have a correlation structure that is different from the typical block correlation structure in multi-level data. Thas introduced a sandwich estimator for the standard errors (frequentist setting) implemented in the R package pim, [2] and [3]. However, a bootstrap approach could also be used - simply take \\(B\\) bootstrap samples (with replacement) of size \\(n\\) and then repeat whatever estimation process was used.\n\n\n\n\n\n\nNote\n\n\n\nThe above specification can be modified to deal with ties by modifying the transformed outcome to \\(I(Y_i &lt; Y_j) + 0.5I(Y_i = Y_j)\\).\n\n\n\nReferences\n\n\n1. De Schryver M. A tutorial on probabilistic index models: Regression models for the effect size p(Y1 &gt; Y2). American Psychological Association. 2019;24.\n\n\n2. Thas O. Probabilistic index models. Journal of the Royal Statistical Society Series B Methodological. 2012;74:623–71.\n\n\n3. Meys J. Pim fit probabilistic index models. 2017."
  },
  {
    "objectID": "notebooks/power-analysis-by-sim.html",
    "href": "notebooks/power-analysis-by-sim.html",
    "title": "Power analysis by simulation",
    "section": "",
    "text": "library(data.table)\nlibrary(ggplot2)\nlibrary(gt)\n\nPower analyses inform us as to chances an experiment has of identifying a treatment differences based on our criteria, various assumptions about the design, sample size and over the effect sizes we imagine might arise. Power analyses can be implemented via simulation. The following methods provide a basic demonstration of the power analysis for an experiment with a single analysis where we consider the binary outcome under differing treatment regimes at a fixed sample and a range of effect sizes. For the sake of the example, logistic regression is used as the analysis model although one might reasonable select from a range of analysis models, with some being more efficient than others. For example, non-parametric approaches may yield lower power, but might make less assumptions, depending on the particular method chosen.\nAssume:\n\na baseline probability of response in the standard treatment that equals 0.2\neffect sizes from -0.25 to 0.25 on the risk scale\nsample size of 100 per treatment type\n\nMethod 1 creates a fine grid over the entire effect size range of interest and then performs a single trial for each of these effect sizes based on data simulated at this effect size. For each trial, we will either detect an effect or not based on the criteria we are using to make a decision (i.e. reject a null hypothesis). The probability of detecting an effect for each simulated trial equates to the power at that effect size. If we interpolate over these indicator values as to whether an effect was detected or not, we can produce an approximation of the power curve over the entire range.\n\nn_sim &lt;- 10000\np_0 &lt;- 0.5\nrd &lt;- seq(-0.25, 0.25, len = n_sim)\nwin &lt;- numeric(n_sim)\n\ny_0 &lt;- rbinom(n_sim, 100, p_0)\ny_1 &lt;- rbinom(n_sim, 100, p_0 + rd)\n\ni &lt;- 1\nfor(i in 1:n_sim){\n  \n  y &lt;- rbind(\n    c(y_0[i], 100 - y_0[i]),\n    c(y_1[i], 100 - y_1[i])\n  )\n  x &lt;- c(0, 1)\n  \n  f1 &lt;- glm(y ~ x, family = binomial)\n  # assume a typical frequentist critria\n  win[i] &lt;- summary(f1)$coef[2, 4] &lt; 0.025\n  \n}\n\n\nd_fig_1 &lt;- data.table(\n  x = rd,\n  win = win\n)\n\nMethod 2 simulates a large number of trials at each effect size of interest. The power at each effect size is computed as the proportion of trials where the effect was identified based on the criteria we are using to make a decision. This process is done for a range of effect sizes below. Method 2 is clearly more computationally demanding than method 1.\n\nn_sim &lt;- 5000\np_0 &lt;- 0.5\np_1 &lt;- p_0 + c(-0.2, -0.1, 0.0, 0.1, 0.2)\n\npwr &lt;- numeric(length(p_1))\n\n\nfor(i in 1:length(p_1)){\n  \n  y_0 &lt;- rbinom(n_sim, 100, p_0)\n  y_1 &lt;- rbinom(n_sim, 100, p_1[i])\n  \n  win &lt;- numeric(n_sim)\n  \n  for(j in 1:n_sim){\n  \n    y &lt;- rbind(\n      c(y_0[j], 100 - y_0[j]),\n      c(y_1[j], 100 - y_1[j])\n    )\n    x &lt;- c(0, 1)\n    \n    f1 &lt;- glm(y ~ x, family = binomial)\n    win[j] &lt;- summary(f1)$coef[2, 4] &lt; 0.025\n    \n  } \n  \n  pwr[i] &lt;- mean(win)\n}\n\n\nd_fig_2 &lt;- data.table(\n  x = p_1 - p_0,\n  pwr = pwr\n)\n\n\n\n\n\n\n\n\n\nFigure 1: Power curves characterising power by effect size at a sample size of 100 per arm in two group study with binary outcome (points show results from method 2)"
  },
  {
    "objectID": "notebooks/mann-whitney-u.html",
    "href": "notebooks/mann-whitney-u.html",
    "title": "Mann-Whitney U",
    "section": "",
    "text": "The MWU checks a rank sum difference between two independent groups and tests whether two groups have been drawn from the same population. It is an alternative to the t-test. But it only requires that the observations from both groups are at least ordinal such that you can discern for any two observations which one is greater (or better in some sense). In the classical interpretation, the test assumes that the distributions are identical under the null hypothesis and that they are not identical under the alternative distribution. Underlying the test is an interest in the probability that a randomly selected unit from one group fairs better than a randomly selected unit from the other. More concretely, the interest is int:\n\\[\n\\text{Pr}(X &gt; Y) \\ne \\text{Pr}(Y &gt; X)\n\\]\nand if this quantity exceeds \\(0.5\\) then, all things being equal, you would prefer assignment to the \\(X\\) group.\nOne approach to the procedure involves combining all the observations and ordering the records from best to worse (keeping track of which record belongs to which group). Each row is assigned a rank, the sum of the ranks is calculated for the first group (\\(T_1\\)) and the second group (\\(T_2\\)) and then the \\(U\\) statistics are computed as:\n\\[\n\\begin{aligned}\nU_1 &= n_1 n_2 + \\frac{n_1(n_1 + 1)}{2} - T_1 \\\\\nU_2 &= n_1 n_2 + \\frac{n_2(n_2 + 1)}{2} - T_2 \\\\\n\\end{aligned}\n\\]\nthe Mann-Whitney-U is given by \\(U = \\text{min}(U_1,U_2)\\), which is the test statistic.\nFor example, say we have units 1 to 20 in the first group and units 21 to 40 in the second with all the units in the first group having a better score than the units in the second.\n\nlibrary(data.table)\nlibrary(ggplot2)\n# contrived setup to make the scores for units ascend \n# (lower values considered better)\nd &lt;- data.table(\n  i = 1:40, score = sort(rnorm(40, 0, 1)), \n  group = rep(1:2, each = 20)\n)\nd[, rank := 1:40]\nn1 &lt;- d[group == 1, .N]\nn2 &lt;- d[group == 2, .N]\n\nNow add up the ranks for both groups.\n\n(T1 = d[group == 1, sum(rank)])\n\n[1] 210\n\n(T2 = d[group == 2, sum(rank)])\n\n[1] 610\n\n\nFrom these compute \\(U_1\\) and \\(U_2\\)\n\n(U_1 = n1*n2 + n1*(n1+1)/2 - T1)\n\n[1] 400\n\n(U_2 = n1*n2 + n2*(n2+1)/2 - T2)\n\n[1] 0\n\n\nThe above is equivalent to a pairwise comparison procedure (see later). From this, we can estimate the probability of superiority as referred to earlier (which I believe, but am not certain, is assuming that the two distributions differ only in location, not in shape) as:\n\n# here the contrived setup says that the value of x_1 and x_2 also equate to their ranks\n# and that lower values are better.\nx_1 &lt;- 1:20\nx_2 &lt;- 21:40\nn_1 &lt;- length(x_1)\nn_2 &lt;- length(x_2)\nu_1 &lt;- 0\nu_2 &lt;- 0\n\nk &lt;- 1\nfor(i in 1:length(x_1)){\n  for(j in 1:length(x_2)){\n    if(x_1[i] &lt; x_2[j]) u_1 &lt;- u_1 + 1\n    else if(x_1[i] &gt; x_2[j]) u_2 &lt;- u_2 + 1\n    k &lt;- k + 1\n  }\n}\nsprintf(\"u1 = %.0f, Pr(X&gt;Y) = %.2f, u2 = %.0f, Pr(Y&gt;X) = %.2f\", u_1, u_1/(n_1*n_2), u_2, u_2/(n_1*n_2))\n\n[1] \"u1 = 400, Pr(X&gt;Y) = 1.00, u2 = 0, Pr(Y&gt;X) = 0.00\"\n\n\nThat is, for all possible pairwise combinations, we compare the value in the first group to each value in second group and add up how often X &gt; Y and compute the probability estimate of \\(\\text(Pr)(X&gt;Y)\\) by simply dividing the number of occurrences by the total number of pairs.\nFor the test, we take the following reference points. First, under no difference, the expected value for \\(U\\) is\n\\[\n\\mathbb{E}[U] = \\frac{n_1 n_2}{2}\n\\]\nwith a standard standard error of:\n\\[\n\\sigma_U = \\sqrt{\\frac{n_1 n_2 (n_1 + n_2 + 1)}{12}}\n\\]\nUsing these, you can compute a z-value using a normal approximation (for large samples &gt; 20 per group) in the usual way by taking the observed value for \\(U\\) subtracting the expected value and dividing by the standard error:\n\\[\nz = \\frac{U - \\mu_u}{\\sigma_U}\n\\]\nand obtain a p-value for the test."
  },
  {
    "objectID": "notebooks/mann-whitney-u.html#footnotes",
    "href": "notebooks/mann-whitney-u.html#footnotes",
    "title": "Mann-Whitney U",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe intuition for the above is that \\(F_e(x)\\) gives the probability that \\(X_e\\) is greater than a specific value of \\(x\\), and \\(f_c(x) dx\\) gives the probability that \\(X_c\\) falls in a small interval around \\(x\\). The product of these terms gives the probability that \\(X_e &gt; X_c\\) for that small interval and thus integrating over all possible values of \\(x\\) gives the value we are after.↩︎"
  },
  {
    "objectID": "notebooks/limits-of-per-protocol.html",
    "href": "notebooks/limits-of-per-protocol.html",
    "title": "Limitations of per-protocol analyses",
    "section": "",
    "text": "Patients in RCTs that discontinue before the endpoint often have poorer prognosis than those who continue treatment. Additionally, discontinuation is more common in the treatments that create more side effects or have less clear benefit.\nThe traditional per-protocol analyses will exclude those patients that are not completers and that deviate in some unacceptable way from the protocol (e.g. early discontinuation of treatment) and then run the primary analysis unchanged. That is, the comparison groups are defined in part by post randomisation events that are influenced by treatment group status and other factors. This can be problematic in that it violates the randomisation principle, the treatment groups can become unbalanced in either known or unknown covariates, selection bias can arise, effect estimates may be biased and external validity threatened, [1]. The following example illustrates this.\nCompare a new antihypertensive (A) to a standard treatment (B) for lowering blood pressure in patients with existing hypertension in an RCT. At 6-months patients are assessed to be either still hypertensive (0) or no longer hypertensive (1). One thousands patients are randomised, 500 to each arm.\nIt is common for trials to show differential adherence by study group. Here, assume that A causes more side effects than B and at 3 months, 20% of patients stop taking A due to side effects and 4% stop taking B for a side effect intercurrent event. We interpret this as a protocol deviation (although who knows whether this would actually be consider to be a protocol deviation in reality). Additionally, assume that the patients who experience side effects (and therefore would dropout of the traditional PP analysis) tend to have more severe hypertension and would have shown less improvement in blood pressure status irrespective of the treatment they received.\nThe traditional per-protocol analysis excludes those that deviate from the protocol and we would therefore have around 400 patients in A and 480 patients in B.\nFinally assume that the true percentage that are no longer hypertensive at 6 months is 40% and 20% in group A and B respectively.\nBelow is a simulation of the assumed data generation process.\n\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(parallel)\n\nget_data &lt;- function(\n    N = 1000\n){\n  \n  d &lt;- data.table(i = 1:N)\n  d[, trt := rep(0:1, each = N/2)]\n  d[, sev := rnorm(N, 0, 1) ]\n  \n  d[trt == 0, p_ae := plogis(qlogis(0.04) + 0.4 * sev)]\n  d[trt == 1, p_ae := plogis(qlogis(0.40) + 0.4 * sev)]\n  \n  d[, discont := rbinom(.N, 1, p_ae)]\n  \n  d[trt == 0, p_y := plogis(qlogis(0.20) - 0.3 * sev)]\n  d[trt == 1, p_y := plogis(qlogis(0.40) - 0.3 * sev)]\n  \n  d[, y := rbinom(.N, 1, p_y)]\n  \n  d\n}\n\nSimulating this data generation process a large number of times, the ITT estimate along with the PP estimates can be computed to gain some insight of their long-run properties.\n\nm_res &lt;- do.call(rbind, mclapply(1:1e4, FUN = function(i){\n  d &lt;- get_data()\n\n  # ITT analysis\n  f1 &lt;- glm(y ~ trt, data = d, family = binomial())\n  rd_f1 &lt;- diff(predict(f1, type = \"response\", newdata = data.table(trt = 0:1)))\n  \n  # Traditional PP analysis \n  f2 &lt;- glm(y ~ trt, data = d[discont == 0], family = binomial()) \n  rd_f2 &lt;- diff(predict(f2, type = \"response\", newdata = data.table(trt = 0:1)))\n  \n  # G-comp - weights per the full sample\n  f3 &lt;- glm(y ~ trt + sev, data = d[discont == 0], family = binomial()) \n  \n  d_trt_1 &lt;- copy(d)\n  d_trt_1[, trt := 1]\n  d_trt_0 &lt;- copy(d)\n  d_trt_0[, trt := 0]\n  \n  f3_eta_trt_1 &lt;- predict(f3, newdata = d_trt_1)\n  f3_eta_trt_0 &lt;- predict(f3, newdata = d_trt_0)\n  \n  rd_f3 &lt;- plogis(mean(f3_eta_trt_1))  - plogis(mean(f3_eta_trt_0))\n  \n  c(rd_f1, rd_f2, rd_f3)\n  \n}, mc.cores = 6))\n\nFigure 1 shows the results. Specifically, the distribution of the estimated treatment effect (risk difference) which is known to have a true value of 0.2 in favour of the new drug (A).\nUnder the ITT analysis (where we ignore the fact that the ICE occurred and simply proceed to analyse all the data, irrespective of whether the ICE occurred or not) the long-run expected value for the MLE estimate of the treatment effect aligns with the true value of 0.2.\nHowever, in the traditional PP analysis, we drop those patients for whom the protocol deviation applies. We fit the same model as was used in the ITT analysis to the remaining data. For this approach, we can see that the expected value for the MLE estimate is inflated.\nThe final plot shows the results from a revised approach to the per-protocol analysis where we:\n\nrun the analysis assuming that the deviations are censored, i.e. we drop anyone that had the side effects\nadd variables that are predictive of the ICE into the model\nfor the entire data set make predictions of the outcome assuming that all patients are assigned to the standard treatment (A)\nrepeat 3 for the patients assuming that all patients are assigned to the standard treatment (B)\ncompute the treatment effect (risk difference) as the difference between the means of the predicted values each transformed back to the risk scale\n\nThis final approach is aligned with a G-computation perspective and gives results similar to those that would be produced from an inverse probability of censoring weighting scheme. In both cases, we are effectively, producing a re-weighted estimate of the effect, but going about it in slightly different ways.\nUnder this revised per-protocol approach, we once again produce an estimate of the treatment effect that has a expected value close to the known true value of 0.2. Clearly, this is a very much simplified and synthetic example, but it is only intended to give an introduction.\n\nd_fig &lt;- data.table(m_res)\nnames(d_fig) &lt;- c(\"itt\", \"pp_1\", \"pp_2\")\nd_fig &lt;- melt(d_fig, measure.vars = names(d_fig))\n\nd_fig[variable == \"itt\", variable := \"ITT\"]\nd_fig[variable == \"pp_1\", variable := \"Traditional PP\"]\nd_fig[variable == \"pp_2\", variable := \"PP via G-Computation\"]\nd_fig[, variable := factor(variable, levels = c(\n  \"ITT\", \"Traditional PP\", \"PP via G-Computation\"\n))]\n\nggplot(d_fig, aes(x = value, group = variable, col = variable)) + \n  geom_density() +\n  geom_vline(data = d_fig[, mean(value), keyby = variable], \n             aes(xintercept = V1, col = variable)) +\n  scale_color_discrete(\"Analysis\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(~variable, ncol  = 1)\n\n\n\n\n\n\n\nFigure 1: Distribution of effect estimates under ITT, traditional per-protocol and g-computation under the present of ICE\n\n\n\n\n\nThe difference between the two per-protocol approaches can be emphasised by looking at the distribution of the percentage differences between the ITT effect and the PP effects as shown in Figure 2. For the traditional approach, the treatment effect is, on average, about 5% higher than it should be whereas under the revised approach, the PP approach aligns fairly well with the ITT estimate (with both being aligned with the true value).\n\nd_fig &lt;- data.table(m_res)\nnames(d_fig) &lt;- c(\"itt\", \"pp_1\", \"pp_2\")\nd_fig[, pct_diff_1 := 100 * (pp_1 - itt)/itt]\nd_fig[, pct_diff_2 := 100 * (pp_2 - itt)/itt]\n\nd_fig &lt;- d_fig[, .(pct_diff_1, pct_diff_2)]\nd_fig &lt;- melt(d_fig, measure.vars = names(d_fig))\n\nd_fig[variable == \"pct_diff_1\", variable := \"% change (traditional PP vs ITT)\"]\nd_fig[variable == \"pct_diff_2\", variable := \"% change (g-computation PP vs ITT)\"]\n\nd_fig[, variable := factor(variable, levels = c(\n  \"% change (traditional PP vs ITT)\", \n  \"% change (g-computation PP vs ITT)\"\n))]\n\n\nggplot(d_fig, aes(x = value, group = variable, col = variable)) + \n  geom_density() +\n  geom_vline(data = d_fig[, mean(value), keyby = variable], \n             aes(xintercept = V1, col = variable)) +\n  scale_color_discrete(\"Approach\") +\n  scale_x_continuous(breaks = seq(-40, 40, by = 10)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(~variable, ncol = 1)\n\n\n\n\n\n\n\nFigure 2: Distribution of percentage change from the ITT estimate - traditional per-protocol and g-computation\n\n\n\n\n\n\n\n\n\nReferences\n\n1. Greenland S. Estimating effects from randomized trials with discontinuations: The need for intent-to-treat design and g-estimation. Clinical Trials. 2008;5:5–13."
  },
  {
    "objectID": "notebooks/door-1.html",
    "href": "notebooks/door-1.html",
    "title": "Desirability of Outcome Ranking (DOOR)",
    "section": "",
    "text": "DOOR analyses are claimed to be more patient centric. Instead of constructing summary measures by group for each outcome, the DOOR approach combines endpoints at a patient level and then creates a summary measure of the composite view for each intervention.\nThere are two approaches to a DOOR analysis, see [1]. The first approach uses the pairwise comparisons as introduced in Mann-Whitney-U. However, unlike the classical MWU, in the DOOR analysis, all the paired results are incorporated into the test statistic (this can also be done in MWU but wasn’t discussed in the earlier post). The other method used for the DOOR is a partial credit approach, but I do not really understand what that is about.\nAs a result, the DOOR analysis gives you an estimate of the probability that a randomly selected patient in the experimental group will have a better ranking than a randomly selected patient in the control group. The calculation used for the aggregated pairwise comparisons is:\n\\[\n\\begin{aligned}\n\\text{Pr}(door) = \\frac{ (n_{win} + 0.5 n_{tie}) } { n_e n_c }\n\\end{aligned}\n\\]\nwhere \\(n_{win}\\) is the number of times the units in the experimental group had better outcomes compared to the control group, \\(n_{tie}\\) is the number of ties, \\(n_e\\) is the number of units in the experimental group and \\(n_c\\) the number of units in the control group. This measure is also referred to as the probabilistic index [2] or probability of superiority, which will be cover in a separate post.\nIf there is no difference between the two arms, the probability will be close to 50%. Uncertainty intervals can be obtained via bootstrap or other means.\n\n\nLoading required package: Rcpp\n\n\nBuyseTest version 3.2.0\n\n\n\n\nTable 1: Ranking criteria for desirability of outcome for PJI\n\n\n\n\n\n\n\n\n\nRank\nAlive\nJoint Function\nTrt Success\nQoL\n\n\n\n\n1\nYes\nGood\nYes\nTiebreaker based on EQ5D5L\n\n\n2\nYes\nGood\nNo\nTiebreaker based on EQ5D5L\n\n\n3\nYes\nPoor1\nYes\nTiebreaker based on EQ5D5L\n\n\n4\nYes\nPoor\nNo\nTiebreaker based on EQ5D5L\n\n\n5\nNo\n-\n-\n-\n\n\n\n1 Good joint function is based on thresholds related to patient reported success. A successful outcome at 12-months will be defined for knee PJI with an Oxford Knee Score (OKS) at 12 months of &gt;36 or an improvement (delta) from baseline of &gt;9 and for hip PJI as a Oxford Hip Score (OHS) of &gt;38 or an improvement of &gt;12 (35).\n\n\n\n\n\n\n\n\n\n\n\nConsider a DOOR schema and ranking specification for prosthetic joint infection as per Table 1. Patients are assessed and assigned ranks based on how they align with the schema with the goal of differentiating the overall or global outcome of a patient state.\nBelow 100 people per group are simulated based on some hypothetical pair of distributions for the schema. The door probability is computed along with its confidence interval (by bootstrapping):\n\nseed &lt;- 1\nset.seed(seed)\n\nn_e &lt;- 100\nn_c &lt;- 100\np_x_e &lt;- c(0.5, 0.3, 0.1, 0.1, 0.0)\np_x_c &lt;- c(0.3, 0.2, 0.2, 0.2, 0.1)\n  \nx_e &lt;- sample(1:5, n_e, replace = T, p_x_e)\nx_c &lt;- sample(1:5, n_c, replace = T, p_x_c)  \n\nn_win &lt;- 0\nn_tie &lt;- 0\nfor(i in 1:n_e){\n  for(j in 1:n_c){\n    if(x_e[i] &lt; x_c[j]) n_win &lt;- n_win + 1\n    if(x_e[i] == x_c[j]) n_tie &lt;- n_tie + 1\n  }\n}\n\n# estimate for door\npr_door &lt;- (n_win + 0.5 * n_tie)/(n_e*n_c)\n\nboot_door &lt;- function(ix_e, ix_c){\n  \n  x_e_new &lt;- x_e[ix_e]\n  x_c_new &lt;- x_c[ix_e]\n  \n  n_win &lt;- 0\n  n_tie &lt;- 0\n  for(i in 1:n_e){\n    for(j in 1:n_c){\n      if(x_e_new[i] &lt; x_c_new[j]) n_win &lt;- n_win + 1\n      if(x_e_new[i] == x_c_new[j]) n_tie &lt;- n_tie + 1\n    }\n  }\n  \n  (n_win + 0.5 * n_tie)/(n_e*n_c)\n}\n\nn_boot &lt;- 1000\npr_door_rep &lt;- numeric(n_boot)\nfor(i in 1:n_boot){\n  ix_e &lt;- sample(1:n_e, size = n_e, replace = T)\n  ix_c &lt;- sample(1:n_c, size = n_c, replace = T)\n  pr_door_rep[i] &lt;- boot_door(ix_e, ix_c)\n}\n# \ndoor_ci &lt;- quantile(pr_door_rep, probs = c(0.025, 0.975))\n\n# c(pr_door, door_ci)\n\nFrom above, the estimate for the door probability is 0.70 with a (bootstrapped) 95% CI of 0.63, 0.77.\nThe process is simple but the procedure itself does not readily admit to complex modelling. However, Follmann proposed using a logistic regression for the probability of superiority for each determinate pair of patients \\(i\\), \\(j\\) and covariate vectors \\(\\vec{z}_{ij} = \\vec{z}_i - \\vec{z}_j\\) such that the parameters in the model correspond to the log-odds that a patient with \\(\\vec{z}_i\\) has an outcome that is better than a patient with \\(\\vec{z}_j\\) [3]. The presentation from Follmann is pretty convoluted and I lost patience with it. The exposition of probabilistic index models by De Schryver, which is analogous, if not equivalent, is much clearer and will be discussed separately, Probabilistic Index Models.\nA shiny application for door analyses can be found at DOOR although it does not give any detail on the implementation of the methods used. Under the probability-based analysis tab, the overall door and then a decomposition based on each of the dichotomous door components is shown.\nScraping the source data of the site, you can at least recreate some of the statistics. For example, the door probabilities for the ARLG CRACKLE-I demo data as detailed in the door probability-based analysis tab, are replicated below for discharge from hospital:\n\n\n\n\nTable 2: Colistin data from shiny application for DOOR\n\n\n\n\n\n\n\n\n\ntrt\ndoor_num\ndoor_txt\nN\n\n\n\n\nCAZ-AVB\n1\nDischarged home\n6\n\n\nCAZ-AVB\n2\nAlive in hosp, discharged not to home, no renal failure\n17\n\n\nCAZ-AVB\n3\nAlive in hosp, discharged not to home, renal failure\n1\n\n\nCAZ-AVB\n4\nHospital death\n2\n\n\nColistin\n1\nDischarged home\n4\n\n\nColistin\n2\nAlive in hosp, discharged not to home, no renal failure\n25\n\n\nColistin\n3\nAlive in hosp, discharged not to home, renal failure\n5\n\n\nColistin\n4\nHospital death\n12\n\n\n\n\n\n\n\n\n\n\n\nn_e &lt;- d[trt == \"CAZ-AVB\", .N]\nn_c &lt;- d[trt == \"Colistin\", .N]\n\nn_win &lt;- 0\nn_tie &lt;- 0\nfor(i in 1:n_e){\n  for(j in 1:n_c){\n    if(d[trt == \"CAZ-AVB\"][i, discharge_num] &lt; \n       d[trt == \"Colistin\"][j, discharge_num]) n_win &lt;- n_win + 1\n    if(d[trt == \"CAZ-AVB\"][i, discharge_num] == \n       d[trt == \"Colistin\"][j, discharge_num]) n_tie &lt;- n_tie + 1\n  }\n}\npr_door_colistin &lt;- (n_win + 0.5 * n_tie)/(n_e*n_c)\n\n\nboot_door &lt;- function(ix_e, ix_c){\n  \n  x_e_new &lt;- d[trt == \"CAZ-AVB\"][ix_e, discharge_num]\n  x_c_new &lt;- d[trt == \"Colistin\"][ix_c, discharge_num]\n  \n  n_win &lt;- 0\n  n_tie &lt;- 0\n  for(i in 1:n_e){\n    for(j in 1:n_c){\n      if(x_e_new[i] &lt; x_c_new[j]) n_win &lt;- n_win + 1\n      if(x_e_new[i] == x_c_new[j]) n_tie &lt;- n_tie + 1\n    }\n  }\n  \n  (n_win + 0.5 * n_tie)/(n_e*n_c)\n}\n\nn_boot &lt;- 1e3\npr_door_rep &lt;- numeric(n_boot)\nfor(i in 1:n_boot){\n  ix_e &lt;- sample(1:n_e, size = n_e, replace = T)\n  ix_c &lt;- sample(1:n_c, size = n_c, replace = T)\n  pr_door_rep[i] &lt;- boot_door(ix_e, ix_c)\n}\npr_door_colistin_ci &lt;- quantile(pr_door_rep, probs = c(0.025, 0.975))\n\nGiving 0.57 and 95% CI of 0.49, 0.66.\nSimilarly, for renal failure:\n\nd[, .N, keyby = .(trt, renal_num, renal_txt)]\n\nKey: &lt;trt, renal_num, renal_txt&gt;\n        trt renal_num renal_txt     N\n     &lt;char&gt;     &lt;int&gt;    &lt;char&gt; &lt;int&gt;\n1:  CAZ-AVB         0        No    25\n2:  CAZ-AVB         1       Yes     1\n3: Colistin         0        No    39\n4: Colistin         1       Yes     7\n\nn_win &lt;- 0\nn_tie &lt;- 0\nfor(i in 1:n_e){\n  for(j in 1:n_c){\n    if(d[trt == \"CAZ-AVB\"][i, renal_num] &lt; \n       d[trt == \"Colistin\"][j, renal_num]) n_win &lt;- n_win + 1\n    if(d[trt == \"CAZ-AVB\"][i, renal_num] == \n       d[trt == \"Colistin\"][j, renal_num]) n_tie &lt;- n_tie + 1\n  }\n}\npr_door_colistin &lt;- (n_win + 0.5 * n_tie)/(n_e*n_c)\n\nwhich gives 0.56 aligning with the shiny app results.\n\nGeneralised pairwise comparisons\nGPC is a related method and frankly it seems a bit better thought out than DOOR, but I am not sure that it is as popular [4]. The outcomes of interest are first ranked in terms of importance and the pairwise comparison is run progressively on each outcome for all pairs. For the ties under each outcome, the procedure moves on to the outcome that has the next highest priority and so on.\nWhile GPC can be used to produce a range of summary measures, the original paper used net treatment benefit (NTB).\n\\[\n\\begin{aligned}\nNTB = \\frac{ (n_{win} - n_{loss}) } { n_{win} + n_{loss} + n_{tie} }\n\\end{aligned}\n\\]\nwhere \\(n_{win} + n_{loss} + n_{tie}\\) is typically equal to the total number of pairwise comparisons.\nUnlike the DOOR approach, GPC allows for component level contribution and event level correlation. In contrast to the Win Ratio, the net treatment benefit incorporates ties.\nAs an example, consider a situation where we have outcomes, as above, for death, joint function, treatment success and QoL. The procedure first runs pairwise comparisons for all units on death and the number of wins, draws and losses recorded, demonstration below.\n\nset.seed(seed)\nN &lt;- 100\nd &lt;- data.table(\n  id = 1:(2*N),\n  # expt is 1\n  trt = rep(1:0, each = N)\n)\nd[, death := rbinom(.N, 1, prob = 0.4 - 0.2 * trt)]\nd[, jf := rbinom(.N, 1, prob = 0.6 - 0 * trt)]\nd[, success := rbinom(.N, 1, prob = 0.65 + 0.15 * trt)]\nd[, qol := rnorm(.N, 0 + 0.4 * trt, 1)]\n\nn_e &lt;- d[trt == 1, .N]\nn_c &lt;- d[trt == 0, .N]\nn_win &lt;- numeric(4)\nn_loss &lt;- numeric(4)\nn_tie &lt;- numeric(4)\n\n# create a grid to compute all comparisons (quicker than looping)\nsetkey(d, id)\nd_all &lt;- CJ(i = 1:100, j = 100 + (1:100))\n# death\nd_all[, death_i := d[i, death]]\nd_all[, death_j := d[j, death]]\n# note sign direction differs dependent on context of comparison\nd_all[death_i &lt; death_j, death_res := 1]\nd_all[death_i &gt; death_j, death_res := -1]\nd_all[death_i == death_j, death_res := 0]\n# jf\nd_all[, jf_i := d[i, jf]]\nd_all[, jf_j := d[j, jf]]\nd_all[jf_i &gt; jf_j, jf_res := 1]\nd_all[jf_i &lt; jf_j, jf_res := -1]\nd_all[jf_i == jf_j, jf_res := 0]\n# success\nd_all[, success_i := d[i, success]]\nd_all[, success_j := d[j, success]]\nd_all[success_i &gt; success_j,  success_res := 1]\nd_all[success_i &lt; success_j,  success_res := -1]\nd_all[success_i == success_j, success_res := 0]\n# success\nd_all[, qol_i := d[i, qol]]\nd_all[, qol_j := d[j, qol]]\nd_all[qol_i &gt;  qol_j, qol_res := 1]\nd_all[qol_i &lt;  qol_j, qol_res := -1]\nd_all[qol_i == qol_j, qol_res := 0]\nhead(d_all)\n\nKey: &lt;i, j&gt;\n       i     j death_i death_j death_res  jf_i  jf_j jf_res success_i success_j\n   &lt;int&gt; &lt;num&gt;   &lt;int&gt;   &lt;int&gt;     &lt;num&gt; &lt;int&gt; &lt;int&gt;  &lt;num&gt;     &lt;int&gt;     &lt;int&gt;\n1:     1   101       0       1         1     1     0      1         1         1\n2:     1   102       0       0         0     1     1      0         1         0\n3:     1   103       0       0         0     1     1      0         1         0\n4:     1   104       0       1         1     1     1      0         1         0\n5:     1   105       0       1         1     1     1      0         1         1\n6:     1   106       0       0         0     1     0      1         1         0\n   success_res    qol_i      qol_j qol_res\n         &lt;num&gt;    &lt;num&gt;      &lt;num&gt;   &lt;num&gt;\n1:           0 1.293674  1.0744410       1\n2:           1 1.293674  1.8956548      -1\n3:           1 1.293674 -0.6029973       1\n4:           1 1.293674 -0.3908678       1\n5:           0 1.293674 -0.4162220       1\n6:           1 1.293674 -0.3756574       1\n\n\nGPC calculations:\n\n# ntb on death is as follows:\nntb &lt;- numeric(4)\nnames(ntb) &lt;- c(\"death\", \"jf\", \"success\", \"qol\")\nd_res &lt;- d_all[, .N, keyby = death_res]\nd_res[, pct := N / nrow(d_all)]\n\nntb[\"death\"] &lt;- (d_res[death_res == 1, N] - d_res[death_res == -1, N]) /  nrow(d_all)\n\n# for the ties on death, compute jf:\nd_res &lt;- d_all[death_res == 0, .N, keyby = jf_res]\nd_res[, pct := N / nrow(d_all)]\nntb[\"jf\"] &lt;- (d_res[jf_res == 1, N] - d_res[jf_res == -1, N]) /  nrow(d_all)\n\n# for comparisons on all pairs, don't condition:\n# d_res &lt;- d_all[, .N, keyby = jf_res]\n# d_res[, pct := N / nrow(d_all)]\n# d_res\n# (d_res[jf_res == 1, N] - d_res[jf_res == -1, N]) /  nrow(d_all)\n\n# for the ties on death and jf, compute success:\nd_res &lt;- d_all[death_res == 0 & jf_res == 0, .N, keyby = success_res]\nd_res[, pct := N / nrow(d_all)]\nntb[\"success\"] &lt;- (d_res[success_res == 1, N] - d_res[success_res == -1, N]) / nrow(d_all)\n\n# for the ties on death, jf and success, compute qol:\nd_res &lt;- d_all[death_res == 0 & jf_res == 0 & success_res == 0, .N, keyby = qol_res]\nd_res[, pct := N / nrow(d_all)]\nntb[\"qol\"] &lt;- (d_res[qol_res == 1, N] - d_res[qol_res == -1, N]) / nrow(d_all)\n\nNote that for all endpoints, we use the total number of pairwise comparisons as the denominator and not the number of ties left over from the previous outcome.\nThe resulting net treatment benefit reported on each outcome:\n\nntb\n\n  death      jf success     qol \n 0.2300  0.0527  0.0574  0.0455 \n\n\nTaking the cumulative sum, progresses from the effect of each component through to an overall effect:\n\ncumsum(ntb)\n\n  death      jf success     qol \n 0.2300  0.2827  0.3401  0.3856 \n\n\nThe NTB is absolute measure ranging from -1 to 1 with zero being no effect. It estimates the probability that a random unit on the expt arm will do better than a random unit on the control arm minus the probability that a random unit on the control arm will do better than a random unit on the expt arm. For example, if \\(Pr(E&gt;C) = 0.7\\), then \\(Pr(E&lt;C) = 0.3\\) and \\(NTB = 0.7 - 0.3 = 0.4\\).\nYou can compute the overall effect directly with the following:\n\nn_win &lt;- d_all[death_res == 1, .N] + d_all[death_res == 0 & jf_res == 1, .N] +\n   + d_all[death_res == 0 & jf_res == 0 & success_res == 1, .N] +\n   + d_all[death_res == 0 & jf_res == 0 & success_res == 0 & qol_res == 1, .N]\n\nn_loss &lt;- d_all[death_res == -1, .N] + d_all[death_res == 0 & jf_res == -1, .N] +\n   + d_all[death_res == 0 & jf_res == 0 & success_res == -1, .N] +\n   + d_all[death_res == 0 & jf_res == 0 & success_res == 0 & qol_res == -1, .N]\n\n# n_ties &lt;- d_all[death_res == 0 & jf_res == 0 & success_res == 0, .N] \n\n(n_win - n_loss) / nrow(d_all)\n\n[1] 0.3856\n\n\nThe NTB is known to be the inverse of the number needed to treat, i.e. 1/ number of pt you need to trt to avoid one bad outcome. For large samples, inference can again be conducted via bootstrap. R provides the BuyseTest package that allows for stratification (beyond the implicit treatment level stratification).\n\nff1 &lt;- trt ~ bin(death, operator = \"&lt;0\") + bin(jf) + bin(success) + cont(qol)\nf1 &lt;- BuyseTest(ff1, data = d, trace = 0)\ns_f1 &lt;- summary(f1)\n\n       Generalized pairwise comparisons with 4 prioritized endpoints\n\n - statistic       : net treatment benefit  (delta: endpoint specific, Delta: global) \n - null hypothesis : Delta == 0 \n - confidence level: 0.95 \n - inference       : H-projection of order 1 after atanh transformation \n - treatment groups: 1 (treatment) vs. 0 (control) \n - neutral pairs   : re-analyzed using lower priority endpoints\n - results\n endpoint total(%) favorable(%) unfavorable(%) neutral(%) uninf(%)  delta\n    death   100.00        33.20          10.20      56.60        0 0.2300\n       jf    56.60        15.30          10.03      31.27        0 0.0527\n  success    31.27         9.86           4.12      17.29        0 0.0574\n      qol    17.29        10.92           6.37       0.00        0 0.0455\n  Delta CI [2.5% ; 97.5%]    p.value    \n 0.2300    [0.106;0.3469] 0.00032703 ***\n 0.2827   [0.1355;0.4177] 0.00022230 ***\n 0.3401   [0.1882;0.4761] 2.2250e-05 ***\n 0.3856     [0.2326;0.52] 2.6463e-06 ***\n\n\nIn the results, the totals, wins, loss and ties are presented as percentages rather than counts. For example, the total column effectively represents the proportion of pairs that carry over from one outcome to the next; for death there were 5660 pairs that carried over to joint function there were 3127 pairs that carried over to the treatment success outcome and so on. These can be visualised as:\n\nd_fig &lt;- data.table(s_f1)\nd_fig &lt;- d_fig[, 1:5]\nnames(d_fig) &lt;- c(\n  \"endpoint\", \"total\", \"wins\", \"losses\", \"tie\"\n)\nd_fig &lt;- melt(d_fig, id.vars = \"endpoint\")\nd_fig &lt;- d_fig[variable != \"total\"]\n\nggplot(d_fig, aes(x = endpoint, y = value, fill = variable)) +\n  geom_bar(stat='identity') +\n  scale_fill_discrete(\"\") +\n  scale_y_continuous(\"Percentage\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: Contribution by each endpoint\n\n\n\n\n\nInference can be conducted on all pairs for all outcomes by indicating that the hierarchical perspective is not required:\n\nf2 &lt;- BuyseTest(ff1, hierarchical = FALSE, data = d, trace = 0)\nsummary(f2)\n\n       Generalized pairwise comparisons with 4 endpoints\n\n - statistic       : net treatment benefit  (delta: endpoint specific, Delta: global) \n - null hypothesis : Delta == 0 \n - confidence level: 0.95 \n - inference       : H-projection of order 1 after atanh transformation \n - treatment groups: 1 (treatment) vs. 0 (control) \n - results\n endpoint weight total(%) favorable(%) unfavorable(%) neutral(%) uninf(%)\n    death   0.25      100        33.20          10.20      56.60        0\n       jf   0.25      100        26.64          17.64      55.72        0\n  success   0.25      100        30.80          13.80      55.40        0\n      qol   0.25      100        64.02          35.98       0.00        0\n  delta  Delta CI [2.5% ; 97.5%]    p.value    \n 0.2300 0.0575   [0.0272;0.0877] 0.00020121 ***\n 0.0900 0.0800    [0.037;0.1227] 0.00026848 ***\n 0.1700 0.1225   [0.0699;0.1744] 5.4724e-06 ***\n 0.2804 0.1926    [0.1296;0.254] 3.3839e-09 ***\n\n\n\n\nReferences\n\n\n1. Chamberlain J. Desirability of outcome ranking for status epilepticus. Research Methods in Neurology. 2023;101.\n\n\n2. De Schryver M. A tutorial on probabilistic index models: Regression models for the effect size p(Y1 &gt; Y2). American Psychological Association. 2019;24.\n\n\n3. Follmann D. Regression analysis based on pairwise ordering of patients’ clinical histories. Statistics in Medicine. 2002;21.\n\n\n4. Buyse M. Generalized pairwise comparisons of prioritized outcomes in the two-sample problem. Statistics in Medicine. 2010;29."
  },
  {
    "objectID": "notebooks/custom-distribution-stan.html",
    "href": "notebooks/custom-distribution-stan.html",
    "title": "User-defined Probability Distributions in Stan",
    "section": "",
    "text": "Overview\nSome of this material can be found in the stan user guide and this is solely to serve as a reference in my own words.\nTo implement, you just need to provide a function to increment the total log-probability appropriately.\n\n\n\n\n\n\nNote\n\n\n\nWhen a function with the name ending in *_lpdf* or *_lpmf* is defined, the stan compiler automatically makes a *_lupdf* or lupmf version. Only normalised custom distributions are permitted.\n\n\nAssume that we want to create a custom distribution per:\n\\[\n\\begin{aligned}\nf(x) &= (1-a) x^{-a}\n\\end{aligned}\n\\]\ndefined for \\(a \\in [0,1]\\) and \\(x \\in [0,1]\\) with cdf:\n\\[\n\\begin{aligned}\nF_x &= x^{a-1}\n\\end{aligned}\n\\]\nWe can generate draws from this distribution using the inverse cdf method:\n\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.9.0\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/mark/.cmdstan/cmdstan-2.36.0\n\n\n- CmdStan version: 2.36.0\n\nf_x &lt;- function(x, a){\n  if(a &lt; 0 | a &gt; 1) stop(\"only defined for a in [0,1]\")\n  if(any(x &lt; 0 | x &gt; 1)) stop(\"only defined for x in [0,1]\")\n  (1-a) * x ^ -a\n}\nF_x &lt;- function(x, a){\n  if(a &lt; 0 | a &gt; 1) stop(\"only defined for a in [0,1]\")\n  if(any(x &lt; 0 | x &gt; 1)) stop(\"only defined for x in [0,1]\")\n  x^(1-a)\n}\nF_inv_x &lt;- function(u, a){\n  if(a &lt; 0 | a &gt; 1) stop(\"only defined for a in [0,1]\")\n  if(any(u &lt; 0 | u &gt; 1)) stop(\"only defined for x in [0,1]\")\n  u ^ (1 / (1-a))\n}\n\na &lt;- 0.35\nx &lt;- seq(0, 1, len = 1000)\nd_fig &lt;- data.table(x = x, y = f_x(x, a))\nd_sim &lt;- data.table(\n  y_sim = F_inv_x(runif(1e6), a)\n)\n\nggplot(d_fig, aes(x = x, y = y)) +\n  geom_histogram(data = d_sim, aes(x = y_sim, y = ..density..),\n               inherit.aes = F, fill = 1, alpha = 0.2,\n               binwidth = density(d_sim$y_sim)$bw) + \n  geom_line() +\n  theme_bw()\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nfunctions {\n  real custom_lpdf(vector x, real alpha) {\n    \n    int n_x = num_elements(x);\n    vector[n_x] lpdf;\n    for(i in 1:n_x){\n      \n      lpdf[i] = log1m(alpha) - alpha * log(x[i]);\n    }  \n    return sum(lpdf);\n  }\n}\ndata {\n  int N;\n  vector[N] y;\n}\n\nparameters {\n  real&lt;lower=0, upper = 1&gt; a;\n}\nmodel {\n  target += exponential_lpdf(a | 1);\n  target += custom_lpdf(y | a);   \n}\n\n\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/custom-dist-1.stan\")\n\nld = list(\n  N = 1000, \n  y = d_sim$y_sim[1:1000]\n)\n\n\nf1 &lt;- m1$sample(\n  ld, iter_warmup = 1000, iter_sampling = 1000,\n  parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = F,\n  max_treedepth = 10)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.2 seconds.\n\nf1$summary(variables = c(\"a\"))\n\n# A tibble: 1 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 a        0.369  0.369 0.0198 0.0203 0.337 0.400  1.01     377.     610.\n\npost &lt;- data.table(f1$draws(variables = \"a\", format = \"matrix\"))\nhist(post$a)\n\n\n\n\n\n\n\n\n\n\nReferences"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "maj-biostat.github.io",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nThursday 13 Mar 2025\n\n\nStan implementation of AFT survival model with log-logistic event times\n\n\n12 min\n\n\n\n\nThursday 13 Mar 2025\n\n\nPower analysis by simulation\n\n\n4 min\n\n\n\n\nWednesday 20 Nov 2024\n\n\nDiscrete parameters in stan\n\n\n5 min\n\n\n\n\nWednesday 13 Nov 2024\n\n\nCOVID-19 Analyses\n\n\n5 min\n\n\n\n\nMonday 21 Oct 2024\n\n\nCONSORT using graphviz\n\n\n4 min\n\n\n\n\nThursday 10 Oct 2024\n\n\nPrincipal Stratification\n\n\n15 min\n\n\n\n\nFriday 4 Oct 2024\n\n\nMultinomial regression\n\n\n10 min\n\n\n\n\nThursday 3 Oct 2024\n\n\nLimitations of per-protocol analyses\n\n\n7 min\n\n\n\n\nMonday 30 Sep 2024\n\n\nProbabilistic Index Models\n\n\n4 min\n\n\n\n\nFriday 27 Sep 2024\n\n\nDesirability of Outcome Ranking (DOOR)\n\n\n12 min\n\n\n\n\nWednesday 25 Sep 2024\n\n\nRobust errors for estimating proportion\n\n\n4 min\n\n\n\n\nWednesday 25 Sep 2024\n\n\nMann-Whitney U\n\n\n14 min\n\n\n\n\nWednesday 25 Sep 2024\n\n\nUser-defined Probability Distributions in Stan\n\n\n2 min\n\n\n\n\nWednesday 18 Sep 2024\n\n\nRandom walk priors\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Biostatistician working mostly in the area of Bayesian adaptive clinical trials using R and stan. The site contains various posts/reminders on topics that are relevant to my day-to-day work."
  },
  {
    "objectID": "about.html#repository-status",
    "href": "about.html#repository-status",
    "title": "About",
    "section": "Repository status",
    "text": "Repository status\n\nlibrary(git2r)\nrepo &lt;- git2r::repository(path = \".\")\nsummary(repo)\n\nLocal:    main /Users/mark/Documents/project/website/src/maj-biostat.github.io\nRemote:   main @ origin (https://github.com/maj-biostat/maj-biostat.github.io)\nHead:     [93a7c89] 2025-07-04: Add draft page ideas, not for rendering yet\n\nBranches:         1\nTags:             0\nCommits:         71\nContributors:     2\nStashes:          0\nIgnored files:    3\nUntracked files: 30\nUnstaged files:   0\nStaged files:     0\n\nLatest commits:\n[93a7c89] 2025-07-04: Add draft page ideas, not for rendering yet\n[564aa93] 2025-07-04: Render site and add minor updates\n[6e5a738] 2025-05-16: Update site\n[b2609f1] 2025-05-16: Fix another error\n[14a8712] 2025-05-16: Fix error in example"
  },
  {
    "objectID": "notebooks/covid-19-vaccine-efficacy.html",
    "href": "notebooks/covid-19-vaccine-efficacy.html",
    "title": "COVID-19 Analyses",
    "section": "",
    "text": "library(data.table)\nlibrary(ggplot2)\nlibrary(gt)\n\nVaccine efficacy is defined as:\n\\[\nVE = 1\\left(1 - \\frac{\\pi_v}{\\pi_p} \\right)\n\\]\nwhere \\(\\pi_v\\), \\(\\pi_p\\) are the probabilities of infection under the vaccine and placebo (i.e. the risk of infection without vaccination), generally expressed as a percentage.\nThe lower the risk of infection on receipt of the vaccine relative to the risk of infection under placebo, the higher the VE with VE capped at 100%, \\(\\lim_{\\pi_v \\to 0} VE = 1\\). For the FDA to approve a vaccine, the VE must be at least 0.3 (30%).\nIf we can reasonably assume that \\(\\pi_v \\le \\pi_p\\) (implying the risk of infection is going to be lower on receipt of the vaccine than on placebo) then the lower bound of VE will be constrained to zero. However, when we cannot make this assumption and \\(\\pi_v &gt; \\pi_p\\) then VE may become negative.\nThe VE is given various representations via the risk-ratio, incidence rate ratio and hazard ratio, specifically:\n\\[\n\\begin{aligned}\nVE &= 1 - RR = 1 - \\frac{c_v/N_v}{c_p/N_p} \\\\\nVE &= 1 - IRR = 1 - \\frac{c_v/T_v}{c_p/T_p}  \\\\\nVE &= 1 - HR = 1 - \\frac{\\lambda_v}{\\lambda_p}\n\\end{aligned}\n\\]\nwhere \\(c_.\\) are the cases and \\(N_.\\) are the total number of participants, \\(T_.\\) is person-time and \\(\\lambda_.\\) are hazard rates.\nIf we take the second representation and set \\(r = T_v/T_p\\) as the ratio of person-time, then a few manipulations give us:\n\\[\n\\frac{c_v}{c_p} = r (1 - VE)\n\\]\nfrom which we can form a case proportion - the proportion of the total cases in the vaccinated group:\n\\[\n\\begin{aligned}\n\\theta &= \\frac{r (1 - VE)}{1 + r(1 - VE)} \\\\\n  &= \\frac{  \\frac{c_v}{c_p}   }{    \\frac{c_v}{c_p} + 1 } \\\\\n  &= \\frac{  \\frac{c_v}{c_p}   }{    \\frac{c_v}{c_p} + \\frac{c_p}{c_p} } \\\\\n  &= \\frac{  c_v   }{   c_v + c_p  }\n\\end{aligned}\n\\]\nwhich is a proportion between 0 and 1, and thus might plausibly admit to a beta distributional assumption.\nSome more manipulations gives a reparameterisation of VE:\n\\[\n\\begin{aligned}\n\\frac{1 + r(1 - VE)}{r (1 - VE)} &= \\frac{1}{\\theta} \\\\\n\\frac{1}{r (1 - VE)} + 1 &= \\frac{1}{\\theta} \\\\\n                         &= \\frac{1-\\theta}{\\theta}\\\\\nr (1 - VE)   &=  \\frac{\\theta}{1-\\theta} \\\\\n1 - VE &= \\frac{\\theta}{r(1-\\theta)} \\\\\nVE - 1 &= - \\frac{\\theta}{r(1-\\theta)} \\\\\n       &=   \\frac{\\theta}{r(\\theta-1)} \\\\\nVE  &= 1 + \\frac{\\theta}{r(\\theta-1)}\n\\end{aligned}\n\\]\nand therefore we can do inference on \\(\\theta\\) and derive VE from that.\nAccording to this paper by Polack, there were 8 cases in the vaccine group and 162 in the placebo group used in the Pfizer vaccine efficacy analysis. Additionally, the follow up times were 2.214 and 2.222 in 1000 person-years in the vaccine and placebo groups respectively.\nTherefore, we can compute the posterior on the case proportion as \\(\\theta \\sim \\text{Beta}(8 + 0.700102, 162 + 1)\\) and derive VE. By simulation:\n\nN_ptcl &lt;- 1e6\n\n# person time followup ratio\nr &lt;- 2.214 / 2.222\n\n# prior\ntheta_pri &lt;- rbeta(N_ptcl, 0.700102, 1)\nmu_theta_pri &lt;- 0.700102 / (1 + 0.700102)\nVE_pri &lt;- 1 + theta_pri / (1 * (theta_pri - 1))\n\n# posterior\ntheta &lt;- rbeta(N_ptcl, 8 + 0.700102, 162 + 1)\nVE &lt;- 1 + theta / (r * (theta - 1))\n\n\n# results\nd_theta &lt;- data.table(par = \"theta\", prior = theta_pri, posterior = theta)\nd_VE &lt;- data.table(par = \"VE\", prior = VE_pri, posterior = VE)\n\nFigure 1 shows the prior and posterior on the case proportion, Figure 2 shows the implied VE and Table 1 gives a summary of the posterior on the VE parameter.\nThe prior on \\(\\theta\\), the case proportion, implies a prior probability that VE is greater than 0.3 equal to 0.54. However, the posterior probability that VE is greater than 0.3 is 1.00.\n\n\n\n\n\n\n\n\nFigure 1: Prior and posterior density on case proportion\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Prior and posterior density on VE\n\n\n\n\n\n\n\n\nTable 1: Summary of posterior on VE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior (mean, 95% CrI)\n\n\n\nmean\n2.5%\n97.5%\n\n\n\n\nVE\n0.946\n0.903\n0.976"
  },
  {
    "objectID": "notebooks/discrete-parameters-stan.html",
    "href": "notebooks/discrete-parameters-stan.html",
    "title": "Discrete parameters in stan",
    "section": "",
    "text": "library(cmdstanr)\n\nThis is cmdstanr version 0.9.0\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/mark/.cmdstan/cmdstan-2.36.0\n\n\n- CmdStan version: 2.36.0\n\nlibrary(data.table)\nlibrary(ggplot2)\n\nHamiltonian Monte Carlo is usually used to estimate continuous parameters. However, sometimes we are interested in discrete parameters.\nHMC runs on log-probabilities. Therefore, as long as we can increment the log-probability associated with our model (whatever the model is), we can code it up, even if it uses discrete parameters. For models with discrete parameter, we have to ‘marginalise out’ the discrete parameter(s) and increment the log-density by the marginal density.\nBen Lambert has an example:\nConsider a series of \\(K\\) experiments where we get someone flips a coin a fixed number of times, \\(n\\), for each experiment, but we are never told \\(n\\). The individual uses the same coin across all the experiments and the probability of heads remains constant \\(\\text{Pr}(X = \\text{heads}) = \\theta\\). We define \\(X_k\\) as the number of heads obtained in each experiment \\(k\\) yielding \\((X_1, X_2, \\dots X_K)\\). Again, both \\(\\theta\\) and \\(n\\) are unknown to us; \\(\\theta\\) is continuous, but \\(n\\) is discrete.\nWe want to be able to make inference on both \\(\\theta\\) and \\(n\\), i.e. talk about the probability intervals for theta and the probability that \\(n\\) takes on certain values.\nAssume that we know \\(K = 10\\) experiments were run and the following data was observed \\(X_k = (2, 4, 3, 3, 3, 3, 3, 3, 4, 4)\\).\nWe adopt independent priors on \\(\\theta\\) and \\(n\\):\n\\[\n\\begin{aligned}\nn &\\sim \\text{Discrete-Unif}(5, 8) \\\\\n\\theta &\\sim \\text{Unif}(0, 1)\n\\end{aligned}\n\\]\nWe can write down the joint posterior of \\(\\theta\\) and \\(n\\), i.e. \\(\\text{Pr}(\\theta, n | X)\\) and then marginalise out the discrete parameter, \\(n\\). Once we have an expression that excludes the \\(n\\), then we can get stan to use that expression to conduct the sampling we want it to do. We have:\n\\[\n\\begin{aligned}\n\\text{Pr}(\\theta | X) &= \\sum_{n=5}^8 \\text{Pr}(\\theta, n | X)\n\\end{aligned}\n\\]\nStan runs on the log probability so we need to think in those terms:\n\\[\n\\begin{aligned}\n\\log(\\text{Pr}(\\theta | X)) &= \\log \\left(\\sum_{n=5}^8 \\text{Pr}(\\theta, n | X) \\right) \\\\\n&= \\log \\left(\\sum_{n=5}^8 \\exp( \\log( \\text{Pr}(\\theta, n | X))) \\right) \\\\\n\\end{aligned}\n\\]\nwhere the second line is to ensure we are dealing with log probabilities for both terms.\nIn stan, the above can be achieved in a mathematically stable way via the log-sum-exp function.\n\\[\n\\begin{aligned}\n\\log(\\text{Pr}(\\theta | X)) &= \\text{log\\_sum\\_exp}_{n=5}^8 \\left( \\log( \\text{Pr}(\\theta, n | X) ) \\right)\n\\end{aligned}\n\\]\nUnfortunately, we do not have \\(\\text{Pr}(\\theta, n | X)\\) but we can use Bayes rule to determine what it is:\n\\[\n\\begin{aligned}\n\\text{Pr}(\\theta, n | X) &\\propto \\text{Pr}(X | \\theta, n) \\text{Pr}(\\theta, n) \\\\\n&= \\text{Pr}(X | \\theta, n) \\text{Pr}(n)\\text{Pr}(\\theta)\n\\end{aligned}\n\\]\nwhere the second line comes from the fact that we use independent priors. Taking logs, we get:\n\\[\n\\begin{aligned}\n\\log(\\text{Pr}(\\theta, n | X)) &\\propto \\log(\\text{Pr}(X | \\theta, n)) +  \\log(\\text{Pr}(n)) + \\log(\\text{Pr}(\\theta)) \\\\\n\\end{aligned}\n\\]\n\nThe first term on the RHS is the likelihood for which we use the binomial distribution.\nThe second term is the log of the discrete uniform distribution that we defined earlier. For a given \\(n \\in \\{ 5,6,7,8 \\}\\) this is \\(\\log(1/4)\\) as we assume each option has equal probability.\nFinally, the third term is standard uniform. However, given that the third term does not contain \\(n\\), we do not actually need to include it in the expression for the joint distribution (although we do still include it in the model block as a standard uniform).\n\nThe above allows us to estimate models with discrete parameters by marginalising them out of the joint density. But, what if we want to do inference on the discrete parameters?\nAnswer; write down the unnormalised density of \\(n\\) conditional on \\(X\\) and estimate via MCMC:\n\\[\n\\begin{aligned}\nq(n | x) \\approx \\frac{1}{B} \\sum_{i = 1}^B q(n, \\theta_i | X)\n\\end{aligned}\n\\]\nwhere \\(B\\) is the number of MCMC samples and \\(\\theta_i\\) are the posterior samples. Essentially, this is averaging over \\(\\theta_i\\).\nTo get the normalised version, we need to form a simplex (sum of elements is 1 and elements are non-negative and less than 1) across the four possible values for \\(n\\) giving probabilities for \\(n = 5\\), \\(n = 6\\), \\(n = 7\\) and \\(n = 8\\). We can obtain this from:\n\\[\n\\begin{aligned}\np(n | x) &\\approx \\frac{q(n|X)}{\\sum_{n = 5}^8 q(n, | X)} \\\\\n&= \\frac{\\exp( \\log( q(n | x) ) )}{ \\exp( \\text{log\\_sum\\_exp} (\\log( q(n | X))) )}\n\\end{aligned}\n\\]\nand in stan, we would write this as:\n\\[\n\\begin{aligned}\np(n | x) = \\exp\\left[  \\log(q(n | X)) - \\text{log\\_sum\\_exp}(\\log( q(n | X)) ) \\right]\n\\end{aligned}\n\\]\nAn implementation for the above discussion is shown below:\n\n\n\n\ndata {\n  // num expt\n  int&lt;lower=0&gt; K;\n  array[K] int X;\n}\ntransformed data{\n  array[4] int n;\n  // these are the permissible values of n, \n  // i.e. 5, 6, 7, 8\n  for(i in 1:4){\n    n[i] = 4 + i;\n  }\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; theta;\n}\ntransformed parameters{\n  // unnormalised density\n  vector[4] lq;\n  for(i in 1:4){\n    // record the unnormalised density for every possible value \n    // of n\n    \n    // log pmf for the array of values in X conditional on a given n[i]\n    // and the parameter theta PLUS the prior on the given n, which is \n    // a discrete uniform, i.e. for each n, the prior is 0.25\n    lq[i] = binomial_lpmf(X | n[i], theta) + log(0.25);\n  }\n}\nmodel {\n  target += uniform_lpdf(theta | 0, 1);\n  \n  // marginalise out the troublesome n\n  target += log_sum_exp(lq);\n  \n}\ngenerated quantities{\n  // probability of n given X, i.e. the distribution of n | x\n  vector[4] p_n_X;\n  p_n_X = exp(lq - log_sum_exp(lq));\n  \n}\n\n\nRunning the model with the assumed data gives us our parameter estimates for both \\(\\theta\\) and \\(n\\).\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/discrete-param-1.stan\")\n\nld &lt;- list(\n  K = 10, X = c(2, 4, 3, 3, 3, 3, 3, 3, 4, 4)\n)\n\nf1 &lt;- m1$sample(\n    ld, iter_warmup = 1000, iter_sampling = 1000,\n    parallel_chains = 4, chains = 4, refresh = 0, show_exceptions = F,\n    max_treedepth = 10)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nf1$summary()\n\n# A tibble: 10 × 10\n   variable     mean     median    sd      mad       q5     q95  rhat ess_bulk\n   &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lp__     -14.9    -14.7      0.668 0.334    -1.63e+1 -14.4    1.00    1557.\n 2 theta      0.580    0.591    0.102 0.105     3.93e-1   0.729  1.00    1241.\n 3 lq[1]    -14.6    -13.7      2.15  0.757    -1.93e+1 -13.1    1.00    1584.\n 4 lq[2]    -15.7    -15.0      1.90  1.33     -1.96e+1 -14.0    1.00    1828.\n 5 lq[3]    -18.4    -17.2      3.84  3.30     -2.61e+1 -14.6    1.00    1509.\n 6 lq[4]    -22.2    -20.9      6.16  6.40     -3.39e+1 -15.1    1.00    1274.\n 7 p_n_X[1]   0.610    0.734    0.361 0.359     6.43e-3   0.994  1.00    1241.\n 8 p_n_X[2]   0.213    0.170    0.175 0.210     5.23e-3   0.507  1.00    1800.\n 9 p_n_X[3]   0.108    0.0166   0.151 0.0246    6.04e-6   0.432  1.00    1351.\n10 p_n_X[4]   0.0692   0.000389 0.161 0.000577  2.32e-9   0.484  1.00    1241.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nObviously, the above is somewhat contrived. We gnerally do not know the bounds on the discrete parameters. For example, how did we know that the bounds of \\(n\\) were 5 and 8? How would we have modified the model to account for observing a 6 in the data?"
  },
  {
    "objectID": "notebooks/graphviz.html",
    "href": "notebooks/graphviz.html",
    "title": "CONSORT using graphviz",
    "section": "",
    "text": "Graphviz (short for Graph Visualization Software) is a package of open-source tools initiated by AT&T Labs Research for drawing graphs specified in DOT language scripts. Three main kinds of objects appear in the DOT language: graphs, nodes, edges. The outer most (main) graph can be directed (digraph) or undirected (graph). Within the main graph, a subgraph defines a subset of nodes and edges.\nThe following provides some basic reference examples, mostly taken from [1].\n\nlibrary(DiagrammeR)\n\n\nInitial Examples\nThe specification of a simple graph is shown in Figure 1. Nodes are created when the name appears. Edges are created when nodes are joined by the edge operator -&gt;.\nUnlike other types of quarto blocks, it is necessary to enter the control elements in the form //| echo: true rather than the usual #| echo: FALSE.\n\ndigraph G {\n  main -&gt; parse -&gt; execute;\n  main -&gt; init;\n  main -&gt; cleanup;\n  execute -&gt; make_string;\n  execute -&gt; printf\n  init -&gt; make_string;\n  main -&gt; printf;\n  execute -&gt; compare;\n}\n\n\n\n\n\n\n\nG\n\n\n\nmain\n\nmain\n\n\n\nparse\n\nparse\n\n\n\nmain-&gt;parse\n\n\n\n\n\ninit\n\ninit\n\n\n\nmain-&gt;init\n\n\n\n\n\ncleanup\n\ncleanup\n\n\n\nmain-&gt;cleanup\n\n\n\n\n\nprintf\n\nprintf\n\n\n\nmain-&gt;printf\n\n\n\n\n\nexecute\n\nexecute\n\n\n\nparse-&gt;execute\n\n\n\n\n\nmake_string\n\nmake_string\n\n\n\nexecute-&gt;make_string\n\n\n\n\n\nexecute-&gt;printf\n\n\n\n\n\ncompare\n\ncompare\n\n\n\nexecute-&gt;compare\n\n\n\n\n\ninit-&gt;make_string\n\n\n\n\n\n\n\n\nFigure 1: Example dot graph\n\n\n\n\n\nNodes and edges can be given attributes to control their representation and placement. The size of the graph can be controlled with the size operator. If the drawing is too large, it is scaled uniformly as necessary to fit.\n\n\n\n\n\n\n\n\nG\n\n\n\nmain\n\nmain\n\n\n\nparse\n\nparse\n\n\n\nmain-&gt;parse\n\n\n\n\n\ninit\n\ninit\n\n\n\nmain-&gt;init\n\n\n\n\n\ncleanup\n\ncleanup\n\n\n\nmain-&gt;cleanup\n\n\n\n\n\nprintf\n\nprintf\n\n\n\nmain-&gt;printf\n\n\n100 times\n\n\n\nexecute\n\nexecute\n\n\n\nparse-&gt;execute\n\n\n\n\n\nmake_string\n\nmake a\nstring\n\n\n\nexecute-&gt;make_string\n\n\n\n\n\nexecute-&gt;printf\n\n\n\n\n\ncompare\n\ncompare\n\n\n\nexecute-&gt;compare\n\n\n\n\n\ninit-&gt;make_string\n\n\n\n\n\n\n\n\nFigure 2: Example dot graph with attributes and comments\n\n\n\n\n\n\n\nAttributes\nSome common node shapes available include box, circle, record and plaintext. A complete list can be found at [www.graphviz.org/doc/info/shapes.html]. The modifier fixedsize=true ensures that the node’s actual size is aligned with width and height. Linework can be doubled up using peripheries=2 and orientation directed with orientation measured in degrees.\nWhile the default name of a node is its name, this can be modified with the label attribute.\n\ndigraph structs {\n  size =\"2,2\"; ratio=fill;\nnode [shape=record,fontsize=5];\n  struct1 [shape=record,label=\"&lt;f0&gt; left|&lt;f1&gt; mid\\ dle|&lt;f2&gt; right\"];\n  struct2 [shape=record,label=\"&lt;f0&gt; one|&lt;f1&gt; two\"];\n  struct3 [shape=record,label=\"hello\\nworld |{ b |{c|&lt;here&gt; d|e}| f}| g | h\"];\n  struct1 -&gt; struct2;\n  struct1 -&gt; struct3;\n}\n\n\n\n\n\n\n\nstructs\n\n\n\nstruct1\n\nleft\n\nmid dle\n\nright\n\n\n\nstruct2\n\none\n\ntwo\n\n\n\nstruct1-&gt;struct2\n\n\n\n\n\nstruct3\n\nhello\nworld\n\nb\n\nc\n\nd\n\ne\n\nf\n\ng\n\nh\n\n\n\nstruct1-&gt;struct3\n\n\n\n\n\n\n\n\nFigure 3: Example of dot graph with nested attributes and comments\n\n\n\n\n\nTo specify the minimum distance between two adjacent nodes, use nodesep and to set the minimum vertical space use ranksep.\n\n\nR Integration\nThere are a number of ways to incorporate DOT within R. One approach is to use the DiagrammeR package, specifically the grViz() function. This allows you to encode the graph using DOT notation but also pass in arguments as defined in R. Figure 4 provides a simple example.\n\nN &lt;- 24\nN_A &lt;- 12\nN_A_excl &lt;- 2\nN_A_analy &lt;- 10\nN_B &lt;- 12\nN_B_analy &lt;- 12\n\nfig &lt;- grViz(\"digraph consort {\n\n  node [fontname = Helvetica, shape = box, width = 1];\n  \n  enrolled [label = 'Enrolled \\n(n = @@1)'];  \n  allocA [label = 'Assigned A \\n(n = @@2)'];\n  allocB [label = 'Assigned B \\n(n = @@3)'];\n  \n  { rank = same; allocA allocB } \n  \n  exclA[label = 'Excl A \\n(n = @@6)'];\n  \n  analyA [label = 'Incl in analysis \\n(n = @@4)'];\n  analyB [label = 'Incl in analysis \\n(n = @@5)'];\n  \n  { rank = same; analyA analyB } \n  \n  blank1[label = '', width = 0.01, height = 0.01];\n\n  blank2[label = '', width = 0.01, height = 0.01];\n  blank3[label = '', width = 0.01, height = 0.01];\n  blank4[label = '', width = 0.01, height = 0.01];\n  /* all have same vertical position */\n  { rank = same; blank2 blank3 blank4 }\n\n  blank2 -&gt; blank3 [arrowhead=none, minlen = 5];\n  blank3 -&gt; blank4 [arrowhead=none, minlen = 5];\n  \n  blank5[label = '', width = 0.01, height = 0.01]; \n  { rank = same; blank5 exclA } \n\n  enrolled -&gt; blank1[dir = none];\n  blank1 -&gt; blank3[arrowhead=none];\n  blank2 -&gt; allocA;\n  blank4 -&gt; allocB;\n  allocA -&gt; blank5[arrowhead=none];\n  blank5 -&gt; exclA;\n  blank5 -&gt; analyA;\n  allocB -&gt; analyB;\n  \n}\n\n  [1]: N\n  [2]: N_A\n  [3]: N_B\n  [4]: N_A_analy\n  [5]: N_B_analy\n  [6]: N_A_excl\n\")\nfig\n\n\n\n\n\n\n\nFigure 4: Example of dot graph that uses arguments defined in R\n\n\n\n\n\n\n\n\n\nReferences\n\n1. Gansner E. Dot user’s manual - drawing graphs with dot. 2015."
  },
  {
    "objectID": "notebooks/log-logistic-aft-in-stan.html",
    "href": "notebooks/log-logistic-aft-in-stan.html",
    "title": "Stan implementation of AFT survival model with log-logistic event times",
    "section": "",
    "text": "Code\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(gt)\nsuppressPackageStartupMessages(library(survival))\nsuppressPackageStartupMessages(library(flexsurv))\noptions(scipen=999)\n\n\nIn a parametric AFT model, the effect of covariates is to speed or slow down time.\n\\[\n\\begin{aligned}\n\\log(T) = X\\gamma + \\text{error}\n\\end{aligned}\n\\]\nWhere:\n\n\\(T\\) is the survival time\n\\(X\\gamma\\) is the linear predictor\n\nand the error term is made up of a scale parameter \\(\\sigma\\) and a random variable \\(W\\) with a specific distribution. In the usual setup, we observe the event/censoring indicator and the associated event or censoring time \\(C\\), with the event and censoring process assumed to be independent.\nFor the log-logistic model, the residual distribution is determined by the shape parameter. If \\(\\log(T) = X\\gamma + \\sigma W\\) where \\(W\\) has a logistic distribution then \\(T\\) follows a log-logistic distribution with scale parameter \\(\\alpha = \\exp(X\\gamma)\\) and shape parameter \\(\\beta = 1/\\sigma\\). For further reference see section 2.2.4 in [1], chapter 13 of [2], chapter 6 of [3] (possibly the clearest explanation) and [4].\nThe hazard function associated with log-logistic event times is hump-shaped, a bit like the log normal case but with longer tails. It initially increases, reaches a maximum and then decreases toward 0 as lifetimes become larger and larger. Definitions for the density function can be found in the stan docs: https://mc-stan.org/docs/functions-reference/positive_continuous_distributions.html#log-logistic-distribution and in the flexsurv help file, see ?flexsurv::dllogis. Unlike lognormal, the log-logistic has a closed form hazard function.\n\\[\n\\begin{aligned}\nf = \\frac{(\\beta/\\alpha)(t/\\alpha)^{\\beta-1}}{(1 + (t/\\alpha)^\\beta)^2}\n\\end{aligned}\n\\]\nwith shape parameter \\(\\beta &gt;0\\) and scale parameter \\(\\alpha &gt;0\\). The cumulative distribution function is\n\\[\n\\begin{aligned}\nF = \\frac{1}{1 + (t/\\alpha)^{-\\beta}}\n\\end{aligned}\n\\]\nthe survival function is \\(1 - F\\):\n\\[\n\\begin{aligned}\nS &= 1 - \\frac{1}{1 + (t/\\alpha)^{-\\beta}} \\\\\n  &= \\frac{1 + (t/\\alpha)^{-\\beta}}{1 + (t/\\alpha)^{-\\beta}} - \\frac{1}{1 + (t/\\alpha)^{-\\beta}} \\\\\n  &= \\frac{(t/\\alpha)^{-\\beta}}{1 + (t/\\alpha)^{-\\beta}} \\\\\n  &= \\frac{1}{(t/\\alpha)^\\beta (1 + (t/\\alpha)^{-\\beta})} \\\\\n  &= \\frac{1}{1 + (t/\\alpha)^{\\beta}} \\\\\n\\end{aligned}\n\\]\nthe hazard function is \\(f/S\\):\n\\[\n\\begin{aligned}\nh &= \\frac{\\frac{(\\beta/\\alpha)(t/\\alpha)^{\\beta-1}}{(1 + (t/\\alpha)^\\beta)^2}}{\\frac{1}{1 + (t/\\alpha)^{\\beta}}} \\\\\n  &= \\frac{(\\beta/\\alpha)(t/\\alpha)^{\\beta-1} (1 + (t/\\alpha)^{\\beta}) }{(1 + (t/\\alpha)^\\beta)^2} \\\\\n  &= \\frac{(\\beta/\\alpha)(t/\\alpha)^{\\beta-1}}{1 + (t/\\alpha)^\\beta} \\quad \\text{cancelling similar terms}\n\\end{aligned}\n\\]\nSay that we want to simulate data where there was 10% cumulative incidence by day 360, e.g. in the first 360 days of life about 10% of infants will experience a medical attendance for RSV, a respiratory illness.\nWe want \\(S(360) = \\frac{1}{1 + (360/\\alpha)^\\beta} = \\pi = 0.9\\). As \\(\\alpha\\) is the scale parameter, which is usually modelled as a linear function of parameters (treatment effects etc), assume that \\(\\beta\\) is known and solve for \\(\\alpha\\)\n\\[\n\\begin{aligned}\n\\alpha = \\frac{360}{((1/\\pi) - 1)^{1/\\beta}}\n\\end{aligned}\n\\]\nFor example, say \\(\\beta = 2\\), this implies \\(\\alpha = \\frac{360}{((1/0.9) - 1)^{1/2}} = 1080\\) and gives the functional forms as shown below. Setting the survival probability to 0.5 and solving for time gives the median survival time under these parameters, i.e \\(1080 \\times 1^{1/\\beta} = 1080\\). Obviously, these values are just for demonstration and can be calibrated to subject matter expertise as necessary for simulating trial designs etc.\n\nCode\n# log-logistic parameters\n# shape parameter\nb &lt;- 2\n# scale\na &lt;- 360 / ( (1/0.9)-1 )^(1/b)    \n\n# Create a data.table with days from 1 to 360\ndt &lt;- data.table(day = 1:1080)\n\n# Compute the survival function S(t) = 1 / (1 + (t/a)^b)\ndt[, survival := 1 / (1 + (day / a)^b)]\n\n# Compute the density f(t) = (gamma/alpha) * (t/alpha)^(gamma-1) / [1 + (t/alpha)^gamma]^2\ndt[, density := (b / a) * (day / a)^(b - 1) / (1 + (day / a)^b)^2]\n\n# Compute the hazard function h(t) = f(t) / S(t)\ndt[, hazard := density / survival]\n\n# Plot the survival curve\nggplot(dt, aes(x = day, y = survival)) +\n  geom_line(color = \"blue\", lwd = 0.4) +\n  geom_vline(xintercept = 360, lwd = 0.2) +\n  labs(title = \"Survival Curve for RSV (Log-Logistic Model)\",\n       x = \"Day of Life\", y = \"Survival Probability S(t)\") +\n  scale_y_continuous(\"Survival S(t)\", limits = c(0.5, 1), seq(0.5, 1, by = 0.1)) +\n  theme_minimal()\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nCode\n# Plot the hazard function\nggplot(dt, aes(x = day, y = hazard)) +\n  geom_line(color = \"darkgreen\", lwd = 0.4) +\n  geom_vline(xintercept = 360, lwd = 0.2) +\n  labs(title = \"Hazard Function for RSV (Log-Logistic Model)\",\n       x = \"Day of Life\", y = \"Hazard h(t)\") +\n  theme_minimal()\n# Plot the density function\nggplot(dt, aes(x = day, y = density)) +\n  geom_line(color = \"red\", lwd = 0.4) +\n  geom_vline(xintercept = 360, lwd = 0.2) +\n  labs(title = \"Density Function for RSV (Log-Logistic Model)\",\n       x = \"Day of Life\", y = \"Density f(t)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn an AFT model, which is appropriate when we are more concerned with direct assessment of event times (AFT is also a way to work around non-proportional hazards) rather than a focus on instantaneous risk, the scale parameter is allowed to vary with the covariates, such as:\n\\[\n\\begin{aligned}\n\\alpha_i &= \\exp(\\mu_i) \\\\\n\\mu &= \\gamma_0 + \\gamma_1 x_1 + \\dots\n\\end{aligned}\n\\]\nThe density for observation \\(i\\) is then:\n\\[\n\\begin{aligned}\nf(t_i) &= \\frac{\\beta}{\\exp(\\mu_i)} \\left( \\frac{t_i}{\\exp(\\mu_i)}\\right)^{\\beta - 1} \\left[  1 + \\left(\\frac{t_i}{\\exp(\\mu_i)} \\right)^\\beta \\right]^{-2}\n\\end{aligned}\n\\]\ntaking logs of this gives the log-likelihood for observation \\(i\\):\n\\[\n\\begin{aligned}\n\\log f(t_i) &= \\log(\\beta) - \\mu_i + (\\beta - 1)\\left[ \\log(t_i) - \\mu_i \\right]   -2 \\log \\left( 1 + (t_i/\\exp(\\mu_i))^\\beta    \\right)\n\\end{aligned}\n\\]\nfor the right censored records, the survival function is used:\n\\[\n\\begin{aligned}\nS = \\frac{1}{1 + (t_i/\\exp(\\mu_i))^\\beta}\n\\end{aligned}\n\\]\ntaking logs:\n\\[\n\\begin{aligned}\n\\log S &= 0 + \\log \\left[ 1 + \\left( \\frac{t_i}{\\exp(\\mu_i)} \\right)^\\beta        \\right]\n\\end{aligned}\n\\]\nImplement stan model:\n\n\n// Log-logistic AFT model\ndata {\n  int&lt;lower=0&gt; N;             // Number of observations\n  int&lt;lower=0&gt; P;             // Number of predictors\n  matrix[N, P] X;             // Predictor matrix X[, 1] is intercept\n  vector&lt;lower=0&gt;[N] y;       // Observed survival times\n  vector&lt;lower=0, upper=1&gt;[N] event;  // Event indicator (1=event, 0=censored)\n  \n  int N_pred;\n  vector[N_pred] t_surv;    // time to predict survival at\n  \n  // prior\n  vector[2] mu0_gamma;   // e.g. c(5, 0)\n  vector[2] sd0_gamma;   // e.g. c(2, 2)\n  \n  real rho_shape;        // e.g. 0.5 to 1\n  \n}\n\nparameters {\n  vector[P] gamma;             // Regression coefficients for scale\n  real&lt;lower=0&gt; shape;        // Shape parameter (b in the formula)\n}\n\ntransformed parameters {\n  // Location parameter (log-scale)\n  vector[N] mu;      \n  \n  mu = X * gamma;\n}\n\nmodel {\n  // Priors - arbitrary at the moment\n  target += normal_lpdf(gamma[1] | 5, 3);\n  target += normal_lpdf(gamma[2] | 0, 2);\n  target += exponential_lpdf(shape | 0.5);\n  \n  // Likelihood\n  for (i in 1:N) {\n    if (event[i] == 1) {\n      // For observed events, use the log-logistic density\n      target += log(shape) - mu[i] + (shape - 1) * (log(y[i]) - mu[i]) - \n                2 * log1p(pow(y[i] / exp(mu[i]), shape));\n    } else {\n      // For censored observations, use the log survival function\n      target += -log1p(pow(y[i] / exp(mu[i]), shape));\n    }\n  }\n}\n\ngenerated quantities {\n  vector[N_pred] surv0;\n  vector[N_pred] surv1;\n  \n  real scale0;\n  real scale1;\n  \n  // these equate to the median survival time\n  scale0 = exp(gamma[1]);\n  scale1 = exp(gamma[1] + gamma[2]);\n  \n  for(i in 1:N_pred){\n    surv0[i] =  1 / (1 + pow(t_surv[i]/exp(gamma[1]),  shape));\n    surv1[i] =  1 / (1 + pow(t_surv[i]/exp(gamma[1] + gamma[2]),  shape));\n  }\n  \n  \n}\n\n\nRunning the model with the assumed data gives parameter estimates.\n\n\nCode\nmod_01 &lt;- cmdstanr::cmdstan_model(\"stan/log-logistic-aft-01.stan\")\n\n# Simulation parameters\nN &lt;- 2000 \n\ngamma_true &lt;- c(log(1080), 1)  \n# True shape parameter\nshape_true &lt;- 2  \n\n# Simulate covariates\nsimulate_data &lt;- function(\n    N = 2000,\n    gamma_true = c(log(1080), 0.3)  ,\n    shape_true = 2  ,\n    t_cen = 360\n    ) {\n  \n  d &lt;- data.table(\n    trt = rep(0:1, length  = N)\n  )\n  \n  d[trt == 0, scale := exp(gamma_true[1])]\n  d[trt == 1, scale := exp(gamma_true[1] + gamma_true[2])]\n  \n  d[, t_evt := flexsurv::rllogis(.N, shape = shape_true, scale = scale)]\n  d[, evt := as.numeric(t_evt &lt;= t_cen)]\n  d[evt == 1, t_evt_obs := t_evt]\n  # Assume everyone is followed up to 360 days. If you are doing this \n  # incrementally then you need to consider the minimum of the censoring\n  # or follow up time.\n  d[evt == 0, t_evt_obs := t_cen]\n\n  # d[, .N, keyby = .(trt, evt)]\n  d\n}\n\n# Simulate data\nd_sim &lt;- simulate_data()\n# d_sim[, .N, keyby = .(trt, evt)]\n\n# Prepare data for Stan\nld &lt;- list(\n  N = nrow(d_sim),\n  P = 2,\n  X = cbind(1, d_sim$trt),\n  y = d_sim$t_evt_obs,\n  event = d_sim$evt,\n  N_pred = 361,\n  t_surv = 0:360,\n  mu0_gamma = c(5, 0),\n  sd0_gamma = c(2, 2),\n  rho_shape = 0.5\n)\n\n\n# d_stan_gamma &lt;- function(x, a, b){\n#   (b^a / gamma(a)) * x^(a-1) * exp(-b * x)\n# }\n# \n# xx &lt;- seq(0, 100, len = 1000)\n# yy &lt;- d_stan_gamma(xx, 1, 0.1)\n# plot(xx, yy, type = \"l\")\n\n\n# Fit the Stan model - sink to remove the noise\n# snk &lt;- capture.output(\n  m1 &lt;- mod_01$sample(\n      ld, iter_warmup = 1000, iter_sampling = 1000,\n      parallel_chains = 4, chains = 4, refresh = 0, show_exceptions = F,\n      max_treedepth = 10)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 4.1 seconds.\nChain 2 finished in 4.2 seconds.\nChain 3 finished in 4.2 seconds.\nChain 4 finished in 4.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4.2 seconds.\nTotal execution time: 4.5 seconds.\n\n\nCode\n# )\n\n\nExtract the parameters that we are interested in:\n\n\nCode\nd_post &lt;- data.table(\n  m1$draws(variables = c(\"gamma\", \"shape\", \"scale0\", \"scale1\"),\n           format = \"matrix\")\n)\nd_post &lt;- melt(d_post, measure.vars = names(d_post))\n\nd_tbl &lt;- d_post[, .(\n  mu = mean(value),\n  q_025 = quantile(value, prob = 0.025),\n  q_975 = quantile(value, prob = 0.975)\n), keyby = variable]\n\ngt(d_tbl) |&gt;\n  cols_align(\n    align = \"left\",\n    columns = c(\"variable\")\n    ) |&gt;\n  fmt_number(columns = everything(), decimals = 2) |&gt;\n  tab_options(\n      table.font.size = \"80%\"\n    ) \n\n\n\n\nTable 1: Parameters estimated by mcmc\n\n\n\n\n\n\n\n\n\nvariable\nmu\nq_025\nq_975\n\n\n\n\ngamma[1]\n7.28\n7.04\n7.55\n\n\ngamma[2]\n0.34\n0.14\n0.55\n\n\nshape\n1.63\n1.39\n1.88\n\n\nscale0\n1,461.68\n1,136.68\n1,894.03\n\n\nscale1\n2,066.93\n1,533.60\n2,845.09\n\n\n\n\n\n\n\n\n\n\nPathfinder variational inference is a bit quicker but coarser approximation:\n\n\nCode\nm3 &lt;- mod_01$pathfinder(\n  ld, \n  init = function() {list(\n    gamma = c(runif(1, 5, 10), runif(1, -1, 1)),\n    shape = runif(1, 0, 4)\n    )},\n  num_paths=4, single_path_draws=250,\n  history_size=50, max_lbfgs_iters=100,\n  refresh = 0, draws = 1000)\n\n\nFinished in  0.6 seconds.\n\n\n\n\nCode\nd_post &lt;- data.table(\n  m3$draws(variables = c(\"gamma\", \"shape\", \"scale0\", \"scale1\"),\n           format = \"matrix\")\n)\nd_post &lt;- melt(d_post, measure.vars = names(d_post))\n\nd_tbl &lt;- d_post[, .(\n  mu = mean(value),\n  q_025 = quantile(value, prob = 0.025),\n  q_975 = quantile(value, prob = 0.975)\n), keyby = variable]\n\ngt(d_tbl) |&gt;\n  cols_align(\n    align = \"left\",\n    columns = c(\"variable\")\n    ) |&gt;\n  fmt_number(columns = everything(), decimals = 2) |&gt;\n  tab_options(\n      table.font.size = \"80%\"\n    ) \n\n\n\n\nTable 2: Parameters estimated by pathfinder algorithm\n\n\n\n\n\n\n\n\n\nvariable\nmu\nq_025\nq_975\n\n\n\n\ngamma[1]\n7.28\n7.06\n7.51\n\n\ngamma[2]\n0.34\n0.13\n0.58\n\n\nshape\n1.63\n1.41\n1.88\n\n\nscale0\n1,456.40\n1,161.42\n1,828.20\n\n\nscale1\n2,059.20\n1,540.54\n2,803.00\n\n\n\n\n\n\n\n\n\n\nAnd by way of a sanity check, run the equivalent model using the flexsurv package.\n\n\nCode\nm2 &lt;- flexsurvreg(Surv(t_evt_obs, evt) ~ trt, data = d_sim, dist = \"llogis\")\nprint(m2)\n\n\nCall:\nflexsurvreg(formula = Surv(t_evt_obs, evt) ~ trt, data = d_sim, \n    dist = \"llogis\")\n\nEstimates: \n       data mean  est       L95%      U95%      se        exp(est)  L95%    \nshape        NA      1.644     1.407     1.921     0.131        NA        NA\nscale        NA   1417.725  1100.460  1826.457   183.241        NA        NA\ntrt       0.500      0.335     0.122     0.548     0.109     1.398     1.129\n       U95%    \nshape        NA\nscale        NA\ntrt       1.729\n\nN = 2000,  Events: 152,  Censored: 1848\nTotal time at risk: 698264.7\nLog-likelihood = -1413.449, df = 3\nAIC = 2832.898\n\n\nOther options for model implementation might be through brms with a custom family (if that is possible).\n\n\nCode\nd_sim[, censored := 1-evt]\n# brms is backwords - \n# for cens, specify 0 to indicate no censoring and 1 to indicate right censoring\n\nbrms::make_stancode(t_evt_obs | cens(censored) ~ trt, data = d_sim, family = lognormal())\nbrms::make_stancode(t_evt_obs | cens(censored) ~ trt, data = d_sim, family = weibull())\n\n\nExponentiating the \\(\\gamma_2\\) parameter gives the acceleration factor associated with the treatment effect. For example, if \\(\\gamma_2 &gt; 0\\) we can say that change from the control to treatment arm is associated with survival times being multiplied by a factor of \\(\\exp(\\gamma_2)\\), indicating prolonged survival/delayed events. Similarly, if \\(\\gamma_2 &lt; 0\\) we have a reduction in survival (the time to event speeds up).\nIn a log-logistic AFT model with the current parameterisation, the median survival time for an individual with covariates \\(x_i\\) is given by \\(\\exp( \\gamma x_i') = \\alpha_i = \\text{scale}_\\text{i}\\). Median survival is a common measure used to contrast groups.\nProduce a posterior for the survival curve:\n\n\nCode\nd_post &lt;- data.table(m1$draws(variables = c(\"surv0\", \"surv1\"), format = \"matrix\"))\nd_post &lt;- melt(d_post, measure.vars = names(d_post))\nd_post[variable %like% \"surv0\", trt := 0]\nd_post[variable %like% \"surv1\", trt := 1]\nd_fig &lt;- copy(d_post)\n\nd_fig[, x := gsub(\".*\\\\[\", \"\", variable)]\nd_fig[, x := gsub(\"\\\\]\", \"\", x)]\nd_fig[, x := as.numeric(x)]\n\nd_fig &lt;- d_fig[\n  , .(mu = mean(value),\n      q_025 = quantile(value, prob = 0.025),\n      q_975 = quantile(value, prob = 0.975)), keyby = .(trt, x)]\nd_fig[, trt := factor(trt, levels = 0:1, labels = c(\"ctl\", \"trt\"))]\n\nggplot(d_fig, aes(x = x, y = mu, group = trt, col = trt)) +\n  geom_ribbon(aes(ymin = q_025, ymax = q_975, fill = trt), alpha = 0.1, col = NA) +\n  geom_line() + \n  scale_y_continuous(limits = c(0.7, 1), breaks = seq(0.7, 1, by = 0.1)) +\n  scale_color_discrete(\"\") +\n  scale_fill_discrete(\"\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nPosterior on the median survival time, the time at which 50% of the cohort has experienced the occurrence of the event, e.g. a medical attendance for RSV ARI.\n\n\nCode\nd_post &lt;- data.table(m1$draws(variables = c(\"scale0\", \"scale1\"), format = \"matrix\"))\n\nd_fig &lt;- melt(d_post, measure.vars = names(d_post), variable.name = \"trt\")\n\nd_fig[, trt := factor(trt, levels = c(\"scale0\", \"scale1\"), labels = c(\"ctl\", \"trt\"))]\n\nggplot(d_fig, aes(x = value, group = trt, col = trt)) +\n  geom_density() +\n  scale_x_continuous(\"Median survival time\") +\n  scale_color_discrete(\"Treatment\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nEstimate the posterior for restricted mean survival time (by treatment group) by integrating under the survival curve function for each draw from the posterior.\nThe RMST can be interpreted as the average survival time (i.e. time without the event, here being occurrence of RSV) during a defined time period ranging from time 0 to a specific follow-up time point.\n\n\nCode\n# Define the function to integrate\nintegrand_1 &lt;- function(\n    x, mu, shape) {\n  \n  a = exp(mu)\n  S = 1 / (1 + (x/a)^shape)\n  S\n  \n}\n\n\nd_post &lt;- data.table(m1$draws(variables = c(\"gamma\", \"shape\"), format = \"matrix\"))\nnames(d_post) &lt;- c(paste0(\"gamma\", 1:2), \"shape\")\ni &lt;- 1\nm_rmst &lt;- matrix(NA, ncol = 2, nrow = nrow(d_post))\nfor(i in 1:nrow(d_post)){\n  m_rmst[i, 1] &lt;- integrate(\n    integrand_1, lower = 0, upper = 360,\n    mu = d_post$gamma1[i], \n    shape = d_post$shape[i])$value  \n  m_rmst[i, 2] &lt;- integrate(\n    integrand_1, lower = 0, upper = 360,\n    mu = d_post$gamma1[i] + d_post$gamma2[i], \n    shape = d_post$shape[i])$value  \n}\n\n\n\nd_rmst &lt;- data.table(m_rmst)\nnames(d_rmst) &lt;- paste0(0:1)\n\nrmst_diff &lt;- d_rmst$`1` -  d_rmst$`0`\n\n\nd_fig &lt;- melt(d_rmst, measure.vars = names(d_rmst), variable.name = \"trt\")\n\nd_fig[, trt := factor(trt, levels = 0:1, labels = c(\"ctl\", \"trt\"))]\n\nggplot(d_fig, aes(x = value, group = trt, col = trt)) +\n  geom_density() +\n  scale_x_continuous(\"RMST\") +\n  scale_color_discrete(\"Treatment\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\nnames(d_rmst) &lt;- paste0(\"rmst\", 0:1)\nd_rmst[, diff := rmst1 - rmst0]\n\n\nFrom here, we could evaluate differences in the RMST between groups considering what level of improvement in the mean survival to 360 days would be warranted to decide on adopting the treatment over the control.\nNote that I have assumed a log-logistic parametric assumption here, primarily because I wanted something similar to a log-normal but more tractable. Other distributional assumptions might be more suitable. For example, if the data have a peaked hazard followed by a decline, then standard log-logistic or generalized log-logistic may work well. However, if the hazard function is more complex (e.g. bathtub shape, non-monotonic tail behavior etc), the Generalized F or Burr distributions might be better. Weibull or gamma models are simpler if only a monotonically increasing or decreasing hazard is required. All of these are reasonably straight forward to code up.\n\n\n\n\n\n\nReferences\n\n1. Sun J. Statistical analysis of interval censored failure time data. Springer; 2006.\n\n\n2. Christensen R. Bayesian ideas and data analysis. CRC Press; 2011.\n\n\n3. Collett D. Modelling survival data in medical research. CRC Press; 2015.\n\n\n4. Cleves M. Introduction to survival analysis using stata. Stata Press; 2010."
  },
  {
    "objectID": "notebooks/multinomial-regression.html",
    "href": "notebooks/multinomial-regression.html",
    "title": "Multinomial regression",
    "section": "",
    "text": "The multinomial distribution is a generalisation of the binomial distribution, [1]. The binomial counts the successes in a fixed number of trials each classed as success or failure. The multinomial distribution keeps track of trials whose outcomes have multiple categories, e.g. agree, neutral, disagree.\nWe have \\(N\\) objects, each is independently placed into one of \\(K\\) categories. An object is placed in category \\(k\\) with probability \\(p_k\\) where \\(\\sum_{k=1}^K p_k = 1\\) and \\(p_k \\ge 0\\) for all \\(k\\). If we let \\(Y_1\\) be the count of category 1 objects, \\(Y_2\\) be the count for category 2 etc so that \\(Y_1 + \\dots + Y_K = N\\) then \\(\\mathbf{Y} = (Y_1, \\dots, Y_K)\\) is said to have a multinomial distribution with parameters \\(N\\) and \\(\\mathbf{p} = (p_1, \\dots, p_K)\\). This can be written as \\(\\mathbf{Y} \\sim \\text{Mult}_K(N, \\mathbf{p})\\) where the \\(\\mathbf{Y}\\) is referred to as a random vector as it is a vector of random variables.\nIf \\(\\mathbf{Y} \\sim \\text{Mult}_K(N, \\mathbf{p})\\) then the joint PMF is:\n\\[\n\\begin{aligned}\n\\text{Pr}(Y_1 = y_1, \\dots, Y_K = y_K) &= \\frac{N!}{y_1!y_2!\\dots y_K!} p_1^{y_1}p_2^{y_2}\\dots  p_K^{y_K} \\\\\n&= \\begin{pmatrix}\nN \\\\\ny_1, \\dots, y_K\n\\end{pmatrix} \\prod_{k=1}^K p_k^{y_k}\n\\end{aligned}\n\\]\nThe marginal distribution of a multinomial are all binomial with \\(Y_k \\sim Bin(N, p_k)\\).\nLumping categories together will form multinomial distribution with the revised \\(K^\\prime\\) representing the new (smaller) set of categories and both counts and probabilities for the lumped groups being additive."
  },
  {
    "objectID": "notebooks/multinomial-regression.html#setup",
    "href": "notebooks/multinomial-regression.html#setup",
    "title": "Multinomial regression",
    "section": "Setup",
    "text": "Setup\nAssume that for a city, it is well known that the dominant modes of travel (to work) are walk/bike, public transport and car with a distribution provided in Table 1. Note that these are mutually exclusive and exhaustive of the possible modes of transport. Now say we want to investigate ways to move people away from cars. Interventions may span from city wide information on the benefits of using public transport, to financial discounts to restrictions on parking or taxes. Assume we want to evaluate whether an information mail-out versus financial discounts on public transport achieves greater transition to public transport.\n\n\n\n\nTable 1: Distribution of dominant mode of transport used to get to work\n\n\n\n\n\n\n\n\n\n\n\n\n\nMode of transport\nProportion\n\n\n\n\nWalk/bike\n0.07\n\n\nPublic transport\n0.33\n\n\nCar\n0.60\n\n\n\n\n\n\n\n\n\n\n\nget_data &lt;- function(\n    N = 250,\n    p0 = c(0.07, 0.33, 0.6),\n    p1 = c(0.10, 0.50, 0.4)){\n  \n  d &lt;- data.table(id = 1:N)\n  d[, trt := rep(0:1, each = N/2)]\n  \n  d[trt == 0, y := sample(seq_along(p0), .N, replace = T, prob = p0)]\n  d[trt == 1, y := sample(seq_along(p1), .N, replace = T, prob = p1)]\n  \n  d\n}\n\nWe take a random sample of adult working residents from the city. The sample is randomised 1:1 to a monthly email lasting for 3-months that details the benefit of using public transport versus a 3-month discount for the all public transport networks within the city. At 3-months, the sample cohort are surveyed to determine their dominant mode of transport in the last month."
  },
  {
    "objectID": "notebooks/multinomial-regression.html#contingency-table",
    "href": "notebooks/multinomial-regression.html#contingency-table",
    "title": "Multinomial regression",
    "section": "Contingency table",
    "text": "Contingency table\nOne approach to the analysis is via a contingency table posing the research hypothesis Do dominant modes of transport differ under the two interventions? The Chi-squared test of independence is run on the observed counts of each cell as follows:\n\\[\n\\begin{aligned}\nX^2 &= \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(O_{ij} - e_{ij})^2}{e_{ij}}\n\\end{aligned}\n\\]\nwhere the expected counts are based on the row and column totals (i.e. assuming independence across groups):\n\\[\n\\begin{aligned}\ne_{ij} = \\frac{O_{i.} O_{.j}}{O_{..}}\n\\end{aligned}\n\\]\nwhere \\(O_{i.}\\) denotes the row totals, \\(O_{.j}\\) the column totals and \\(O_{..}\\) denotes the grand total.\nIf the null hypothesis is true then the observed and the expected frequencies will be similar to one another and \\(\\chi^2\\) will be small. If \\(\\chi^2\\) is too large then the null would be rejected suggesting that the treatment and dominant modes of travel are not independent. Specifically, we would reject the null if the test statistic was found to be greater than or equal to a \\(\\chi^2\\) distribution that has \\((r-1)(c-1)\\) degrees of freedom, i.e. reject null if \\(X^2 \\ge \\chi^2_{(r-1)(c-1);\\alpha}\\) where \\(\\alpha\\) is the significance level.\nA sketch of the approach is shown below.\n\nset.seed(1)\nd &lt;- get_data()\n\n# observed\nm_obs &lt;- as.matrix(dcast(d[, .N, keyby = .(trt, y)],\n                      trt ~ y, value.var = \"N\"))\n\ntot_cols &lt;- colSums(m_obs[, 2:4])\ntot_rows &lt;- rowSums(m_obs[, 2:4])\n\n# expected\nm_e &lt;- rbind(\n  tot_cols  * tot_rows[1] / sum(tot_rows),\n  tot_cols  * tot_rows[2] / sum(tot_rows)\n)\n\n# chisqured statistic vs critical value\n\nv_stats &lt;- c(\n  chisq_obs = sum(  ((m_obs[, -1] - m_e)^2)/m_e ) ,\n  chisq_crit = qchisq(0.95, 2 * 1)\n)\n\nround(c(\n  v_stats, \n  p_value = pchisq(v_stats[1], 2 * 1, lower.tail = F)), 3)\n\n        chisq_obs        chisq_crit p_value.chisq_obs \n            6.053             5.991             0.048 \n\n\nSince the observed test statistic is greater than the critical value, we would reject the null in this case. The above can be automatically with the built-in function:\n\nres &lt;- chisq.test(m_obs[, -1])\nres\n\n\n    Pearson's Chi-squared test\n\ndata:  m_obs[, -1]\nX-squared = 6.0528, df = 2, p-value = 0.04849"
  },
  {
    "objectID": "notebooks/multinomial-regression.html#multi-logit-regression",
    "href": "notebooks/multinomial-regression.html#multi-logit-regression",
    "title": "Multinomial regression",
    "section": "Multi-logit regression",
    "text": "Multi-logit regression\nTo implement the multi-logit regression a long format is usually adopted so that we have each unit \\(i\\) has an outcome \\(Y_i\\) equal to one of the possible categories.\nA simple sum-to-zero implementation is shown below.\n\n\ndata {\n  // Each unit has an outcome corresponding to one of K categories\n  int K;\n  // Number of units\n  int N;\n  // Number of terms in linear predictor (all lp have the \n  // same terms here). Includes intercept.\n  int D;\n  // Outcome variable (one of the categories)\n  array[N] int y;\n  // design matrix\n  matrix[N, D] x;\n}\nparameters {\n  vector[K-1] a_raw;\n  vector[K-1] b_raw;\n}\ntransformed parameters {\n  //            k1  k2  k3 etc\n  // intercept  -   -   -\n  //         x  -   -   -\n  matrix[D, K] b;\n  \n  b[1, ] = append_row(a_raw, -sum(a_raw))';\n  b[2, ] = append_row(b_raw, -sum(b_raw))';\n}\nmodel {\n  matrix[N, K] x_beta = x * b;\n\n  to_vector(a_raw) ~ normal(0, 5);\n  to_vector(b_raw) ~ normal(0, 5);\n\n  for (n in 1:N) {\n    y[n] ~ categorical_logit(x_beta[n]');\n  }\n}\ngenerated quantities{\n  \n  matrix[K, 2] p;\n  \n  vector[K] l0 = to_vector(b[1, ]);\n  vector[K] l1 = to_vector(b[1, ] + b[2, ]);\n  \n  p[, 1] = softmax(l0);\n  p[, 2] = softmax(l1);\n  \n}\n\n\nAnd a fixed pivot implementation is\n\n\ndata {\n  // Each unit has an outcome corresponding to one of K categories\n  int K;\n  // Number of units\n  int N;\n  // Number of terms in linear predictor (all lp have the \n  // same terms here). Includes intercept.\n  int D;\n  // Outcome variable (one of the categories)\n  array[N] int y;\n  // design matrix\n  matrix[N, D] x;\n}\nparameters {\n  //            k1  k2  k3 etc\n  // intercept  -   -   -\n  //         x  -   -   -\n  matrix[D, K-1] b_raw;\n}\ntransformed parameters {\n  //            k1  k2  k3 etc\n  // intercept  -   -   -\n  //         x  -   -   -\n  matrix[D, K] b;\n  \n  b[, 1:(K-1)] = b_raw;\n  b[, K] = rep_vector(0.0, D);\n}\nmodel {\n  matrix[N, K] x_beta = x * b;\n\n  to_vector(b_raw) ~ normal(0, 5);\n\n  for (n in 1:N) {\n    y[n] ~ categorical_logit(x_beta[n]');\n  }\n}\ngenerated quantities{\n  \n  matrix[K, 2] p;\n  \n  vector[K] l0 = to_vector(b[1, ]);\n  vector[K] l1 = to_vector(b[1, ] + b[2, ]);\n  \n  p[, 1] = softmax(l0);\n  p[, 2] = softmax(l1);\n  \n}\n\n\nFit both models to the data\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/multi-logit-01.stan\")\nm2 &lt;- cmdstanr::cmdstan_model(\"stan/multi-logit-02.stan\")\n\nld &lt;- list(\n  N = nrow(d),\n  K = length(unique(d$y)),\n  y = d$y,\n  D = 2,\n  x = cbind(1, d$trt)\n)\n\nf1 &lt;- m1$sample(data = ld, chains = 1, iter_sampling = 1000, refresh = 0)\nf2 &lt;- m2$sample(data = ld, chains = 1, iter_sampling = 1000, refresh = 0)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.8 seconds.\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.6 seconds.\n\n\nBoth approaches faithfully recover the empirical proportions for the two treatment groups as shown in Table 2. The pivot model gives the most direct path to interpreting the model parameters. However, note that the models are structurally different and cannot be considered completely equivalent since different prior weights enter the models.\n\n\n\n\nTable 2: Observed and modeled distribution for mode of transport\n\n\n\n\n\n\n\n\n\ntrt\ny\nN\ntot\np\nmu_f1\nmu_f2\n\n\n\n\n0\n1\n7\n125\n0.056\n0.055\n0.058\n\n\n0\n2\n46\n125\n0.368\n0.368\n0.369\n\n\n0\n3\n72\n125\n0.576\n0.576\n0.573\n\n\n1\n1\n12\n125\n0.096\n0.097\n0.097\n\n\n1\n2\n60\n125\n0.480\n0.479\n0.478\n\n\n1\n3\n53\n125\n0.424\n0.425\n0.425\n\n\n\n\n\n\n\n\n\n\nThe parameter estimates from the linear predictors from the two models are summarised below. The terms (treatment effects) of interest are those associated with the second model f2, specifically, b[2,1] and b[2,2]. These suggesting that (1) units in the treatment group are more likely to have walking/bike than car as their dominant mode of transport and (2) units in the treatment group are more likely to have public transport than car as their dominant mode of transport.\n\nf1$summary(variables = \"b\")\n\n# A tibble: 6 × 10\n  variable    mean  median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b[1,1]   -1.46   -1.44   0.269 0.267 -1.92  -1.04   1.01     181.     170.\n2 b[2,1]    0.408   0.405  0.348 0.347 -0.162  0.965  1.01     177.     200.\n3 b[1,2]    0.503   0.494  0.172 0.172  0.227  0.794  1.02     180.     319.\n4 b[2,2]    0.0822  0.0948 0.225 0.213 -0.312  0.444  1.01     164.     285.\n5 b[1,3]    0.955   0.949  0.154 0.149  0.707  1.22   1.00     338.     314.\n6 b[2,3]   -0.490  -0.487  0.211 0.216 -0.810 -0.134  1.00     453.     512.\n\nf2$summary(variables = \"b\")\n\n# A tibble: 6 × 10\n  variable   mean median    sd   mad       q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b[1,1]   -2.35  -2.32  0.374 0.375 -3.00    -1.79   1.00     499.     403.\n2 b[2,1]    0.834  0.835 0.492 0.492  0.00937  1.63   1.00     456.     588.\n3 b[1,2]   -0.445 -0.443 0.185 0.174 -0.759   -0.145  1.01     643.     527.\n4 b[2,2]    0.563  0.561 0.260 0.260  0.133    0.982  1.00     619.     540.\n5 b[1,3]    0      0     0     0      0        0     NA         NA       NA \n6 b[2,3]    0      0     0     0      0        0     NA         NA       NA \n\n\nThese results are consistent with the results from the contingency table analysis in that they suggest the public transport discount intervention may increase the likelihood of people taking this form of transport to work.\nUnlike the contingency analysis, the the multi-logit approach offers the usual regression benefit of being able to adjust for covariates that may be relevant."
  },
  {
    "objectID": "notebooks/multinomial-regression.html#poisson-regression",
    "href": "notebooks/multinomial-regression.html#poisson-regression",
    "title": "Multinomial regression",
    "section": "Poisson regression",
    "text": "Poisson regression\nMultinomial logit models can also be fit using an equivalent log-linear model and a series of Poisson likelihoods. This is mathematically equivalent to the multinomial and computationally the Poisson approach can be easier, see McElreath2020 p365.\nAn implementation is shown below\n\n\ndata {\n  // Each unit has an outcome corresponding to one of K categories\n  int K;\n  // Number of units\n  int N;\n  // Outcome variable (one of the categories)\n  // y1= walk/bike, y2=public transport, y3=car\n  array[N] int y1;\n  array[N] int y2;\n  array[N] int y3;\n  // design matrix\n  vector[N] x;\n}\nparameters {\n  vector[K] a;\n  vector[K] b;\n}\nmodel {\n\n  to_vector(a) ~ normal(0, 5);\n  to_vector(b) ~ normal(0, 5);\n  \n  target += poisson_log_lpmf(y1 | a[1] + x * b[1]);\n  target += poisson_log_lpmf(y2 | a[2] + x * b[2]);\n  target += poisson_log_lpmf(y3 | a[3] + x * b[3]);\n  \n}\ngenerated quantities{\n  \n  matrix[K, 2] p;\n  \n  p[1, 1] = exp(a[1]) * inv( exp(a[1]) + exp(a[2]) + exp(a[3]));\n  p[2, 1] = exp(a[2]) * inv( exp(a[1]) + exp(a[2]) + exp(a[3]));\n  p[3, 1] = exp(a[3]) * inv( exp(a[1]) + exp(a[2]) + exp(a[3]));\n  \n  p[1, 2] = exp(a[1] + b[1]) * inv( exp(a[1] + b[1]) + exp(a[2] + b[2]) + exp(a[3] + b[3]));\n  p[2, 2] = exp(a[2] + b[2]) * inv( exp(a[1] + b[1]) + exp(a[2] + b[2]) + exp(a[3] + b[3]));\n  p[3, 2] = exp(a[3] + b[3]) * inv( exp(a[1] + b[1]) + exp(a[2] + b[2]) + exp(a[3] + b[3]));\n\n}\n\n\nIn order to fit the model, it is necessary to first create an indicator variable for the occurrence of each category.\n\n# need to create a binary indicator for each category:\nd[, y1 := ifelse(y == 1, 1, 0)]\nd[, y2 := ifelse(y == 2, 1, 0)]\nd[, y3 := ifelse(y == 3, 1, 0)]\n\nm3 &lt;- cmdstanr::cmdstan_model(\"stan/poisson-01.stan\")\n\nld &lt;- list(\n  N = nrow(d),\n  K = length(unique(d$y)),\n  y1 = d$y1, \n  y2 = d$y2,\n  y3 = d$y3,\n  x = d$trt\n)\n\nf3 &lt;- m3$sample(data = ld, chains = 1, iter_sampling = 1000, refresh = 0)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.4 seconds.\n\nf3$summary(variables = \"b\")\n\n# A tibble: 3 × 10\n  variable   mean median    sd   mad      q5     q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b[1]      0.568  0.567 0.509 0.526 -0.259   1.43    1.00     800.     576.\n2 b[2]      0.272  0.266 0.188 0.187 -0.0329  0.581   1.00     719.     701.\n3 b[3]     -0.318 -0.318 0.178 0.179 -0.630  -0.0241  1.00     792.     837.\n\n\nThe combined set of results are shown below, again the empirical proportions for each group are captured by the poisson implementation of the model.\n\n\n\n\nTable 3: Observed and modeled distribution for mode of transport (multi-logit and poisson)\n\n\n\n\n\n\n\n\n\ntrt\ny\nN\ntot\np\nmu_f1\nmu_f2\nmu_f3\n\n\n\n\n0\n1\n7\n125\n0.056\n0.055\n0.058\n0.056\n\n\n0\n2\n46\n125\n0.368\n0.368\n0.369\n0.367\n\n\n0\n3\n72\n125\n0.576\n0.576\n0.573\n0.577\n\n\n1\n1\n12\n125\n0.096\n0.097\n0.097\n0.096\n\n\n1\n2\n60\n125\n0.480\n0.479\n0.478\n0.482\n\n\n1\n3\n53\n125\n0.424\n0.425\n0.425\n0.422"
  },
  {
    "objectID": "notebooks/multinomial-regression.html#extensions",
    "href": "notebooks/multinomial-regression.html#extensions",
    "title": "Multinomial regression",
    "section": "Extensions",
    "text": "Extensions\nOne of the natural extensions of multinomial regression is the use of cluster level effects that are correlated across categories. For more detail, see [4]."
  },
  {
    "objectID": "notebooks/multinomial-regression.html#footnotes",
    "href": "notebooks/multinomial-regression.html#footnotes",
    "title": "Multinomial regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that if we have three categories and assign category 3 as the reference then even though our logits are in terms of \\(\\log(p_1/p_3)\\) and \\(\\log(p_2/p_3)\\), we can still compute a logit for \\(\\log(p_1/p_2)\\) by subtracting the logit for \\(\\log(p_2/p_3)\\) from that for \\(\\log(p_1/p_3)\\).↩︎"
  },
  {
    "objectID": "notebooks/principal-stratification.html",
    "href": "notebooks/principal-stratification.html",
    "title": "Principal Stratification",
    "section": "",
    "text": "We routinely consider pre-treatment adjustment, however, post-treatment confounding is not so often considered. Post-treatment confounding arises as a result of intermediate variables \\(D\\) that lie on the pathway between the treatment \\(Z\\) and the outcome \\(Y\\).\nAdjusting analyses for these post-treatment variables in the same manner as one would do for baseline covariates can lead to biased effect estimates. Non-compliance is one example of an intermediate variable where this can occur.\nTypically, \\(Z\\) (assignment) strongly influence \\(D\\) but \\(D \\ne Z\\) for some (or many) units. Given that units with \\(Z=1, D = d\\) are sometimes not the same as units with \\(Z=0, D = d\\) direct comparisons can be problematic. For example, the units that do not comply when assigned to treatment may not be the same as units that do not comply when assigned to control. This could arise if, say, the units that do not comply when assigned to treatment are sicker than average and cannot accommodate the side effects on the treatment arm, which do not occur so readily on the control arm.\nThe ideas for principal stratification were popularised in the paper by Frangakis and Rubin [1]. They generalised the instrument variable approach. Other references include [2], [3], [4], [5].\nThe key concept is that principal strata can be conceptualised and these are not affected by treatment assignment. That is, the units can be group in terms of joint potential outcomes for \\(D\\) and membership in any stratum only reflects a subjects characteristics, which are defined pre-treatment. Within stratum comparisons are thus well defined causal effects.\nThese strata are defined differently dependent on the setting - take noncompliance as an example. Define \\(D_i(z)\\) as the unit level potential outcome for the intermediate variable given assignment to treatment arm \\(z\\) for \\(z=0,1\\). \\(D_i(z) = 0\\) if subject \\(i\\) received control given assignment to \\(z\\) \\(D_i(z) = 1\\) if subject \\(i\\) received treatment given assignment to \\(z\\)\n\nnever takers = \\(\\{i: D_i(0) = 0, D_i(1) = 0 \\}\\)\ndefiers = \\(\\{i: D_i(0) = 1, D_i(1) = 0 \\}\\)\ncompliers = \\(\\{i: D_i(0) = 0, D_i(1) = 1 \\}\\)\nalways takers = \\(\\{i: D_i(0) = 1, D_i(1) = 1 \\}\\)\n\nthe complier average causal effect is then\n\\[\n\\tau^{\\text{CACE}} = \\mathbb{E}[ Y_i(1) - Y_i(0) | D_i(0) = 0, D_i(1) = 1  ]\n\\]\nWhile this is one set of PS that might be associated with the case of non-compliance, the definitions differ dependent on the situation. Centrally, a PS has the goal of characterising treatment effect heterogeneity across different subpopulations.\n\n\n\n\nTable 1: Composition of principal strata for non-compliance setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssigned (Z)\n\nReceived (D)\n\n\n\nControl (D = 0)\nTreatment (D = 1)\n\n\n\n\nControl (Z = 0)\nnever-takers and compliers\nalways-takers and defiers\n\n\nTreatment (Z = 1)\nnever-takers and defiers\nalways-takers and compliers\n\n\n\n\n\n\n\n\n\n\nThe issue with PS is that what we observe constitute latent mixtures of units rather than specific strata as shown in Table 1. From the above example, of those people who we assign and observe to take the control assignment, we do not know which units are those that would take control regardless of what their assignment was versus which units are the compliers. That is, the specific strata are not observed and therefore additional assumptions are required to estimate principal causal effects.\nThe assumption we absolutely need are unconfounded assignment. That is, the potential outcomes for \\(Y\\) and \\(D\\) are independent of treatment assignment conditional on covariates. This implies that membership in a strata has the same distribution between treatment arms. Without this, there is no way to move forward.\nAdditional assumptions that sharpen identification are monotonicity and exclusion restriction.\nMonotonicity rules out certain components of a mixture that contributes to a given strata. For example, we rule out the possibility of defiers.\nExclusion restriction rules out any direct effect from treatment not mediated through the actual treatment among never-takers and always-takers.\nWhile we can estimate the CACE in the simplest case from the identiability assumptions, we can attempt to resolve the strata in a more comprehensive way by adopting a latent mixture model approach, as explicated by Liu and Li [6].\nFor each unit there exists \\((Y_i(1), Y_i(0), D_i(1), D_i(0), \\mathbf{X_i}, Z_i)\\) but we only observe \\(Y\\) based on the assigned treatment and \\(D\\) under that treatment. Therefore, strata membership, \\(S_i = (D_i(0), D_i(1))\\), is unobserved.\nTo proceed, we need a model for the strata membership \\(S\\) and the outcome \\(Y\\). For example, the \\(S\\) model can take the form of a multinomial model of some form and the outcome might be a GLM. The full likelihood decomposes into an S-model (a principal strata model given the covariates) and a Y-model (an outcome model given the stratum, covariates and treatment). This yields:\n\\[\n\\begin{aligned}\nl(\\theta) \\propto \\prod_{i=1}^n \\left(  \\sum_{s \\in \\mathcal{S}:D_i=D(s,Z_i)}  \\text{Pr}(S_i = s | X_i, \\theta)  \\text{Pr}(Y_i | S_i = s, Z_i, X_i, \\theta) \\right)\n\\end{aligned}\n\\]\nfor the \\(i = 1, \\dots n\\) units in the sample where \\(\\mathcal{S}\\) is the set of all PS and \\(D(s, z)\\) denotes the actual treatment \\(D_i\\) induced by PS \\(S_i = s\\) and assigned treatment \\(Z_i = z\\), i.e. a product of multiple components:\n\\[\n\\begin{aligned}\nl(\\theta) &\\propto \\prod_{i:Z_i=0,D_i=0} (\\pi_{i,c}f_{i,c0} + \\pi_{i,n}f_{i,n0}) \\times \\prod_{i:Z_i=0,D_i=1} (\\pi_{i,a}f_{i,a0} + \\pi_{i,d}f_{i,d0}) \\\\\n\\quad &\\times \\prod_{i:Z_i=1,D_i=0} (\\pi_{i,n}f_{i,n1} + \\pi_{i,d}f_{i,d1}) \\times \\prod_{i:Z_i=1,D_i=1} (\\pi_{i,a}f_{i,a1} + \\pi_{i,c}f_{i,c1})\n\\end{aligned}\n\\]\nwhere \\(f_{i,sz} = \\text{Pr}(Y_i | S_i = s, Z_i, \\mathbf{X}_i, \\theta)\\) and \\(\\pi_{i,s} = \\text{Pr}(S_i = s | \\mathbf{X}_i, \\theta)\\) and where the \\(c,n,a,d\\) denote the compliers, never-takeres, always-takers and defiers (for the noncompliance case considered here).\nFor any given strata, we can write the PCE based on the iterated expectations:\n\\[\n\\begin{aligned}\n\\tau_s = \\mathbb{E}[  \\mathbb{E}[Y_i | Z_i = 1, S_i = s, X_i] | S_i = s] - \\mathbb{E}[  \\mathbb{E}[Y_i | Z_i = 0, S_i = s, X_i] | S_i = s]\n\\end{aligned}\n\\]\nIf we let \\(g_{z,s}(x;\\theta) = \\mathbb{E}[ Y_i | Z_i = z, S_i = s, X_i = x, \\theta]\\) and \\(p_s(x;\\theta) = \\text{Pr}(S_i = s | X_i = x, \\theta)\\) from the Y-model and S-model respectively, then the PCE can be computed from the posterior as:\n\\[\n\\begin{aligned}\n\\hat{\\tau}_s (\\theta) = \\frac{ \\sum_{i=1}^n g_{1,s}(X_i;\\theta) p_s(X_i; \\theta)   }{\\sum_i=1^n  p_s(X_i; \\theta)}  - \\frac{ \\sum_{i=1}^n g_{0,s}(X_i;\\theta) p_s(X_i; \\theta)   }{\\sum_i=1^n  p_s(X_i; \\theta)}\n\\end{aligned}\n\\]\nAssuming that \\(\\theta_k\\) are samples from the posterior distribution for \\(\\theta\\) then these can be plugged into the above to approximate the distribution of \\(\\tau_s\\).\n\nImplementation\nAn implementation of the combined S-model and Y-model is shown below for the case of a binary outcome and a binary treatment with a binary intermediate variable. The model assumes neither monotonicity hence and exclusion restriction.\n\n\ndata {\n    int&lt;lower=0&gt; N; // number of observations\n    int&lt;lower=0&gt; PS; // number of predictors for principal stratum model\n    int&lt;lower=0&gt; PG; // number of predictors for outcome model\n    int&lt;lower=0, upper=1&gt; Z[N]; // treatment arm\n    int&lt;lower=0, upper=1&gt; D[N]; // post randomization confounding variable\n    int&lt;lower=0, upper=1&gt; Y[N]; // binary outcome\n    matrix[N, PS] XS; // model matrix for principal stratum model\n    matrix[N, PG] XG; // model matrix for outcome model\n}\ntransformed data {\n    int S[8];\n   S[1] = 1;\n   S[2] = 1;\n   S[3] = 2;\n   S[4] = 2;\n   S[5] = 3;\n   S[6] = 3;\n   S[7] = 4;\n   S[8] = 4;\n}\n \nparameters {\n    matrix[3, PS] beta_S; // coefficients for principal stratum model\n    matrix[8, PG] beta_G; // coefficients for outcome model\n}\ntransformed parameters {\n}\nmodel {\n    // random effect\n    // prior\n    if (PS &gt;= 2)\n        to_vector(beta_S[:, 2:PS]) ~ normal(0, 1);\n    if (PG &gt;= 2)\n        to_vector(beta_G[:, 2:PG]) ~ normal(0, 1);\n    // model\n    for (n in 1:N) {\n        int length;\n        real log_prob[4];\n        log_prob[1] = 0;\n        for (s in 2:4) {\n            log_prob[s] = XS[n] * beta_S[s-1]';\n        }\n        if (Z[n] == 0 && D[n] == 0)\n            length = 2;\n        else if (Z[n] == 1 && D[n] == 0)\n            length = 2;\n        else if (Z[n] == 1 && D[n] == 1)\n            length = 2;\n        else if (Z[n] == 0 && D[n] == 1)\n            length = 2;\n        {\n            real log_l[length];\n            if (Z[n] == 0 && D[n] == 0) {\n                // Z:0 D:0 S:0/1\n                log_l[1] = log_prob[1] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[1]'));\n                log_l[2] = log_prob[2] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[3]'));\n            }\n            else if (Z[n] == 1 && D[n] == 0) {\n                // Z:1 D:0 S:0/2\n                log_l[1] = log_prob[1] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[2]'));\n                log_l[2] = log_prob[3] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[6]'));\n            }\n            else if (Z[n] == 1 && D[n] == 1) {\n                // Z:1 D:1 S:1/3\n                log_l[1] = log_prob[2] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[4]'));\n                log_l[2] = log_prob[4] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[8]'));\n            }\n            else if (Z[n] == 0 && D[n] == 1) {\n                // Z:0 D:1 S:2/3\n                log_l[1] = log_prob[3] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[5]'));\n                log_l[2] = log_prob[4] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[7]'));\n            }\n            target += log_sum_exp(log_l) - log_sum_exp(log_prob);\n        }\n    }\n}\ngenerated quantities {\n    vector[4] strata_prob; // the probability of being in each stratum\n    vector[8] mean_effect; // mean response\n    {\n        matrix[N, 4] log_prob;\n        vector[8] numer;\n        matrix[N, 8] expected_mean;\n        for (i in 1:N)\n            for (j in 1:8)\n                expected_mean[i, j] = inv_logit(XG[i] * beta_G[j]');\n        log_prob[:, 1] = rep_vector(0, N);\n        log_prob[:, 2:4] = XS * beta_S';\n        for (n in 1:N) {\n            log_prob[n] -= log_sum_exp(log_prob[n]);\n        }\n        for (s in 1:4) strata_prob[s] = mean(exp(log_prob[:, s]));\n        for (g in 1:8) {\n            numer[g] = mean(expected_mean[:, g] .* exp(log_prob[:, S[g]]));\n            mean_effect[g] = numer[g] / strata_prob[S[g]];\n        }\n    }\n}\n\n\nIn the simplest case, where we have no \\(X\\) design matrix, i.e. just intercepts, the observed likelihood is\n\\[\n\\begin{aligned}\nL = p(Z, D, Y) = p(Z) \\sum_s p(S = s) p(D | Z, S = s) p(Y | D, Z, S = s)\n\\end{aligned}\n\\]\nHowever, the \\(p(D | Z, S = s)\\) part is either 0 or 1, depending on whether \\((D, Z, S)\\) are consistent with each other or not. This results in a simplification to:\n\\[\n\\begin{aligned}\nL \\propto \\sum_{s \\text{ consistent with } Z \\text{ and } D} p(S=s)p(Y | Z, S = s)\n\\end{aligned}\n\\]\nwhich is what is happening in the conditional elements of the model block.\nSpecifically, the linear predictors for the multinomial S-model are implemented as the XS[n] * beta_S[s-1]' code. With a single column in the XS design matrix, an implicit uniform prior is being placed on the intercept term, being defined as the log-probability (actually it is the log-odds). Note that the reference category is set to zero for identifiability.\nFor each unit, the combination of the treatment assignment and the occurrence of the intermediate event dictate whether there are one or two contributions to the log-likelihood corresponding to the two cohorts that make up the units within each strata for this example. Without the monotonicity and exclusion restriction assumptions, all combinations result in contributions.\nFor example, when control is the assignment and the received intervention is the control, there are contributions from both the never-takes (log_prob[1]) and the compliers (log_prob[2]). Similarly, when treatment is the assignment and the received intervention is the control, there are contributions from the never-takers (again the log_prob[1]) and the defiers (log_prob[3]) and so on.\nThe two assumptions (monotonicity/ER) can be introduced to the model as follows:\n\n\ndata {\n    int&lt;lower=0&gt; N; // number of observations\n    int&lt;lower=0&gt; PS; // number of predictors for principal stratum model\n    int&lt;lower=0&gt; PG; // number of predictors for outcome model\n    array[N] int&lt;lower=0, upper=1&gt; Z; // treatment arm\n    array[N] int&lt;lower=0, upper=1&gt; D; // post randomization confounding variable\n    array[N] int&lt;lower=0, upper=1&gt; Y; // binary outcome\n    matrix[N, PS] XS; // model matrix for principal stratum model\n    matrix[N, PG] XG; // model matrix for outcome model\n}\n \ntransformed data {\n   array[4] int S;\n   S[1] = 1;\n   S[2] = 2;\n   S[3] = 2;\n   S[4] = 3;\n}\n \nparameters {\n    matrix[2, PS] beta_S; // coefficients for principal stratum model\n    matrix[4, PG] beta_G; // coefficients for outcome model\n}\n \ntransformed parameters {\n}\n \nmodel {\n    // random effect\n    // prior\n    \n    // use informative prior for intercepts\n    beta_S[:, 1] ~ normal(0, 2);\n    beta_G[:, 1] ~ normal(0, 2);\n    \n    if (PS &gt;= 2)\n        to_vector(beta_S[:, 2:PS]) ~ normal(0, 1);\n    if (PG &gt;= 2)\n        to_vector(beta_G[:, 2:PG]) ~ normal(0, 1);\n    // model\n    for (n in 1:N) {\n        int length;\n        array[3] real log_prob;\n        log_prob[1] = 0;\n        for (s in 2:3) {\n            log_prob[s] = XS[n] * beta_S[s-1]';\n        }\n        if (Z[n] == 0 && D[n] == 0)\n            length = 2;\n        else if (Z[n] == 1 && D[n] == 0)\n            length = 1;\n        else if (Z[n] == 1 && D[n] == 1)\n            length = 2;\n        else if (Z[n] == 0 && D[n] == 1)\n            length = 1;\n        {\n            array[length] real log_l;\n            if (Z[n] == 0 && D[n] == 0) {\n                // Z:0 D:0 S:0/1 never takers or compliers\n                log_l[1] = log_prob[1] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[1]'));\n                log_l[2] = log_prob[2] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[2]'));\n            }\n            else if (Z[n] == 1 && D[n] == 0) {\n                // Z:1 D:0 S:0 never takers (defiers don't exist)\n                log_l[1] = log_prob[1] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[1]'));\n            }\n            else if (Z[n] == 1 && D[n] == 1) {\n                // Z:1 D:1 S:1/2 compliers or always takers\n                log_l[1] = log_prob[2] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[3]'));\n                log_l[2] = log_prob[3] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[4]'));\n            }\n            else if (Z[n] == 0 && D[n] == 1) {\n                // Z:0 D:1 S:2 always takers\n                log_l[1] = log_prob[3] + bernoulli_lpmf(Y[n] | inv_logit(XG[n] * beta_G[4]'));\n            }\n            target += log_sum_exp(log_l) - log_sum_exp(log_prob);\n        }\n    }\n}\n \ngenerated quantities {\n    vector[3] strata_prob; // the probability of being in each stratum\n    vector[4] mean_effect; // mean response\n    {\n        matrix[N, 3] log_prob;\n        vector[4] numer;\n        matrix[N, 4] expected_mean;\n        for (i in 1:N)\n            for (j in 1:4)\n                expected_mean[i, j] = inv_logit(XG[i] * beta_G[j]');\n        log_prob[:, 1] = rep_vector(0, N);\n        log_prob[:, 2:3] = XS * beta_S';\n        for (n in 1:N) {\n            log_prob[n] -= log_sum_exp(log_prob[n]);\n        }\n        for (s in 1:3) strata_prob[s] = mean(exp(log_prob[:, s]));\n        for (g in 1:4) {\n            numer[g] = mean(expected_mean[:, g] .* exp(log_prob[:, S[g]]));\n            mean_effect[g] = numer[g] / strata_prob[S[g]];\n        }\n    }\n}\n\n\nwhere for certain combinations of \\(Z\\) and \\(D\\) only one group is assumed to contribute. Specifically, for the monotonicity assumption the set of possible strata is restricted. Units are assumed to never be defiers and therefore for the cases where \\((Z=1,D=0)\\) and \\((Z=0,D=1)\\) the contribution to the Y-model is one-dimensional.\nAdditionally, an informative prior was placed on the intercepts in the above model.\nFor the exclusion restriction assumption, the number of non-zero effects is truncated, reducing the number of free parameters. This happens because we are assuming that the strata of units that never take the treatment are unable to show a treatment effect and similarly so for those that always take treatment.\nFinally, the model section increments the target via the log_sum_exp calculations for the contributions of all units. These effectively map to\n\\[\nL \\propto \\sum_{s \\text{ consistent with } Z \\text{ and } D}  \\frac{\\exp(\\beta_s)p(Y | Z, S = s)}{\\sum_{s \\text{ consistent with } Z \\text{ and } D} \\exp(\\beta_s)}   \n\\]\nThe generated quantities block (version 1 of the stan model) is used to (1) compute the expected values of the outcome for each unit for all strata, which can be used to derive the principal causal effects. In version 2 of the model, this process is simplified as there are less strata and comparisons to consider.\n\n\nApplication\nThis draws heavily on the notes from Fan Li’s (Duke Uni) lecture notes and labs on causal inference.\nMimic a two-arm trial \\(z = 0, 1\\) with two-sided non-compliance and all cause mortality as the primary outcome. Define strata where \\(D_i(z)\\) indicates the treatment received under assignment to \\(z\\):\n\nnever takers \\((0,0) = \\{i: D_i(0) = 0, D_i(1) = 0\\}\\)\ncompliant \\((0,1) = \\{i: D_i(0) = 0, D_i(1) = 1\\}\\)\nalways takers \\((1,1) = \\{i: D_i(0) = 1, D_i(1) = 1\\}\\)\n\ni.e. assume monotonicity - that defiers do not exist. Simulate strata membership using independent samples drawn with probability 0.2, 0.6, 0.2. Define baseline covariates for disease severity \\(X_1 \\sim \\mathcal{N}(0, 1)\\) and age above 60 \\(X_2 \\sim \\text{Bernoulli}(0.6)\\).\n\\[\n\\begin{aligned}\n(Y | S = (0,0), Z = z, X_1, X_2) &\\sim \\text{Bernoulli}(g^{-1}(\\eta_{s[1]})) \\\\\n(Y | S = (0,1), Z = z, X_1, X_2) &\\sim \\text{Bernoulli}(g^{-1}(\\eta_{s[2]})) \\\\\n(Y | S = (1,1), Z = z, X_1, X_2) &\\sim \\text{Bernoulli}(g^{-1}(\\eta_{s[3]})) \\\\ \\\\\n\\eta_{s[1]} &=  0.1        + 1.1 X_1 + 0.4 X_2  \\\\\n\\eta_{s[2]} &=  0   - 2 z  + 1.1 X_1 + 0.4 X_2   \\\\\n\\eta_{s[3]} &= -0.3        + 1.1 X_1 + 0.4 X_2   \\\\\n\\end{aligned}\n\\]\n\nset.seed(973589239)\n\nN &lt;- 1e4\ns &lt;- sample(1:3, N, replace = TRUE, prob = c(0.2, 0.6, 0.2))\nz &lt;- sample(c(0, 1), N, replace = TRUE, prob = c(0.5, 0.5))\nx1 &lt;- rnorm(N)\nx2 &lt;- rbinom(N, 1, 0.6)\n\nd_1 &lt;- data.table(s = s, z = z, x1 = x1, x2 = x2)\n\nd_1[s == 1, d := 0]\nd_1[s == 2, d := z]\nd_1[s == 3, d := 1]\n\nd_1[s == 1, eta := 0.1 + 1.1*x1 + 0.4*x2]\nd_1[s == 2, eta := 0  -2*z + 1.1*x1 + 0.4*x2]\nd_1[s == 3, eta := -0.3      + 1.1*x1 + 0.4*x2]\n\nd_1[, y := rbinom(N, 1, plogis(eta))]\n\n\n# expected linear predictor for control 0 + -2*0 + 1.1*0 + 0.4*0.6 = 0.24\n# expected linear predictor for control 0 + -2*1 + 1.1*0 + 0.4*0.6 = -1.76\nd_1[s == 2 & z == 0, mean(eta)]\n## [1] 0.2463996\nd_1[s == 2 & z == 1, mean(eta)]\n## [1] -1.772282\n\n\n# convert to prob scale \nd_1[s == 2 & z == 0, plogis(mean(eta))]\n## [1] 0.5612901\nd_1[s == 2 & z == 1, plogis(mean(eta))]\n## [1] 0.1452588\nd_1[s == 2 & z == 1, plogis(mean(eta))] - d_1[s == 2 & z == 0, plogis(mean(eta))]\n## [1] -0.4160314\n\n\n# average probability across strata by treatment group\nd_1[s == 2 & z == 0, mean(plogis(eta))]\n## [1] 0.5480463\nd_1[s == 2 & z == 1, mean(plogis(eta))]\n## [1] 0.1926421\nd_1[s == 2 & z == 1, mean(plogis(eta))] - d_1[s == 2 & z == 0, mean(plogis(eta))]\n## [1] -0.3554042\n\n\n# should align somewhat to the observed data\nd_1[s == 2 & z == 0, mean(y)]\n## [1] 0.5494505\nd_1[s == 2 & z == 1, mean(y)]\n## [1] 0.2080828\nd_1[s == 2 & z == 1, mean(y)] - d_1[s == 2 & z == 0, mean(y)]\n## [1] -0.3413677\n\nGiven we simulate the data, we know which strata is which so we can use g-computation to calculate the risk difference in the observed data. On average, this should be somewhere near the expected value but will not be equal to it.\n\n# compliers\nd_s2 &lt;- d_1[s == 2]\nf0 &lt;- glm(y ~ z + x1 + x2, data = d_s2, family = binomial())\n\nd_s2_0 &lt;- copy(d_s2)\nd_s2_0[, z := 0]\neta_0 &lt;- predict(f0, newdata = d_s2_0)\n\nd_s2_1 &lt;- copy(d_s2)\nd_s2_1[, z := 1]\neta_1 &lt;- predict(f0, newdata = d_s2_1)\n\nrd &lt;- plogis(mean(eta_1)) - plogis(mean(eta_0))\nrd\n## [1] -0.3965591\n\n# average risk by group, not average log-odds transformed as above\n# plogis(E[X]) \\ne E[plogis(X)] due to nonlinearity\nrd &lt;- mean(plogis(eta_1)) - mean(plogis(eta_0))\nrd\n## [1] -0.3397087\n\nCompile and fit the second implementation of the principal strata model that includes both monotonicity and ER assumptions:\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/principal-stratum-02.stan\")\n\nld &lt;- list(\n  N = nrow(d_1),\n  PS = 1, # intercept only model\n  PG = 3, # intercept plus additive x1, x2, no interaction\n  Z = d_1$z,\n  D = d_1$d,\n  Y = d_1$y,\n  XS = matrix(rep(1, nrow(d_1)), ncol = 1),\n  XG = cbind(1, d_1$x1, d_1$x2)\n  )\n\nf1 &lt;- m1$sample(data = ld, chains = 1, iter_sampling = 1000, refresh = 0,\n                adapt_delta = 0.9)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 134.1 seconds.\n\n\nWe can visualise the posterior for strata membership as shown in Figure 1.\n\nd_p1 &lt;- data.table(f1$draws(variables = \"strata_prob\", format = \"matrix\"))\nnames(d_p1) &lt;- paste0(1:ncol(d_p1))\nd_fig &lt;- melt(d_p1, measure.vars = names(d_p1), variable.name = \"strata\")\nd_fig[strata == \"1\", strata := \"n (0,0)\"]\nd_fig[strata == \"2\", strata := \"c (0,1)\"]\nd_fig[strata == \"3\", strata := \"a (1,1)\"]\n\nggplot(d_fig, aes(x = value, group = strata, col = strata)) +\n  geom_density() +\n  scale_color_discrete(\"Strata\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: Posterior proportions for strata membership\n\n\n\n\n\nAnd calculate the posterior expected mean for each arm and each strata as follows:\n\nd_p1 &lt;- data.table(f1$draws(variables = \"mean_effect\", format = \"matrix\"))\n\n# by definition and assumptions:\nN_strata &lt;- 3\nN_trt &lt;- 2\n\nd_grid &lt;- CJ(\n  s = 1:N_strata,\n  z = 0:(N_trt-1)\n)\n# strata definitions (0,0) never, (0,1)compliant, (1,1) always\nd_grid[, d := c(\n  0, 0, 0, 1, 1, 1\n)]\n# index into effect estimate for comparison\nd_grid[, g := c(\n  1, 1, 2, 3, 4, 4\n)]\n\na_out &lt;- array(NA, dim = c(N_strata, N_trt, nrow(d_p1)))\n\nfor(i in 1:N_strata){\n  for(j in 1:N_trt){\n    g_ref &lt;-  d_grid[s == i & z == (j-1), g] \n    a_out[i,j,] &lt;- d_p1[, g_ref, with = F][[1]]\n  }\n}\n\ndimnames(a_out)[[1]] &lt;- c(\"n\", \"c\", \"a\")\ndimnames(a_out)[[2]] &lt;- c(\"0\", \"1\")\n\n# summary outputs (means):\napply(a_out, c(1,2), mean)\n\n          0         1\nn 0.5708451 0.5708451\nc 0.5483657 0.2098348\na 0.5012453 0.5012453\n\n\nand then summarise treatment effects via simple comparisons between the arms:\n\nrbind(\n  n = quantile(a_out[1, 2,] - a_out[1, 1,], probs = c(0.5, 0.025, 0.975)),\n  c = quantile(a_out[2, 2,] - a_out[2, 1,], probs = c(0.5, 0.025, 0.975)),\n  a = quantile(a_out[3, 2,] - a_out[3, 1,], probs = c(0.5, 0.025, 0.975))\n)\n\n        50%       2.5%      97.5%\nn  0.000000  0.0000000  0.0000000\nc -0.338292 -0.3676863 -0.3087216\na  0.000000  0.0000000  0.0000000\n\n\nFurther testing would be beneficial to evaluate the long-run properties of this particular estimator.\n\n\n\n\n\nReferences\n\n1. Frangakis C, Rubin D. Principal stratification in causal inference. Clinical Trials. 2002;58:21–9.\n\n\n2. Page L. Principal stratification: A tool for understanding variation in program effects across endogenous subgroups. American Journal of Evaluation. 2015. https://doi.org/10.1177/1098214015594419.\n\n\n3. Mercatanti A. Do debit cards decrease cash demand?: Causal inference and sensitivity analysis using principal stratification. JRSS Applied Statistics Series C. 2017. https://doi.org/10.1111/rssc.12193.\n\n\n4. Liu B. Principal stratification analysis of noncompliance with time-to-event outcomes. Biometric Methodology. 2024. https://doi.org/10.1093/biomtc/ujad016.\n\n\n5. Hirano K. Assessing the effect of an influenza vaccine in an encouragement design. Biostatistics. 2000. https://doi.org/10.1093/biostatistics/1.1.69.\n\n\n6. Liu B, Li F. PStrata: An r package for principal stratification. Journal of Statistical Software. 2023."
  },
  {
    "objectID": "notebooks/random-walk-prior.html",
    "href": "notebooks/random-walk-prior.html",
    "title": "Random walk priors",
    "section": "",
    "text": "First order random walk\nFor regular spacings, a first-order random walk prior can be specified as:\n\\[\n\\begin{aligned}\n\\eta_0 &\\sim \\text{Logistic}(0,1) \\\\\n\\delta &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma_\\delta &\\sim \\text{Exponential}(1) \\\\\n\\eta_{[1]} &= \\eta_0 \\\\\n\\eta_{[k]} &= \\sum_{i = 2}^{N}(\\eta_{[k-1]}  + \\delta  \\sigma_\\delta) \\\\\n\\end{aligned}\n\\]\nSimulate data from an oscillator:\n\nlibrary(data.table)\nlibrary(ggplot2)\n\nset.seed(2)\nd_obs &lt;- data.table(\n  x = sort(runif(100, 0, 2*pi))\n)\nd_obs[, eta := sin(x)]\nd_obs[, n := rpois(.N, 200)]\nd_obs[, y := rbinom(.N, n, plogis(eta))]\n\n# we only observe 30% of the data generated\nd_obs[, y_mis := rbinom(.N, 1, 0.7)]\n\nNaive implementation of a first order random walk in stan.\n\n\ndata {    \n  int N; \n  // the way the model is set up it does not matter if some of the n's are\n  // zero because the likelihood uses y_sub, which is obtained by reference\n  // to the missing indicator y_mis, which explicitly says that there were\n  // no observations at the given value of x.\n  array[N] int y;    \n  array[N] int n;    \n  vector[N] x;    \n  array[N] int y_mis; \n  \n  int prior_only;    \n  \n  // priors\n  real r_nu;\n  \n}    \ntransformed data {\n  // x_diff gives us the variable spacing in x and allows us to scale\n  // the variance appropriately\n  vector[N-1] x_diff;\n  // the number of observations we truly had once missingness is accounted for\n  int N_sub = N - sum(y_mis);\n  // our truly observed responses (successes) and trials\n  array[N_sub] int y_sub;\n  array[N_sub] int n_sub;\n  // \n  for(i in 1:(N-1)){x_diff[i] = x[i+1] - x[i];}\n  // go through the data that was passed in and build the data on which \n  // we will fit the model\n  int j = 1;\n  for(i in 1:N){\n    if(y_mis[i] == 0){\n      y_sub[j] = y[i];\n      n_sub[j] = n[i];\n      j += 1;\n    }\n  }  \n}\nparameters{  \n  // the first response\n  real b0;    \n  // offsets\n  vector[N-1] delta;    \n  // how variable the response is\n  real&lt;lower=0&gt; nu;   \n}    \ntransformed parameters{    \n  // the complete modelled mean response\n  vector[N] e; \n  // this is the variance scaled for the distance between each x\n  // note this is truly a variance and not an sd\n  vector[N-1] tau;    \n  // \n  vector[N_sub] eta_sub;    \n  // adjust the variance for the distance b/w doses    \n  // note that nu is squared to turn it into variance\n  for(i in 2:N){tau[i-1] = x_diff[i-1]*pow(nu, 2);}    \n  // resp is random walk with missingness filled in due to the \n  // dependency in the prior\n  e[1] = b0;    \n  // each subsequent observation has a mean equal to the previous one\n  // plus some normal deviation with mean zero and variance calibrated for\n  // the distance between subsequent observations.\n  for(i in 2:N){e[i] = e[i-1] + delta[i-1] * sqrt(tau[i-1]);}    \n  // eta_sub is what gets passed to the likelihood\n  { \n    int k = 1;\n    for(i in 1:N){\n      if(y_mis[i] == 0){\n        eta_sub[k] = e[i];\n        k += 1;\n      }\n    }\n  }\n}    \nmodel{    \n  // prior on initial response\n  target += logistic_lpdf(b0 | 0, 1);\n  // prior on sd\n  target += exponential_lpdf(nu | r_nu);\n  // standard normal prior on the offsets\n  target += normal_lpdf(delta | 0, 1);    \n  if(!prior_only){target += binomial_logit_lpmf(y_sub | n_sub, eta_sub);}    \n}    \ngenerated quantities{    \n  // predicted values at each value of x\n  vector[N] p;    \n  vector[N-1] e_diff;    \n  vector[N-1] e_grad;    \n  // compute diffs\n  for(i in 1:(N-1)){e_diff[i] = e[i+1] - e[i];}\n  e_grad = e_diff ./ x_diff;\n  p = inv_logit(e);\n}    \n\n\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/random-walk-01.stan\")\n\n\nld = list(\n  N = nrow(d_obs), \n  y = d_obs[, y], \n  n = d_obs[, n],\n  x = d_obs[, x], \n  y_mis = d_obs[, y_mis], \n  prior_only = F, \n  r_nu =  3\n  )\n\n\nf1 &lt;- m1$sample(\n  ld, iter_warmup = 1000, iter_sampling = 2000,\n  parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = F,\n  max_treedepth = 10)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 3.0 seconds.\n\nf1$summary(variables = c(\"nu\"))\n\n# A tibble: 1 × 10\n  variable  mean median    sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 nu       0.533  0.523 0.100 0.0962 0.390 0.710  1.00    1298.    1384.\n\n\nRepresentation of output.\n\nd_out &lt;- data.table(f1$draws(variables = \"p\", format = \"matrix\"))\n\nd_fig &lt;- melt(d_out, measure.vars = names(d_out))\nd_fig &lt;- d_fig[, .(\n  mu = mean(value), \n  q_025 = quantile(value, prob = 0.025),\n  q_975 = quantile(value, prob = 0.975)\n), keyby = variable]\nd_fig[, ix := gsub(\"p[\", \"\", variable, fixed = T)]\nd_fig[, ix := as.numeric(gsub(\"]\", \"\", ix, fixed = T))]\nd_fig[, x := d_obs[ix, x]]\n\n\nggplot(d_obs, aes(x = x, y = plogis(eta))) +\n  geom_line(lty = 1) +\n  geom_point(data = d_obs[y_mis == 0],\n             aes(x = x, y = y/n), size = 0.7) +\n  geom_point(data = d_obs[y_mis == 1],\n             aes(x = x, y = y/n), size = 0.7, pch = 2) +\n  geom_ribbon(data = d_fig, \n              aes(x = x, ymin = q_025, ymax = q_975),\n              inherit.aes = F, fill = 2, alpha = 0.3) +\n  geom_line(data = d_fig, \n              aes(x = x, y = mu), col = 2) +\n  geom_point(data = d_fig, \n              aes(x = x, y = mu), col = 2, size = 0.6) +\n  scale_x_continuous(\"x\") +\n  scale_y_continuous(\"Probability\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 1: True function (black line), data on which the model was fit (black points), data we did not see (black triangles), random walk (red line) with interpolated points (red) and uncertainty (red ribbon).\n\n\n\n\n\n\n\nSecond order random walk\nThe second order random walk for regular locations has density\n\\[\n\\begin{aligned}\n\\pi(x) \\propto \\exp\\left( -\\frac{1}{2} \\sum_{i=2}^{n-1} (x_{i-1} - 2x_i + x_{i+1})^2  \\right)\n\\end{aligned}\n\\]\nThe main term can be interpreted as an estimate of the second order derivative of a continuous time function. But this is not generally suitable for irregular spacings of x [1].\n\n\nReferences\n\n\n1. Lindgren F, Rue H. On the second-order random walk model for irregular locations. Scandinavian Journal of Statistics. 2008;35:691–700."
  }
]