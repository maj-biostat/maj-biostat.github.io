[
  {
    "objectID": "notebooks/robust-errors.html",
    "href": "notebooks/robust-errors.html",
    "title": "Robust errors for estimating proportion",
    "section": "",
    "text": "The goto approach for estimating an uncertainty interval for a proportion is to use the normal approximation of the binomial distribution:\n\\[\n\\begin{aligned}\n\\hat{p} \\pm z_{(1 - \\alpha)/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\end{aligned}\n\\]\nwhere \\(n\\) is the sample size, \\(\\hat{p}\\) is the observed sample proportion and \\(z\\) is the standard normal quantile (and typically set to \\(\\approx 2\\)).\nSay we had multiple estimates of the proportion, e.g. number of times we observe antimicrobial resistance out of the positive blood cultures we collected over the previous year. These estimates might come from differing hospitals and some might include repeat tests on individuals. This means that we have multiple levels of variation to deal with. One approach is to use a robust (sometimes call a Hubert White or sandwich) estimator for the standard errors.\nThis can be achieved by fitting a standard glm and then making an adjustment to the standard errors using the tools provided in the R sandwich package.\nSimulate data for 500 patients from 10 sites, some patients having repeat measures.\n\nlibrary(sandwich)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(marginaleffects)\nlibrary(lme4)\n\nLoading required package: Matrix\n\n# library(gee)\nlibrary(\"geepack\")\n\n# N unique pts on which we have multiple obs, each pt nested within one of\n# the 10 sites\nN &lt;- 500\nN_site &lt;- 10\np_site &lt;- as.numeric(extraDistr::rdirichlet(1, rep(1, 10)))\nsites &lt;- sample(1:N_site, N, replace = T, prob = p_site)\nd_pt &lt;- data.table(\n  id_pt = 1:N,\n  site = sort(sites)\n)\n# number obs per pt - inflated here to make a point\nn_obs &lt;- rpois(N, 2)\nd &lt;- d_pt[unlist(lapply(1:N, function(x){\n  rep(x, n_obs[x])\n}))]\nd[, id_obs := 1:.N, keyby = id_pt]\n\n# about 60% (plogis(0.4)) resistant but with site and subject level\n# variability beyond the natural sampling variability due to varying \n# number of subjects per site\nnu &lt;- rnorm(N, 0, 0.5)\n# treat site as true fixed effect\nrho &lt;- rnorm(N_site, 0, 0.7)\nd[, eta := 0.4 + rho[site] + nu[id_pt]]\n\nd[, y := rbinom(.N, 1, plogis(eta))]\nd[, site := factor(site)]\nd[, id_pt := factor(id_pt)]\n\np_obs &lt;- d[, sum(y)/.N]\n\n# d[, .(y = sum(y), n = .N)]\n# distribution of frequency of observations on a pt\n# hist(d[, .N, keyby = id_pt][, N])\n\nThe raw numbers of observations at each site are shown in Figure 1.\n\nd_fig &lt;- copy(d)\nd_fig[y == 0, resp := \"Susceptible\"]\nd_fig[y == 1, resp := \"Resistant\"]\nggplot(d_fig, aes(x = site, fill = resp)) +\n  geom_bar() +\n  scale_fill_discrete(\"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: Observations by site\n\n\n\n\n\nOverall, the observed proportion resistant to antibiotics is 0.58. Various ways exist to estimate the uncertainty.\nThe wald estimate for the uncertainty interval is calculated as:\n\n# wald (normal approximation)\nse_wald &lt;- sqrt(p_obs * (1-p_obs) / nrow(d))\np_0_lb &lt;- p_obs - qnorm(0.975) * se_wald\np_0_ub &lt;- p_obs + qnorm(0.975) * se_wald\n\nA GLM with only an intercept term will give the same prediction as the observed proportion and we can calculate the naive estimate of uncertainty as:\n\n# standard glm, not accounting for pt\nf1 &lt;- glm(y ~ 1, family = binomial, data = d)\n\npredict(f1, type = \"response\")[1]\n\n        1 \n0.5802592 \n\n# get naive standard errors\ns_f1 &lt;- summary(f1)$coef\n\n# model uncertainty naive\nlo_1 &lt;- qlogis(p_obs)\n# se from intercept term, i.e. we are just looking at the 'average' or\n# typical pt, over which we would expect heterogeneity\np_1_lb &lt;- plogis(lo_1 - qnorm(0.975) * s_f1[1, 2])\np_1_ub &lt;- plogis(lo_1 + qnorm(0.975) * s_f1[1, 2])\n\nWe can use the sandwich estimator to adjuste for heterogeneity as:\n\n# adjusted to account for heterogeneity due to site (we did not \n# adjust for site in the model) and repeat measure for pt\nsw_se &lt;- sqrt(vcovCL(f1, cluster = d[, .(site, id_pt)], type = \"HC1\")[1,1])\np_2_lb &lt;- plogis(lo_1 - qnorm(0.975) * sw_se)\np_2_ub &lt;- plogis(lo_1 + qnorm(0.975) * sw_se)\n\n\nf2 &lt;- glmer(y ~ (1|site) + (1|id_pt), data = d, family = binomial)\n\nboundary (singular) fit: see help('isSingular')\n\n\nNote that in the glmer model (with a non-linear link function) the predictions are first made on the link scale, averaged, and then back transformed. This means that the average prediction may not be exactly identical to the average of predictions.\nYou’ll note that the point estimate from the glmer deviates from the observed proportion. This can be for all of the following reasons:\n\nThe GLMM provides subject-specific estimates, conditional on the random effects.\nGLMMs involve shrinkage, where estimates for groups with less data are pulled towards the overall mean.\nLarger random effects can lead to bigger differences.\nThe GLMM estimate is a model-based estimate that accounts for the hierarchical structure of the data and provides a framework for inference.\n\nIn theory, a GEE could also be used but in R the GEE framework is not particularly well set up for multiple levels of clustering.\n\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_0_lb, p_0_ub)\n\n[1] \"0.5803 (0.5497, 0.6108)\"\n\n# Model based adjusting for site.\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_1_lb, p_1_ub)\n\n[1] \"0.5803 (0.5494, 0.6105)\"\n\n# Adjusted - account for heterogeneity due to site (we did not \n# adjust for site) and repeat measure for pt\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_2_lb, p_2_ub)\n\n[1] \"0.5803 (0.4808, 0.6736)\"\n\n# Finally a random effects model\navg_predictions(f2, re.form = NA, type = \"link\")\n\n\n Estimate Std. Error    z Pr(&gt;|z|)   S   2.5 % 97.5 %\n    0.325      0.175 1.85   0.0639 4.0 -0.0188  0.669\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  link \n\navg_predictions(f2, re.form = NA, type = \"response\")\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    0.581     0.0427 13.6   &lt;0.001 137.4 0.497  0.664\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n## Step 1\npred &lt;- predictions(f2, type = \"link\", re.form = NA)$estimate\n## Step 2: average\nplogis(mean(pred))\n\n[1] 0.5805439\n\n\n\nReferences"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Biostatistician working mostly in the area of Bayesian adaptive clinical trials using R and stan. The site contains various posts/reminders on topics that are relevant to my day-to-day work."
  },
  {
    "objectID": "about.html#repository-status",
    "href": "about.html#repository-status",
    "title": "About",
    "section": "Repository status",
    "text": "Repository status\n\nlibrary(git2r)\nrepo &lt;- git2r::repository(path = \".\")\nsummary(repo)\n\nLocal:    main /Users/mark/Documents/project/website/src/maj-biostat.github.io\nRemote:   main @ origin (https://github.com/maj-biostat/maj-biostat.github.io)\nHead:     [5cf7ba8] 2024-09-18: wip\n\nBranches:         1\nTags:             0\nCommits:         10\nContributors:     2\nStashes:          0\nIgnored files:    2\nUntracked files:  5\nUnstaged files:  26\nStaged files:     2\n\nLatest commits:\n[5cf7ba8] 2024-09-18: wip\n[cfd1ac9] 2024-09-18: wip\n[ceabb23] 2024-09-18: minor edit\n[9e01149] 2024-09-18: add listing\n[19b90b8] 2024-09-18: wip"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "maj-biostat.github.io",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n2024-09-25\n\n\nDesirability of Outcome Ranking (DOOR)\n\n\n1 min\n\n\n\n\n\n\n\n2024-09-25\n\n\nMann-Whitney U\n\n\n14 min\n\n\n\n\n\n\n\n2024-09-25\n\n\nRandom walk priors\n\n\n3 min\n\n\n\n\n\n\n\n2024-09-25\n\n\nRobust errors for estimating proportion\n\n\n4 min\n\n\n\n\n\n\n\n2024-09-25\n\n\nUser-defined Probability Distributions in Stan\n\n\n2 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/random-walk-prior.html",
    "href": "notebooks/random-walk-prior.html",
    "title": "Random walk priors",
    "section": "",
    "text": "First order random walk\nFor regular spacings, a first-order random walk prior can be specified as:\n\\[\n\\begin{aligned}\n\\eta_0 &\\sim \\text{Logistic}(0,1) \\\\\n\\delta &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma_\\delta &\\sim \\text{Exponential}(1) \\\\\n\\eta_{[1]} &= \\eta_0 \\\\\n\\eta_{[k]} &= \\sum_{i = 2}^{N}(\\eta_{[k-1]}  + \\delta  \\sigma_\\delta) \\\\\n\\end{aligned}\n\\]\nSimulate data from an oscillator:\n\nlibrary(data.table)\nlibrary(ggplot2)\n\nset.seed(2)\nd_obs &lt;- data.table(\n  x = sort(runif(100, 0, 2*pi))\n)\nd_obs[, eta := sin(x)]\nd_obs[, n := rpois(.N, 200)]\nd_obs[, y := rbinom(.N, n, plogis(eta))]\n\n# we only observe 30% of the data generated\nd_obs[, y_mis := rbinom(.N, 1, 0.7)]\n\nNaive implementation of a first order random walk in stan.\n\n\ndata {    \n  int N; \n  // the way the model is set up it does not matter if some of the n's are\n  // zero because the likelihood uses y_sub, which is obtained by reference\n  // to the missing indicator y_mis, which explicitly says that there were\n  // no observations at the given value of x.\n  array[N] int y;    \n  array[N] int n;    \n  vector[N] x;    \n  array[N] int y_mis; \n  \n  int prior_only;    \n  \n  // priors\n  real r_nu;\n  \n}    \ntransformed data {\n  // x_diff gives us the variable spacing in x and allows us to scale\n  // the variance appropriately\n  vector[N-1] x_diff;\n  // the number of observations we truly had once missingness is accounted for\n  int N_sub = N - sum(y_mis);\n  // our truly observed responses (successes) and trials\n  array[N_sub] int y_sub;\n  array[N_sub] int n_sub;\n  // \n  for(i in 1:(N-1)){x_diff[i] = x[i+1] - x[i];}\n  // go through the data that was passed in and build the data on which \n  // we will fit the model\n  int j = 1;\n  for(i in 1:N){\n    if(y_mis[i] == 0){\n      y_sub[j] = y[i];\n      n_sub[j] = n[i];\n      j += 1;\n    }\n  }  \n}\nparameters{  \n  // the first response\n  real b0;    \n  // offsets\n  vector[N-1] delta;    \n  // how variable the response is\n  real&lt;lower=0&gt; nu;   \n}    \ntransformed parameters{    \n  // the complete modelled mean response\n  vector[N] e; \n  // this is the variance scaled for the distance between each x\n  // note this is truly a variance and not an sd\n  vector[N-1] tau;    \n  // \n  vector[N_sub] eta_sub;    \n  // adjust the variance for the distance b/w doses    \n  // note that nu is squared to turn it into variance\n  for(i in 2:N){tau[i-1] = x_diff[i-1]*pow(nu, 2);}    \n  // resp is random walk with missingness filled in due to the \n  // dependency in the prior\n  e[1] = b0;    \n  // each subsequent observation has a mean equal to the previous one\n  // plus some normal deviation with mean zero and variance calibrated for\n  // the distance between subsequent observations.\n  for(i in 2:N){e[i] = e[i-1] + delta[i-1] * sqrt(tau[i-1]);}    \n  // eta_sub is what gets passed to the likelihood\n  { \n    int k = 1;\n    for(i in 1:N){\n      if(y_mis[i] == 0){\n        eta_sub[k] = e[i];\n        k += 1;\n      }\n    }\n  }\n}    \nmodel{    \n  // prior on initial response\n  target += logistic_lpdf(b0 | 0, 1);\n  // prior on sd\n  target += exponential_lpdf(nu | r_nu);\n  // standard normal prior on the offsets\n  target += normal_lpdf(delta | 0, 1);    \n  if(!prior_only){target += binomial_logit_lpmf(y_sub | n_sub, eta_sub);}    \n}    \ngenerated quantities{    \n  // predicted values at each value of x\n  vector[N] p;    \n  vector[N-1] e_diff;    \n  vector[N-1] e_grad;    \n  // compute diffs\n  for(i in 1:(N-1)){e_diff[i] = e[i+1] - e[i];}\n  e_grad = e_diff ./ x_diff;\n  p = inv_logit(e);\n}    \n\n\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/random-walk-01.stan\")\n\n\nld = list(\n  N = nrow(d_obs), \n  y = d_obs[, y], \n  n = d_obs[, n],\n  x = d_obs[, x], \n  y_mis = d_obs[, y_mis], \n  prior_only = F, \n  r_nu =  3\n  )\n\n\nf1 &lt;- m1$sample(\n  ld, iter_warmup = 1000, iter_sampling = 2000,\n  parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = F,\n  max_treedepth = 10)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 2.9 seconds.\n\n\nWarning: 2 of 2000 (0.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\nf1$summary(variables = c(\"nu\"))\n\n# A tibble: 1 × 10\n  variable  mean median    sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 nu       0.533  0.519 0.103 0.0946 0.392 0.722  1.00    1221.    1114.\n\n\nRepresentation of output.\n\nd_out &lt;- data.table(f1$draws(variables = \"p\", format = \"matrix\"))\n\nd_fig &lt;- melt(d_out, measure.vars = names(d_out))\nd_fig &lt;- d_fig[, .(\n  mu = mean(value), \n  q_025 = quantile(value, prob = 0.025),\n  q_975 = quantile(value, prob = 0.975)\n), keyby = variable]\nd_fig[, ix := gsub(\"p[\", \"\", variable, fixed = T)]\nd_fig[, ix := as.numeric(gsub(\"]\", \"\", ix, fixed = T))]\nd_fig[, x := d_obs[ix, x]]\n\n\nggplot(d_obs, aes(x = x, y = plogis(eta))) +\n  geom_line(lty = 1) +\n  geom_point(data = d_obs[y_mis == 0],\n             aes(x = x, y = y/n), size = 0.7) +\n  geom_point(data = d_obs[y_mis == 1],\n             aes(x = x, y = y/n), size = 0.7, pch = 2) +\n  geom_ribbon(data = d_fig, \n              aes(x = x, ymin = q_025, ymax = q_975),\n              inherit.aes = F, fill = 2, alpha = 0.3) +\n  geom_line(data = d_fig, \n              aes(x = x, y = mu), col = 2) +\n  geom_point(data = d_fig, \n              aes(x = x, y = mu), col = 2, size = 0.6) +\n  scale_x_continuous(\"x\") +\n  scale_y_continuous(\"Probability\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 1: True function (black line), data on which the model was fit (black points), data we did not see (black triangles), random walk (red line) with interpolated points (red) and uncertainty (red ribbon).\n\n\n\n\n\n\n\nSecond order random walk\nThe second order random walk for regular locations has density\n\\[\n\\begin{aligned}\n\\pi(x) \\propto \\exp\\left( -\\frac{1}{2} \\sum_{i=2}^{n-1} (x_{i-1} - 2x_i + x_{i+1})^2  \\right)\n\\end{aligned}\n\\]\nThe main term can be interpreted as an estimate of the second order derivative of a continuous time function. But this is not generally suitable for irregular spacings of x [1].\n\n\nReferences\n\n\n1. Lindgren F, Rue H. On the second-order random walk model for irregular locations. Scandinavian Journal of Statistics. 208AD;35:691–700."
  },
  {
    "objectID": "notebooks/mann-whitney-u.html",
    "href": "notebooks/mann-whitney-u.html",
    "title": "Mann-Whitney U",
    "section": "",
    "text": "The MWU checks a rank sum difference between two independent groups and tests whether two groups have been drawn from the same population. It is an alternative to the t-test. But it only requires that the observations from both groups are at least ordinal such that you can discern for any two observations which one is greater (or better in some sense). In the classical interpretation, the test assumes that the distributions are identical under the null hypothesis and that they are not identical under the alternative distribution. Underlying the test is an interest in the probability that a randomly selected unit from one group fairs better than a randomly selected unit from the other. More concretely, the interest is int:\n\\[\n\\text{Pr}(X &gt; Y) \\ne \\text{Pr}(Y &gt; X)\n\\]\nand if this quantity exceeds \\(0.5\\) then, all things being equal, you would prefer assignment to the \\(X\\) group.\nOne approach to the procedure involves combining all the observations and ordering the records from best to worse (keeping track of which record belongs to which group). Each row is assigned a rank, the sum of the ranks is calculated for the first group (\\(T_1\\)) and the second group (\\(T_2\\)) and then the \\(U\\) statistics are computed as:\n\\[\n\\begin{aligned}\nU_1 &= n_1 n_2 + \\frac{n_1(n_1 + 1)}{2} - T_1 \\\\\nU_2 &= n_1 n_2 + \\frac{n_2(n_2 + 1)}{2} - T_2 \\\\\n\\end{aligned}\n\\]\nthe Mann-Whitney-U is given by \\(U = \\text{min}(U_1,U_2)\\), which is the test statistic.\nFor example, say we have units 1 to 20 in the first group and units 21 to 40 in the second with all the units in the first group having a better score than the units in the second.\n\nlibrary(data.table)\nlibrary(ggplot2)\n# contrived setup to make the scores for units ascend \n# (lower values considered better)\nd &lt;- data.table(\n  i = 1:40, score = sort(rnorm(40, 0, 1)), \n  group = rep(1:2, each = 20)\n)\nd[, rank := 1:40]\nn1 &lt;- d[group == 1, .N]\nn2 &lt;- d[group == 2, .N]\n\nNow add up the ranks for both groups.\n\n(T1 = d[group == 1, sum(rank)])\n\n[1] 210\n\n(T2 = d[group == 2, sum(rank)])\n\n[1] 610\n\n\nFrom these compute \\(U_1\\) and \\(U_2\\)\n\n(U_1 = n1*n2 + n1*(n1+1)/2 - T1)\n\n[1] 400\n\n(U_2 = n1*n2 + n2*(n2+1)/2 - T2)\n\n[1] 0\n\n\nThe above is equivalent to a pairwise comparison procedure (see later). From this, we can estimate the probability of superiority as referred to earlier (which I believe, but am not certain, is assuming that the two distributions differ only in location, not in shape) as:\n\n# here the contrived setup says that the value of x_1 and x_2 also equate to their ranks\n# and that lower values are better.\nx_1 &lt;- 1:20\nx_2 &lt;- 21:40\nn_1 &lt;- length(x_1)\nn_2 &lt;- length(x_2)\nu_1 &lt;- 0\nu_2 &lt;- 0\n\nk &lt;- 1\nfor(i in 1:length(x_1)){\n  for(j in 1:length(x_2)){\n    if(x_1[i] &lt; x_2[j]) u_1 &lt;- u_1 + 1\n    else if(x_1[i] &gt; x_2[j]) u_2 &lt;- u_2 + 1\n    k &lt;- k + 1\n  }\n}\nsprintf(\"u1 = %.0f, Pr(X&gt;Y) = %.2f, u2 = %.0f, Pr(Y&gt;X) = %.2f\", u_1, u_1/(n_1*n_2), u_2, u_2/(n_1*n_2))\n\n[1] \"u1 = 400, Pr(X&gt;Y) = 1.00, u2 = 0, Pr(Y&gt;X) = 0.00\"\n\n\nThat is, for all possible pairwise combinations, we compare the value in the first group to each value in second group and add up how often X &gt; Y and compute the probability estimate of \\(\\text(Pr)(X&gt;Y)\\) by simply dividing the number of occurrences by the total number of pairs.\nFor the test, we take the following reference points. First, under no difference, the expected value for \\(U\\) is\n\\[\n\\mathbb{E}[U] = \\frac{n_1 n_2}{2}\n\\]\nwith a standard standard error of:\n\\[\n\\sigma_U = \\sqrt{\\frac{n_1 n_2 (n_1 + n_2 + 1)}{12}}\n\\]\nUsing these, you can compute a z-value using a normal approximation (for large samples &gt; 20 per group) in the usual way by taking the observed value for \\(U\\) subtracting the expected value and dividing by the standard error:\n\\[\nz = \\frac{U - \\mu_u}{\\sigma_U}\n\\]\nand obtain a p-value for the test."
  },
  {
    "objectID": "notebooks/mann-whitney-u.html#footnotes",
    "href": "notebooks/mann-whitney-u.html#footnotes",
    "title": "Mann-Whitney U",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe intuition for the above is that \\(F_e(x)\\) gives the probability that \\(X_e\\) is greater than a specific value of \\(x\\), and \\(f_c(x) dx\\) gives the probability that \\(X_c\\) falls in a small interval around \\(x\\). The product of these terms gives the probability that \\(X_e &gt; X_c\\) for that small interval and thus integrating over all possible values of \\(x\\) gives the value we are after.↩︎"
  },
  {
    "objectID": "notebooks/custom-distribution-stan.html",
    "href": "notebooks/custom-distribution-stan.html",
    "title": "User-defined Probability Distributions in Stan",
    "section": "",
    "text": "Overview\nSome of this material can be found in the stan user guide and this is solely to serve as a reference in my own words.\nTo implement, you just need to provide a function to increment the total log-probability appropriately.\n\n\n\n\n\n\nNote\n\n\n\nWhen a function with the name ending in *_lpdf* or *_lpmf* is defined, the stan compiler automatically makes a *_lupdf* or lupmf version. Only normalised custom distributions are permitted.\n\n\nAssume that we want to create a custom distribution per:\n\\[\n\\begin{aligned}\nf(x) &= (1-a) x^{-a}\n\\end{aligned}\n\\]\ndefined for \\(a \\in [0,1]\\) and \\(x \\in [0,1]\\) with cdf:\n\\[\n\\begin{aligned}\nF_x &= x^{a-1}\n\\end{aligned}\n\\]\nWe can generate draws from this distribution using the inverse cdf method:\n\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.8.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/mark/.cmdstan/cmdstan-2.35.0\n\n\n- CmdStan version: 2.35.0\n\nf_x &lt;- function(x, a){\n  if(a &lt; 0 | a &gt; 1) stop(\"only defined for a in [0,1]\")\n  if(any(x &lt; 0 | x &gt; 1)) stop(\"only defined for x in [0,1]\")\n  (1-a) * x ^ -a\n}\nF_x &lt;- function(x, a){\n  if(a &lt; 0 | a &gt; 1) stop(\"only defined for a in [0,1]\")\n  if(any(x &lt; 0 | x &gt; 1)) stop(\"only defined for x in [0,1]\")\n  x^(1-a)\n}\nF_inv_x &lt;- function(u, a){\n  if(a &lt; 0 | a &gt; 1) stop(\"only defined for a in [0,1]\")\n  if(any(u &lt; 0 | u &gt; 1)) stop(\"only defined for x in [0,1]\")\n  u ^ (1 / (1-a))\n}\n\na &lt;- 0.35\nx &lt;- seq(0, 1, len = 1000)\nd_fig &lt;- data.table(x = x, y = f_x(x, a))\nd_sim &lt;- data.table(\n  y_sim = F_inv_x(runif(1e6), a)\n)\n\nggplot(d_fig, aes(x = x, y = y)) +\n  geom_histogram(data = d_sim, aes(x = y_sim, y = ..density..),\n               inherit.aes = F, fill = 1, alpha = 0.2,\n               binwidth = density(d_sim$y_sim)$bw) + \n  geom_line() +\n  theme_bw()\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nfunctions {\n  real custom_lpdf(vector x, real alpha) {\n    \n    int n_x = num_elements(x);\n    vector[n_x] lpdf;\n    for(i in 1:n_x){\n      \n      lpdf[i] = log1m(alpha) - alpha * log(x[i]);\n    }  \n    return sum(lpdf);\n  }\n}\ndata {\n  int N;\n  vector[N] y;\n}\n\nparameters {\n  real&lt;lower=0, upper = 1&gt; a;\n}\nmodel {\n  target += exponential_lpdf(a | 1);\n  target += custom_lpdf(y | a);   \n}\n\n\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/custom-dist-1.stan\")\n\nld = list(\n  N = 1000, \n  y = d_sim$y_sim[1:1000]\n)\n\n\nf1 &lt;- m1$sample(\n  ld, iter_warmup = 1000, iter_sampling = 1000,\n  parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = F,\n  max_treedepth = 10)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.2 seconds.\n\nf1$summary(variables = c(\"a\"))\n\n# A tibble: 1 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 a        0.385  0.387 0.0204 0.0203 0.349 0.416  1.00     345.     476.\n\npost &lt;- data.table(f1$draws(variables = \"a\", format = \"matrix\"))\nhist(post$a)\n\n\n\n\n\n\n\n\n\n\nReferences"
  }
]