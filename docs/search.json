[
  {
    "objectID": "notebooks/robust-errors.html",
    "href": "notebooks/robust-errors.html",
    "title": "Robust errors for estimating proportion",
    "section": "",
    "text": "The goto approach for estimating an uncertainty interval for a proportion is to use the normal approximation of the binomial distribution:\n\\[\n\\begin{aligned}\n\\hat{p} \\pm z_{(1 - \\alpha)/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\end{aligned}\n\\]\nwhere \\(n\\) is the sample size, \\(\\hat{p}\\) is the observed sample proportion and \\(z\\) is the standard normal quantile (and typically set to \\(\\approx 2\\)).\nSay we had multiple estimates of the proportion, e.g. number of times we observe antimicrobial resistance out of the positive blood cultures we collected over the previous year. These estimates might come from differing hospitals and some might include repeat tests on individuals. This means that we have multiple levels of variation to deal with. One approach is to use a robust (sometimes call a Hubert White or sandwich) estimator for the standard errors.\nThis can be achieved by fitting a standard glm and then making an adjustment to the standard errors using the tools provided in the R sandwich package.\nSimulate data for 500 patients from 10 sites, some patients having repeat measures.\n\nlibrary(sandwich)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(marginaleffects)\nlibrary(lme4)\n\nLoading required package: Matrix\n\n# N unique pts on which we have multiple obs, each pt nested within one of\n# the 10 sites\nN &lt;- 500\nN_site &lt;- 10\np_site &lt;- as.numeric(extraDistr::rdirichlet(1, rep(1, 10)))\nsites &lt;- sample(1:N_site, N, replace = T, prob = p_site)\nd_pt &lt;- data.table(\n  id_pt = 1:N,\n  site = sort(sites)\n)\n# number obs per pt - inflated here to make a point\nn_obs &lt;- rpois(N, 2)\nd &lt;- d_pt[unlist(lapply(1:N, function(x){\n  rep(x, n_obs[x])\n}))]\nd[, id_obs := 1:.N, keyby = id_pt]\n\n# about 60% (plogis(0.4)) resistant but with site and subject level\n# variability beyond the natural sampling variability due to varying \n# number of subjects per site\nnu &lt;- rnorm(N, 0, 0.5)\n# treat site as true fixed effect\nrho &lt;- rnorm(N_site, 0, 0.7)\nd[, eta := 0.4 + rho[site] + nu[id_pt]]\n\nd[, y := rbinom(.N, 1, plogis(eta))]\nd[, site := factor(site)]\nd[, id_pt := factor(id_pt)]\n\np_obs &lt;- d[, sum(y)/.N]\n\n# d[, .(y = sum(y), n = .N)]\n# distribution of frequency of observations on a pt\n# hist(d[, .N, keyby = id_pt][, N])\n\nThe raw numbers of observations at each site are shown in Figure 1.\n\nd_fig &lt;- copy(d)\nd_fig[y == 0, resp := \"Susceptible\"]\nd_fig[y == 1, resp := \"Resistant\"]\nggplot(d_fig, aes(x = site, fill = resp)) +\n  geom_bar() +\n  scale_fill_discrete(\"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: Observations by site\n\n\n\n\n\nOverall, the observed proportion resistant to antibiotics is 0.652. Various ways exist to estimate the uncertainty.\nThe wald estimate for the uncertainty interval is calculated as:\n\n# wald (normal approximation)\nse_wald &lt;- sqrt(p_obs * (1-p_obs) / nrow(d))\np_0_lb &lt;- p_obs - qnorm(0.975) * se_wald\np_0_ub &lt;- p_obs + qnorm(0.975) * se_wald\n\nA GLM with only an intercept term will give the same prediction as the observed proportion and we can calculate the naive estimate of uncertainty as:\n\n# standard glm, not accounting for pt\nf1 &lt;- glm(y ~ 1, family = binomial, data = d)\n\npredict(f1, type = \"response\")[1]\n\n        1 \n0.6518595 \n\n# get naive standard errors\ns_f1 &lt;- summary(f1)$coef\n\n# model uncertainty naive\nlo_1 &lt;- qlogis(p_obs)\n# se from intercept term, i.e. we are just looking at the 'average' or\n# typical pt, over which we would expect heterogeneity\np_1_lb &lt;- plogis(lo_1 - qnorm(0.975) * s_f1[1, 2])\np_1_ub &lt;- plogis(lo_1 + qnorm(0.975) * s_f1[1, 2])\n\nWe can use the sandwich estimator to adjuste for heterogeneity as:\n\n# adjusted to account for heterogeneity due to site (we did not \n# adjust for site in the model) and repeat measure for pt\nsw_se &lt;- sqrt(vcovCL(f1, cluster = d[, .(site, id_pt)], type = \"HC1\")[1,1])\np_2_lb &lt;- plogis(lo_1 - qnorm(0.975) * sw_se)\np_2_ub &lt;- plogis(lo_1 + qnorm(0.975) * sw_se)\n\n\nf2 &lt;- glmer(y ~ (1|site) + (1|id_pt), data = d, family = binomial)\n\nNote that in the glmer model (with a non-linear link function) the predictions are first made on the link scale, averaged, and then back transformed. This means that the average prediction may not be exactly identical to the average of predictions.\nYou’ll note that the point estimate from the glmer deviates from the observed proportion. This can be for all of the following reasons:\n\nThe GLMM provides subject-specific estimates, conditional on the random effects.\nGLMMs involve shrinkage, where estimates for groups with less data are pulled towards the overall mean.\nLarger random effects can lead to bigger differences.\nThe GLMM estimate is a model-based estimate that accounts for the hierarchical structure of the data and provides a framework for inference.\n\n\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_0_lb, p_0_ub)\n\n[1] \"0.6519 (0.6218, 0.6819)\"\n\n# Model based adjusting for site.\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_1_lb, p_1_ub)\n\n[1] \"0.6519 (0.6213, 0.6812)\"\n\n# Adjusted - account for heterogeneity due to site (we did not \n# adjust for site) and repeat measure for pt\nsprintf(\"%.4f (%.4f, %.4f)\", p_obs, p_2_lb, p_2_ub)\n\n[1] \"0.6519 (0.5537, 0.7386)\"\n\n# Finally a random effects model\navg_predictions(f2, re.form = NA, type = \"link\")\n\n\n Estimate Std. Error    z Pr(&gt;|z|)   S   2.5 % 97.5 %\n    0.434      0.233 1.86    0.063 4.0 -0.0235  0.891\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  link \n\navg_predictions(f2, re.form = NA, type = \"response\")\n\n\n Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n    0.607     0.0557 10.9   &lt;0.001 89.5 0.498  0.716\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n## Step 1\npred &lt;- predictions(f2, type = \"link\", re.form = NA)$estimate\n## Step 2: average\nplogis(mean(pred))\n\n[1] 0.606777\n\n\n\nReferences"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Biostatistician working mostly in the area of Bayesian adaptive clinical trials using R and stan. The site contains various posts/reminders on topics that are relevant to my day-to-day work."
  },
  {
    "objectID": "about.html#repository-status",
    "href": "about.html#repository-status",
    "title": "About",
    "section": "Repository status",
    "text": "Repository status\n\nlibrary(git2r)\nrepo &lt;- git2r::repository(path = \".\")\nsummary(repo)\n\nLocal:    main /Users/mark/Documents/project/website/src/maj-biostat.github.io\nRemote:   main @ origin (https://github.com/maj-biostat/maj-biostat.github.io)\nHead:     [5cf7ba8] 2024-09-18: wip\n\nBranches:         1\nTags:             0\nCommits:         10\nContributors:     2\nStashes:          0\nIgnored files:    2\nUntracked files:  5\nUnstaged files:  26\nStaged files:     2\n\nLatest commits:\n[5cf7ba8] 2024-09-18: wip\n[cfd1ac9] 2024-09-18: wip\n[ceabb23] 2024-09-18: minor edit\n[9e01149] 2024-09-18: add listing\n[19b90b8] 2024-09-18: wip"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "maj-biostat.github.io",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n2024-09-23\n\n\nRandom walk priors\n\n\n3 min\n\n\n\n\n\n\n\n2024-09-23\n\n\nRobust errors for estimating proportion\n\n\n4 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/random-walk-prior.html",
    "href": "notebooks/random-walk-prior.html",
    "title": "Random walk priors",
    "section": "",
    "text": "First order random walk\nFor regular spacings, a first-order random walk prior can be specified as:\n\\[\n\\begin{aligned}\n\\eta_0 &\\sim \\text{Logistic}(0,1) \\\\\n\\delta &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma_\\delta &\\sim \\text{Exponential}(1) \\\\\n\\eta_{[1]} &= \\eta_0 \\\\\n\\eta_{[k]} &= \\sum_{i = 2}^{N}(\\eta_{[k-1]}  + \\delta  \\sigma_\\delta) \\\\\n\\end{aligned}\n\\]\nSimulate data from an oscillator:\n\nlibrary(data.table)\nlibrary(ggplot2)\n\nset.seed(2)\nd_obs &lt;- data.table(\n  x = sort(runif(100, 0, 2*pi))\n)\nd_obs[, eta := sin(x)]\nd_obs[, n := rpois(.N, 200)]\nd_obs[, y := rbinom(.N, n, plogis(eta))]\n\n# we only observe 30% of the data generated\nd_obs[, y_mis := rbinom(.N, 1, 0.7)]\n\nNaive implementation of a first order random walk in stan.\n\n\ndata {    \n  int N; \n  // the way the model is set up it does not matter if some of the n's are\n  // zero because the likelihood uses y_sub, which is obtained by reference\n  // to the missing indicator y_mis, which explicitly says that there were\n  // no observations at the given value of x.\n  array[N] int y;    \n  array[N] int n;    \n  vector[N] x;    \n  array[N] int y_mis; \n  \n  int prior_only;    \n  \n  // priors\n  real r_nu;\n  \n}    \ntransformed data {\n  // x_diff gives us the variable spacing in x and allows us to scale\n  // the variance appropriately\n  vector[N-1] x_diff;\n  // the number of observations we truly had once missingness is accounted for\n  int N_sub = N - sum(y_mis);\n  // our truly observed responses (successes) and trials\n  array[N_sub] int y_sub;\n  array[N_sub] int n_sub;\n  // \n  for(i in 1:(N-1)){x_diff[i] = x[i+1] - x[i];}\n  // go through the data that was passed in and build the data on which \n  // we will fit the model\n  int j = 1;\n  for(i in 1:N){\n    if(y_mis[i] == 0){\n      y_sub[j] = y[i];\n      n_sub[j] = n[i];\n      j += 1;\n    }\n  }  \n}\nparameters{  \n  // the first response\n  real b0;    \n  // offsets\n  vector[N-1] delta;    \n  // how variable the response is\n  real&lt;lower=0&gt; nu;   \n}    \ntransformed parameters{    \n  // the complete modelled mean response\n  vector[N] e; \n  // this is the variance scaled for the distance between each x\n  // note this is truly a variance and not an sd\n  vector[N-1] tau;    \n  // \n  vector[N_sub] eta_sub;    \n  // adjust the variance for the distance b/w doses    \n  // note that nu is squared to turn it into variance\n  for(i in 2:N){tau[i-1] = x_diff[i-1]*pow(nu, 2);}    \n  // resp is random walk with missingness filled in due to the \n  // dependency in the prior\n  e[1] = b0;    \n  // each subsequent observation has a mean equal to the previous one\n  // plus some normal deviation with mean zero and variance calibrated for\n  // the distance between subsequent observations.\n  for(i in 2:N){e[i] = e[i-1] + delta[i-1] * sqrt(tau[i-1]);}    \n  // eta_sub is what gets passed to the likelihood\n  { \n    int k = 1;\n    for(i in 1:N){\n      if(y_mis[i] == 0){\n        eta_sub[k] = e[i];\n        k += 1;\n      }\n    }\n  }\n}    \nmodel{    \n  // prior on initial response\n  target += logistic_lpdf(b0 | 0, 1);\n  // prior on sd\n  target += exponential_lpdf(nu | r_nu);\n  // standard normal prior on the offsets\n  target += normal_lpdf(delta | 0, 1);    \n  if(!prior_only){target += binomial_logit_lpmf(y_sub | n_sub, eta_sub);}    \n}    \ngenerated quantities{    \n  // predicted values at each value of x\n  vector[N] p;    \n  vector[N-1] e_diff;    \n  vector[N-1] e_grad;    \n  // compute diffs\n  for(i in 1:(N-1)){e_diff[i] = e[i+1] - e[i];}\n  e_grad = e_diff ./ x_diff;\n  p = inv_logit(e);\n}    \n\n\n\nm1 &lt;- cmdstanr::cmdstan_model(\"stan/random-walk-01.stan\")\n\n\nld = list(\n  N = nrow(d_obs), \n  y = d_obs[, y], \n  n = d_obs[, n],\n  x = d_obs[, x], \n  y_mis = d_obs[, y_mis], \n  prior_only = F, \n  r_nu =  3\n  )\n\n\nf1 &lt;- m1$sample(\n  ld, iter_warmup = 1000, iter_sampling = 2000,\n  parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = F,\n  max_treedepth = 10)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 2.9 seconds.\n\n\nWarning: 2 of 2000 (0.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\nf1$summary(variables = c(\"nu\"))\n\n# A tibble: 1 × 10\n  variable  mean median    sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 nu       0.533  0.519 0.103 0.0946 0.392 0.722  1.00    1221.    1114.\n\n\nRepresentation of output.\n\nd_out &lt;- data.table(f1$draws(variables = \"p\", format = \"matrix\"))\n\nd_fig &lt;- melt(d_out, measure.vars = names(d_out))\nd_fig &lt;- d_fig[, .(\n  mu = mean(value), \n  q_025 = quantile(value, prob = 0.025),\n  q_975 = quantile(value, prob = 0.975)\n), keyby = variable]\nd_fig[, ix := gsub(\"p[\", \"\", variable, fixed = T)]\nd_fig[, ix := as.numeric(gsub(\"]\", \"\", ix, fixed = T))]\nd_fig[, x := d_obs[ix, x]]\n\n\nggplot(d_obs, aes(x = x, y = plogis(eta))) +\n  geom_line(lty = 1) +\n  geom_point(data = d_obs[y_mis == 0],\n             aes(x = x, y = y/n), size = 0.7) +\n  geom_point(data = d_obs[y_mis == 1],\n             aes(x = x, y = y/n), size = 0.7, pch = 2) +\n  geom_ribbon(data = d_fig, \n              aes(x = x, ymin = q_025, ymax = q_975),\n              inherit.aes = F, fill = 2, alpha = 0.3) +\n  geom_line(data = d_fig, \n              aes(x = x, y = mu), col = 2) +\n  geom_point(data = d_fig, \n              aes(x = x, y = mu), col = 2, size = 0.6) +\n  scale_x_continuous(\"x\") +\n  scale_y_continuous(\"Probability\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 1: True function (black line), data on which the model was fit (black points), data we did not see (black triangles), random walk (red line) with interpolated points (red) and uncertainty (red ribbon).\n\n\n\n\n\n\n\nSecond order random walk\nThe second order random walk for regular locations has density\n\\[\n\\begin{aligned}\n\\pi(x) \\propto \\exp\\left( -\\frac{1}{2} \\sum_{i=2}^{n-1} (x_{i-1} - 2x_i + x_{i+1})^2  \\right)\n\\end{aligned}\n\\]\nThe main term can be interpreted as an estimate of the second order derivative of a continuous time function. But this is not generally suitable for irregular spacings of x [1].\n\n\nReferences\n\n\n1. Lindgren F, Rue H. On the second-order random walk model for irregular locations. Scandinavian Journal of Statistics. 208AD;35:691–700."
  }
]